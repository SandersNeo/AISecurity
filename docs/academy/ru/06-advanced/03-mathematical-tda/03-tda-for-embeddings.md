# TDA –¥–ª—è –ê–Ω–∞–ª–∏–∑–∞ Embeddings

> **–£—Ä–æ–≤–µ–Ω—å:** ›ÍÒÔÂÚ  
> **–í—Ä–µ–º—è:** 55 –º–∏–Ω—É—Ç  
> **–¢—Ä–µ–∫:** 06 ‚Äî Mathematical Foundations  
> **–ú–æ–¥—É–ª—å:** 06.1 ‚Äî TDA (Topological Data Analysis)  
> **–í–µ—Ä—Å–∏—è:** 1.0

---

## –¶–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è

- [ ] –ü–æ–Ω—è—Ç—å —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞ embedding spaces
- [ ] –ü—Ä–∏–º–µ–Ω—è—Ç—å TDA –º–µ—Ç–æ–¥—ã –∫ –∞–Ω–∞–ª–∏–∑—É LLM embeddings
- [ ] –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å TDA-based detection –≤ security pipeline
- [ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å persistence diagrams –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π

---

## 1. Embeddings –∏ Topology

### 1.1 –ü–æ—á–µ–º—É TDA –¥–ª—è Embeddings?

LLM embeddings –æ–±—Ä–∞–∑—É—é—Ç —Å–ª–æ–∂–Ω—ã–µ manifolds –≤ –≤—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. TDA –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä—É.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              EMBEDDINGS –ö–ê–ö –¢–û–ü–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –û–ë–™–ï–ö–¢                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                    ‚îÇ
‚îÇ  Text ‚Üí [LLM Encoder] ‚Üí Embedding ‚àà ‚Ñù‚Åø (n = 384, 768, 1536...)    ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  –ö–æ–ª–ª–µ–∫—Ü–∏—è embeddings = Point Cloud –≤ ‚Ñù‚Åø                          ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  TDA –∏–∑–≤–ª–µ–∫–∞–µ—Ç:                                                    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ H‚ÇÄ: –°–≤—è–∑–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (–∫–ª–∞—Å—Ç–µ—Ä—ã —Å–º—ã—Å–ª–æ–≤)                    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ H‚ÇÅ: –¶–∏–∫–ª—ã/–¥—ã—Ä—ã (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø–µ—Ç–ª–∏)                         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ H‚ÇÇ: –ü–æ–ª–æ—Å—Ç–∏ (—Å–ª–æ–∂–Ω—ã–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã)                ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫ Security:                                            ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Normal embeddings ‚Üí —Å—Ç–∞–±–∏–ª—å–Ω–∞—è —Ç–æ–ø–æ–ª–æ–≥–∏—è                     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Attack embeddings ‚Üí –Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ features                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ –î–µ—Ç–µ–∫—Ü–∏—è = —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ persistence diagrams                    ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.2 –ú–µ—Ç—Ä–∏–∫–∏ –≤ Embedding Space

```python
import numpy as np
from scipy.spatial.distance import pdist, squareform
from sklearn.metrics.pairwise import cosine_distances

class EmbeddingMetrics:
    """–†–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è embedding space"""
    
    @staticmethod
    def euclidean_distance_matrix(embeddings: np.ndarray) -> np.ndarray:
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –µ–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ"""
        return squareform(pdist(embeddings, metric='euclidean'))
    
    @staticmethod
    def cosine_distance_matrix(embeddings: np.ndarray) -> np.ndarray:
        """
        –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ ‚Äî –±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è embeddings,
        —Ç–∞–∫ –∫–∞–∫ –≤–∞–∂–Ω—ã –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è, –∞ –Ω–µ magnitude.
        """
        return cosine_distances(embeddings)
    
    @staticmethod
    def normalized_euclidean(embeddings: np.ndarray) -> np.ndarray:
        """–ï–≤–∫–ª–∏–¥–æ–≤–∞ –º–µ—Ç—Ä–∏–∫–∞ –ø–æ—Å–ª–µ L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"""
        normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
        return squareform(pdist(normalized, metric='euclidean'))
    
    @staticmethod
    def angular_distance(embeddings: np.ndarray) -> np.ndarray:
        """
        –£–≥–ª–æ–≤–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ ‚Äî arccos –æ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏.
        –ú–µ—Ç—Ä–∏–∫–∞ (—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤—É —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∞).
        """
        cos_sim = np.dot(embeddings, embeddings.T)
        norms = np.linalg.norm(embeddings, axis=1)
        cos_sim = cos_sim / np.outer(norms, norms)
        cos_sim = np.clip(cos_sim, -1, 1)  # –ß–∏—Å–ª–µ–Ω–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
        return np.arccos(cos_sim) / np.pi  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ [0, 1]
```

---

## 2. Persistence Homology –¥–ª—è Embeddings

### 2.1 Vietoris-Rips Complex

```python
from ripser import ripser
from persim import plot_diagrams, wasserstein, bottleneck
import matplotlib.pyplot as plt

class EmbeddingPersistence:
    """
    Persistent Homology –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ embedding space.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Vietoris-Rips filtration.
    """
    
    def __init__(self, max_dim: int = 1, max_edge_length: float = np.inf):
        """
        Args:
            max_dim: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≥–æ–º–æ–ª–æ–≥–∏–π (0, 1, 2)
            max_edge_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ä–µ–±—Ä–∞ –≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        """
        self.max_dim = max_dim
        self.max_edge_length = max_edge_length
        self.diagrams = None
        self.distance_matrix = None
    
    def compute(self, embeddings: np.ndarray, 
                metric: str = 'cosine') -> dict:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç persistent homology –¥–ª—è embeddings.
        
        Args:
            embeddings: –ú–∞—Ç—Ä–∏—Ü–∞ embeddings (n_samples, n_features)
            metric: 'euclidean', 'cosine', –∏–ª–∏ 'angular'
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å diagrams –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞–º–∏
        """
        # –í—ã—á–∏—Å–ª—è–µ–º distance matrix
        if metric == 'euclidean':
            self.distance_matrix = EmbeddingMetrics.euclidean_distance_matrix(embeddings)
        elif metric == 'cosine':
            self.distance_matrix = EmbeddingMetrics.cosine_distance_matrix(embeddings)
        elif metric == 'angular':
            self.distance_matrix = EmbeddingMetrics.angular_distance(embeddings)
        else:
            raise ValueError(f"Unknown metric: {metric}")
        
        # Ripser –¥–ª—è persistent homology
        result = ripser(
            self.distance_matrix,
            maxdim=self.max_dim,
            thresh=self.max_edge_length,
            distance_matrix=True
        )
        
        self.diagrams = result['dgms']
        
        return {
            'diagrams': self.diagrams,
            'h0_features': len(self.diagrams[0]),
            'h1_features': len(self.diagrams[1]) if self.max_dim >= 1 else 0,
            'statistics': self._compute_statistics()
        }
    
    def _compute_statistics(self) -> dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ persistence diagrams"""
        stats = {}
        
        for dim, dgm in enumerate(self.diagrams):
            if len(dgm) == 0:
                continue
            
            # Lifetime = death - birth
            lifetimes = dgm[:, 1] - dgm[:, 0]
            # –§–∏–ª—å—Ç—Ä—É–µ–º inf
            finite_lifetimes = lifetimes[np.isfinite(lifetimes)]
            
            if len(finite_lifetimes) > 0:
                stats[f'H{dim}_count'] = len(dgm)
                stats[f'H{dim}_mean_lifetime'] = np.mean(finite_lifetimes)
                stats[f'H{dim}_max_lifetime'] = np.max(finite_lifetimes)
                stats[f'H{dim}_std_lifetime'] = np.std(finite_lifetimes)
                stats[f'H{dim}_total_persistence'] = np.sum(finite_lifetimes)
        
        return stats
    
    def get_persistent_features(self, min_persistence: float = 0.1) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ persistent features (—Å –±–æ–ª—å—à–∏–º lifetime).
        
        Args:
            min_persistence: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π lifetime –¥–ª—è feature
        
        Returns:
            –£—Å—Ç–æ–π—á–∏–≤—ã–µ features –ø–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è–º
        """
        persistent = {}
        
        for dim, dgm in enumerate(self.diagrams):
            lifetimes = dgm[:, 1] - dgm[:, 0]
            mask = (lifetimes >= min_persistence) & np.isfinite(lifetimes)
            persistent[f'H{dim}'] = dgm[mask]
        
        return persistent
    
    def plot(self, save_path: str = None):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è persistence diagrams"""
        if self.diagrams is None:
            raise ValueError("Call compute() first")
        
        fig, axes = plt.subplots(1, self.max_dim + 1, figsize=(5 * (self.max_dim + 1), 4))
        
        if self.max_dim == 0:
            axes = [axes]
        
        plot_diagrams(self.diagrams, ax=axes[0], show=False)
        
        for i, ax in enumerate(axes):
            ax.set_title(f'H{i} Persistence Diagram')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path)
        
        return fig
```

### 2.2 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Persistence Diagrams

```python
class PersistenceComparator:
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ persistence diagrams –¥–ª—è detection.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Wasserstein –∏ Bottleneck distances.
    """
    
    def __init__(self):
        self.baseline_diagrams = None
    
    def set_baseline(self, diagrams: list):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç baseline diagrams"""
        self.baseline_diagrams = diagrams
    
    def compare(self, target_diagrams: list) -> dict:
        """
        –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç target diagrams —Å baseline.
        
        Args:
            target_diagrams: Diagrams –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        
        Returns:
            Distances –ø–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è–º
        """
        if self.baseline_diagrams is None:
            raise ValueError("Set baseline first")
        
        results = {}
        
        for dim in range(min(len(self.baseline_diagrams), len(target_diagrams))):
            baseline_dgm = self.baseline_diagrams[dim]
            target_dgm = target_diagrams[dim]
            
            # Wasserstein distance (p=2)
            try:
                w_dist = wasserstein(baseline_dgm, target_dgm, matching=False)
            except:
                w_dist = float('inf')
            
            # Bottleneck distance
            try:
                b_dist = bottleneck(baseline_dgm, target_dgm, matching=False)
            except:
                b_dist = float('inf')
            
            results[f'H{dim}_wasserstein'] = w_dist
            results[f'H{dim}_bottleneck'] = b_dist
        
        return results
    
    def is_anomaly(self, target_diagrams: list, 
                   wasserstein_threshold: float = 0.5,
                   bottleneck_threshold: float = 0.3) -> dict:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ target –∞–Ω–æ–º–∞–ª—å–Ω—ã–º.
        
        Args:
            target_diagrams: Diagrams –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            wasserstein_threshold: –ü–æ—Ä–æ–≥ –ø–æ Wasserstein
            bottleneck_threshold: –ü–æ—Ä–æ–≥ –ø–æ Bottleneck
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç anomaly detection
        """
        distances = self.compare(target_diagrams)
        
        anomalies = []
        for key, value in distances.items():
            if 'wasserstein' in key and value > wasserstein_threshold:
                anomalies.append({
                    'metric': key,
                    'value': value,
                    'threshold': wasserstein_threshold
                })
            elif 'bottleneck' in key and value > bottleneck_threshold:
                anomalies.append({
                    'metric': key,
                    'value': value,
                    'threshold': bottleneck_threshold
                })
        
        return {
            'is_anomaly': len(anomalies) > 0,
            'distances': distances,
            'violations': anomalies
        }
```

---

## 3. Topological Signatures –¥–ª—è –¢–µ–∫—Å—Ç–æ–≤

### 3.1 Embedding Topology Signature

```python
from sentence_transformers import SentenceTransformer
from typing import List
import hashlib

class TopologicalSignature:
    """
    –¢–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–∏–≥–Ω–∞—Ç—É—Ä–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π.
    """
    
    def __init__(self, embedding_model: str = "all-MiniLM-L6-v2"):
        self.encoder = SentenceTransformer(embedding_model)
        self.persistence = EmbeddingPersistence(max_dim=1)
    
    def compute_signature(self, texts: List[str], 
                         metric: str = 'cosine') -> dict:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —Å–∏–≥–Ω–∞—Ç—É—Ä—É –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤.
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            metric: –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è embeddings
        
        Returns:
            –¢–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–∏–≥–Ω–∞—Ç—É—Ä–∞
        """
        # Embeddings
        embeddings = self.encoder.encode(texts)
        
        # Persistent homology
        result = self.persistence.compute(embeddings, metric=metric)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ features
        signature = {
            'n_texts': len(texts),
            'embedding_dim': embeddings.shape[1],
            'metric': metric,
            
            # H0 features
            'h0_count': result['statistics'].get('H0_count', 0),
            'h0_mean_lifetime': result['statistics'].get('H0_mean_lifetime', 0),
            'h0_max_lifetime': result['statistics'].get('H0_max_lifetime', 0),
            
            # H1 features
            'h1_count': result['statistics'].get('H1_count', 0),
            'h1_mean_lifetime': result['statistics'].get('H1_mean_lifetime', 0),
            'h1_total_persistence': result['statistics'].get('H1_total_persistence', 0),
            
            # Diagrams
            'diagrams': result['diagrams']
        }
        
        # Signature hash
        signature['hash'] = self._compute_hash(signature)
        
        return signature
    
    def _compute_hash(self, signature: dict) -> str:
        """–í—ã—á–∏—Å–ª—è–µ—Ç hash —Å–∏–≥–Ω–∞—Ç—É—Ä—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        key_values = [
            signature['h0_count'],
            round(signature['h0_mean_lifetime'], 3),
            signature['h1_count'],
            round(signature['h1_mean_lifetime'], 3)
        ]
        return hashlib.md5(str(key_values).encode()).hexdigest()[:16]
    
    def compare_signatures(self, sig1: dict, sig2: dict) -> dict:
        """
        –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –¥–≤–µ —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–∏–≥–Ω–∞—Ç—É—Ä—ã.
        
        Args:
            sig1: –ü–µ—Ä–≤–∞—è —Å–∏–≥–Ω–∞—Ç—É—Ä–∞
            sig2: –í—Ç–æ—Ä–∞—è —Å–∏–≥–Ω–∞—Ç—É—Ä–∞
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        """
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö statistics
        stat_diffs = {}
        for key in ['h0_count', 'h0_mean_lifetime', 'h1_count', 'h1_mean_lifetime']:
            diff = sig2.get(key, 0) - sig1.get(key, 0)
            rel_diff = diff / (sig1.get(key, 1) + 1e-10)
            stat_diffs[key] = {
                'absolute': diff,
                'relative': rel_diff
            }
        
        # Diagram distances
        comparator = PersistenceComparator()
        comparator.set_baseline(sig1['diagrams'])
        diagram_dists = comparator.compare(sig2['diagrams'])
        
        return {
            'hash_match': sig1['hash'] == sig2['hash'],
            'statistic_differences': stat_diffs,
            'diagram_distances': diagram_dists,
            'is_similar': self._assess_similarity(stat_diffs, diagram_dists)
        }
    
    def _assess_similarity(self, stat_diffs: dict, diagram_dists: dict) -> bool:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–±—â—É—é –ø–æ—Ö–æ–∂–µ—Å—Ç—å —Å–∏–≥–Ω–∞—Ç—É—Ä"""
        # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è < 50%
        for key, diff in stat_diffs.items():
            if abs(diff['relative']) > 0.5:
                return False
        
        # Diagram distances —Ä–∞–∑—É–º–Ω—ã–µ
        for key, dist in diagram_dists.items():
            if 'wasserstein' in key and dist > 0.5:
                return False
        
        return True
```

### 3.2 Sliding Window TDA

```python
class SlidingWindowTDA:
    """
    TDA –∞–Ω–∞–ª–∏–∑ —Å sliding window –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç–æ–ø–æ–ª–æ–≥–∏–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏.
    """
    
    def __init__(self, 
                 window_size: int = 100,
                 step_size: int = 20,
                 embedding_model: str = "all-MiniLM-L6-v2"):
        self.window_size = window_size
        self.step_size = step_size
        self.encoder = SentenceTransformer(embedding_model)
        self.persistence = EmbeddingPersistence(max_dim=1)
        
        self.history = []
        self.current_window = []
    
    def add_text(self, text: str) -> dict:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –∞–Ω–∞–ª–∏–∑.
        
        Args:
            text: –ù–æ–≤—ã–π —Ç–µ–∫—Å—Ç
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –æ–∫–Ω–∞ (–µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç step_size)
        """
        self.current_window.append(text)
        
        if len(self.current_window) >= self.window_size:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–∫–Ω–æ
            result = self._analyze_window()
            
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º
            if self.history:
                change = self._detect_change(result)
                result['change_detected'] = change
            
            self.history.append(result)
            
            # –°–¥–≤–∏–≥–∞–µ–º –æ–∫–Ω–æ
            self.current_window = self.current_window[self.step_size:]
            
            return result
        
        return None
    
    def _analyze_window(self) -> dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—É—â–µ–µ –æ–∫–Ω–æ"""
        embeddings = self.encoder.encode(self.current_window)
        result = self.persistence.compute(embeddings, metric='cosine')
        
        return {
            'window_start': len(self.history) * self.step_size,
            'window_texts': len(self.current_window),
            'statistics': result['statistics'],
            'diagrams': result['diagrams']
        }
    
    def _detect_change(self, current: dict) -> dict:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ–∫–Ω–∞"""
        prev = self.history[-1]
        
        comparator = PersistenceComparator()
        comparator.set_baseline(prev['diagrams'])
        distances = comparator.compare(current['diagrams'])
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∞–Ω–æ–º–∞–ª–∏—é
        anomaly = comparator.is_anomaly(
            current['diagrams'],
            wasserstein_threshold=0.3,
            bottleneck_threshold=0.2
        )
        
        return {
            'distances': distances,
            'is_anomaly': anomaly['is_anomaly'],
            'violations': anomaly['violations']
        }
    
    def get_trend(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç—Ä–µ–Ω–¥ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ç–æ–ø–æ–ª–æ–≥–∏–∏"""
        if len(self.history) < 2:
            return {'status': 'insufficient_data'}
        
        h0_counts = [h['statistics'].get('H0_count', 0) for h in self.history]
        h1_counts = [h['statistics'].get('H1_count', 0) for h in self.history]
        
        return {
            'n_windows': len(self.history),
            'h0_trend': np.polyfit(range(len(h0_counts)), h0_counts, 1)[0],
            'h1_trend': np.polyfit(range(len(h1_counts)), h1_counts, 1)[0],
            'h0_variance': np.var(h0_counts),
            'h1_variance': np.var(h1_counts)
        }
```

---

## 4. Security Applications

### 4.1 Injection Detection via TDA

```python
class TDAInjectionDetector:
    """
    –î–µ—Ç–µ–∫—Ç–æ—Ä prompt injection –Ω–∞ –æ—Å–Ω–æ–≤–µ TDA.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ embedding space.
    """
    
    def __init__(self, embedding_model: str = "all-MiniLM-L6-v2"):
        self.encoder = SentenceTransformer(embedding_model)
        self.persistence = EmbeddingPersistence(max_dim=1)
        self.comparator = PersistenceComparator()
        
        self.baseline_signature = None
        self.thresholds = {
            'wasserstein': 0.4,
            'bottleneck': 0.25,
            'h1_count_change': 3
        }
    
    def train(self, normal_texts: List[str]):
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
        –°—Ç—Ä–æ–∏—Ç baseline —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —Å–∏–≥–Ω–∞—Ç—É—Ä—É.
        """
        embeddings = self.encoder.encode(normal_texts)
        result = self.persistence.compute(embeddings, metric='cosine')
        
        self.baseline_signature = {
            'diagrams': result['diagrams'],
            'statistics': result['statistics'],
            'n_samples': len(normal_texts)
        }
        
        self.comparator.set_baseline(result['diagrams'])
    
    def detect(self, texts: List[str]) -> dict:
        """
        –î–µ—Ç–µ–∫—Ü–∏—è injection –≤ —Ç–µ–∫—Å—Ç–∞—Ö.
        
        Args:
            texts: –¢–µ–∫—Å—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏
        """
        if self.baseline_signature is None:
            raise ValueError("Train the detector first")
        
        # Compute embeddings and persistence
        embeddings = self.encoder.encode(texts)
        result = self.persistence.compute(embeddings, metric='cosine')
        
        # Compare with baseline
        anomaly_check = self.comparator.is_anomaly(
            result['diagrams'],
            wasserstein_threshold=self.thresholds['wasserstein'],
            bottleneck_threshold=self.thresholds['bottleneck']
        )
        
        # Additional checks
        h1_baseline = self.baseline_signature['statistics'].get('H1_count', 0)
        h1_current = result['statistics'].get('H1_count', 0)
        h1_change = abs(h1_current - h1_baseline)
        
        # Aggregate detection
        is_injection = anomaly_check['is_anomaly'] or h1_change > self.thresholds['h1_count_change']
        
        # Confidence score
        confidence = self._compute_confidence(anomaly_check['distances'], h1_change)
        
        return {
            'is_injection': is_injection,
            'confidence': confidence,
            'distances': anomaly_check['distances'],
            'violations': anomaly_check['violations'],
            'h1_change': h1_change,
            'current_statistics': result['statistics'],
            'recommendation': self._get_recommendation(is_injection, confidence)
        }
    
    def _compute_confidence(self, distances: dict, h1_change: int) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç confidence score"""
        score = 0.0
        
        # Wasserstein contribution
        w_h0 = distances.get('H0_wasserstein', 0)
        w_h1 = distances.get('H1_wasserstein', 0)
        score += min(w_h0 / self.thresholds['wasserstein'], 1.0) * 0.3
        score += min(w_h1 / self.thresholds['wasserstein'], 1.0) * 0.3
        
        # H1 change contribution
        score += min(h1_change / self.thresholds['h1_count_change'], 1.0) * 0.4
        
        return min(score, 1.0)
    
    def _get_recommendation(self, is_injection: bool, confidence: float) -> str:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        if not is_injection:
            return "SAFE: –¢–æ–ø–æ–ª–æ–≥–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç baseline"
        elif confidence < 0.5:
            return "LOW_RISK: –ù–µ–±–æ–ª—å—à–∏–µ —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è"
        elif confidence < 0.8:
            return "MEDIUM_RISK: –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞"
        else:
            return "HIGH_RISK: –°–∏–ª—å–Ω—ã–µ —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏, –≤–æ–∑–º–æ–∂–Ω–∞ injection"
```

### 4.2 Multi-Modal TDA Detection

```python
class MultiModalTDADetector:
    """
    Multi-modal –¥–µ—Ç–µ–∫—Ç–æ—Ä, –∫–æ–º–±–∏–Ω–∏—Ä—É—é—â–∏–π TDA features —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.
    """
    
    def __init__(self, embedding_model: str = "all-MiniLM-L6-v2"):
        self.encoder = SentenceTransformer(embedding_model)
        self.tda_detector = TDAInjectionDetector(embedding_model)
        
        # Feature weights
        self.weights = {
            'tda': 0.4,
            'semantic': 0.3,
            'structural': 0.3
        }
    
    def train(self, normal_texts: List[str], attack_texts: List[str] = None):
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö (–∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –∞—Ç–∞–∫—É—é—â–∏—Ö) –¥–∞–Ω–Ω—ã—Ö.
        """
        self.tda_detector.train(normal_texts)
        
        # Semantic baseline
        self.normal_embeddings = self.encoder.encode(normal_texts)
        self.normal_centroid = np.mean(self.normal_embeddings, axis=0)
        self.normal_radius = np.max(
            np.linalg.norm(self.normal_embeddings - self.normal_centroid, axis=1)
        )
        
        # Attack patterns (if provided)
        self.attack_embeddings = None
        if attack_texts:
            self.attack_embeddings = self.encoder.encode(attack_texts)
    
    def detect(self, texts: List[str]) -> dict:
        """
        Multi-modal –¥–µ—Ç–µ–∫—Ü–∏—è.
        
        Returns:
            –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏
        """
        embeddings = self.encoder.encode(texts)
        
        # 1. TDA Detection
        tda_result = self.tda_detector.detect(texts)
        tda_score = tda_result['confidence']
        
        # 2. Semantic Detection (distance from centroid)
        distances = np.linalg.norm(embeddings - self.normal_centroid, axis=1)
        outside_radius = np.mean(distances > self.normal_radius * 1.5)
        semantic_score = outside_radius
        
        # 3. Structural Detection (similarity to known attacks)
        structural_score = 0.0
        if self.attack_embeddings is not None:
            # Max similarity to any attack
            for emb in embeddings:
                sims = np.dot(self.attack_embeddings, emb) / (
                    np.linalg.norm(self.attack_embeddings, axis=1) * np.linalg.norm(emb)
                )
                structural_score = max(structural_score, np.max(sims))
        
        # Combined score
        combined_score = (
            self.weights['tda'] * tda_score +
            self.weights['semantic'] * semantic_score +
            self.weights['structural'] * structural_score
        )
        
        return {
            'is_attack': combined_score > 0.5,
            'combined_score': combined_score,
            'scores': {
                'tda': tda_score,
                'semantic': semantic_score,
                'structural': structural_score
            },
            'tda_details': tda_result,
            'recommendation': self._get_recommendation(combined_score)
        }
    
    def _get_recommendation(self, score: float) -> str:
        if score < 0.3:
            return "SAFE"
        elif score < 0.5:
            return "LOW_RISK: Monitor closely"
        elif score < 0.7:
            return "MEDIUM_RISK: Review required"
        else:
            return "HIGH_RISK: Block and investigate"
```

---

## 5. SENTINEL Integration

```python
from dataclasses import dataclass
from enum import Enum

class RiskLevel(Enum):
    SAFE = "safe"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

@dataclass
class TDASecurityConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è TDA Security Engine"""
    embedding_model: str = "all-MiniLM-L6-v2"
    max_homology_dim: int = 1
    wasserstein_threshold: float = 0.4
    bottleneck_threshold: float = 0.25
    metric: str = "cosine"
    use_multimodal: bool = True

class SENTINELTDAEngine:
    """
    TDA Engine –¥–ª—è SENTINEL framework.
    –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è security detection.
    """
    
    def __init__(self, config: TDASecurityConfig):
        self.config = config
        
        if config.use_multimodal:
            self.detector = MultiModalTDADetector(config.embedding_model)
        else:
            self.detector = TDAInjectionDetector(config.embedding_model)
        
        self.signature_cache = {}
        self.is_trained = False
    
    def train(self, 
              normal_texts: List[str],
              attack_texts: List[str] = None,
              signature_name: str = "default"):
        """
        –û–±—É—á–µ–Ω–∏–µ engine –Ω–∞ –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            normal_texts: –ù–æ—Ä–º–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
            attack_texts: –ê—Ç–∞–∫—É—é—â–∏–µ —Ç–µ–∫—Å—Ç—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            signature_name: –ò–º—è —Å–∏–≥–Ω–∞—Ç—É—Ä—ã –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        if self.config.use_multimodal:
            self.detector.train(normal_texts, attack_texts)
        else:
            self.detector.train(normal_texts)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∏–≥–Ω–∞—Ç—É—Ä—É
        sig_computer = TopologicalSignature(self.config.embedding_model)
        self.signature_cache[signature_name] = sig_computer.compute_signature(
            normal_texts, self.config.metric
        )
        
        self.is_trained = True
    
    def analyze(self, texts: List[str]) -> dict:
        """
        –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤.
        
        Returns:
            –ü–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        if not self.is_trained:
            raise RuntimeError("Train the engine first")
        
        result = self.detector.detect(texts)
        
        # Determine risk level
        score = result.get('combined_score', result.get('confidence', 0))
        risk_level = self._determine_risk_level(score)
        
        return {
            'risk_level': risk_level.value,
            'is_attack': result.get('is_attack', result.get('is_injection', False)),
            'score': score,
            'details': result,
            'action': self._get_action(risk_level)
        }
    
    def _determine_risk_level(self, score: float) -> RiskLevel:
        if score < 0.2:
            return RiskLevel.SAFE
        elif score < 0.4:
            return RiskLevel.LOW
        elif score < 0.6:
            return RiskLevel.MEDIUM
        elif score < 0.8:
            return RiskLevel.HIGH
        else:
            return RiskLevel.CRITICAL
    
    def _get_action(self, risk_level: RiskLevel) -> str:
        actions = {
            RiskLevel.SAFE: "ALLOW",
            RiskLevel.LOW: "ALLOW_WITH_LOGGING",
            RiskLevel.MEDIUM: "REQUIRE_REVIEW",
            RiskLevel.HIGH: "BLOCK_PENDING_REVIEW",
            RiskLevel.CRITICAL: "BLOCK_AND_ALERT"
        }
        return actions.get(risk_level, "BLOCK")
```

---

## 6. –†–µ–∑—é–º–µ

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –û–ø–∏—Å–∞–Ω–∏–µ |
|-----------|----------|
| **Persistence Homology** | –ò–∑–≤–ª–µ–∫–∞–µ—Ç H‚ÇÄ, H‚ÇÅ features –∏–∑ embedding space |
| **Wasserstein/Bottleneck** | –ú–µ—Ç—Ä–∏–∫–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è persistence diagrams |
| **Topological Signature** | –ö–æ–º–ø–∞–∫—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–ø–æ–ª–æ–≥–∏–∏ –∫–æ—Ä–ø—É—Å–∞ |
| **Sliding Window TDA** | –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Ç–æ–ø–æ–ª–æ–≥–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ |
| **Multi-Modal Detection** | –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ TDA —Å —Å–µ–º–∞–Ω—Ç–∏–∫–æ–π –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π |

---

## –°–ª–µ–¥—É—é—â–∏–π —É—Ä–æ–∫

‚Üí [Track 07: Governance](../../07-governance/README.md)

---

*AI Security Academy | Track 06: Mathematical Foundations | Module 06.1: TDA*
