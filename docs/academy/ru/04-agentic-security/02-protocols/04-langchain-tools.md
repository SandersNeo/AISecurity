# –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å LangChain Tools

> **–£—Ä–æ–≤–µ–Ω—å:** —Â‰ÌËÈ | **–í—Ä–µ–º—è:** 35 –º–∏–Ω | **–¢—Ä–µ–∫:** 04 | **–ú–æ–¥—É–ª—å:** 04.2

---

## 1. –û–±–∑–æ—Ä LangChain Tools

LangChain –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ tool interfaces –¥–ª—è LLM –∞–≥–µ–Ω—Ç–æ–≤.

```python
from langchain.tools import BaseTool, tool
from pydantic import BaseModel, Field

class SearchInput(BaseModel):
    query: str = Field(description="Search query")

class SecureSearchTool(BaseTool):
    name = "search"
    description = "Search the knowledge base"
    args_schema = SearchInput
    
    def _run(self, query: str) -> str:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if not self._validate_query(query):
            return "Invalid query"
        return self._perform_search(query)
    
    def _validate_query(self, query: str) -> bool:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ injection patterns
        dangerous = ["ignore previous", "system:", "admin"]
        return not any(d in query.lower() for d in dangerous)
```

---

## 2. Security Threats

```
LangChain Tool Threats:
‚îú‚îÄ‚îÄ Tool Confusion (–≤—ã–∑–æ–≤ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ tool)
‚îú‚îÄ‚îÄ Parameter Injection (–≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–µ args)
‚îú‚îÄ‚îÄ Chain Manipulation (–∏–∑–º–µ–Ω–µ–Ω–∏–µ execution flow)
‚îî‚îÄ‚îÄ Memory Poisoning (–ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ agent memory)
```

---

## 3. Secure Tool Implementation

```python
class SecureToolExecutor:
    def __init__(self, allowed_tools: list):
        self.tools = {t.name: t for t in allowed_tools}
        self.audit_log = []
    
    def execute(self, tool_name: str, args: dict, context: dict) -> str:
        # 1. –í–∞–ª–∏–¥–∞—Ü–∏—è —á—Ç–æ tool —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if tool_name not in self.tools:
            raise SecurityError(f"Unknown tool: {tool_name}")
        
        tool = self.tools[tool_name]
        
        # 2. –í–∞–ª–∏–¥–∞—Ü–∏—è args –ø—Ä–æ—Ç–∏–≤ schema
        validated = tool.args_schema(**args)
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ permissions
        if not self._check_permission(tool_name, context):
            raise PermissionError("Access denied")
        
        # 4. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å audit
        self.audit_log.append({
            "tool": tool_name, "args": args, 
            "user": context.get("user_id")
        })
        
        return tool._run(**validated.dict())
```

---

## 4. Chain Security

```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

class SecureChain:
    def __init__(self, llm, tools: list):
        self.llm = llm
        self.tool_executor = SecureToolExecutor(tools)
        self.max_iterations = 10
    
    def run(self, input_text: str, context: dict) -> str:
        # –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è input
        sanitized = self._sanitize_input(input_text)
        
        iterations = 0
        while iterations < self.max_iterations:
            # –ü–æ–ª—É—á–∞–µ–º LLM response
            response = self.llm.invoke(sanitized)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ tool call
            if tool_call := self._extract_tool_call(response):
                result = self.tool_executor.execute(
                    tool_call["name"], 
                    tool_call["args"], 
                    context
                )
                sanitized = f"{sanitized}\nTool result: {result}"
            else:
                return response
            
            iterations += 1
        
        raise SecurityError("Max iterations exceeded")
```

---

## 5. –†–µ–∑—é–º–µ

1. **Validation:** Schema-based parameter validation
2. **Permissions:** Tool-level access control  
3. **Audit:** Log –≤—Å–µ—Ö tool invocations
4. **Limits:** Iteration –∏ resource bounds

---

*AI Security Academy | Track 04: Agentic Security | Module 04.2: Protocols*
