# ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Transformer

> **Ğ£Ñ€Ğ¾Ğ²ĞµĞ½ÑŒ:** Beginner  
> **Ğ’Ñ€ĞµĞ¼Ñ:** 60 Ğ¼Ğ¸Ğ½ÑƒÑ‚  
> **Ğ¢Ñ€ĞµĞº:** 01 â€” ĞÑĞ½Ğ¾Ğ²Ñ‹ AI  
> **ĞœĞ¾Ğ´ÑƒĞ»ÑŒ:** 01.1 â€” Ğ¢Ğ¸Ğ¿Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹  
> **Ğ’ĞµÑ€ÑĞ¸Ñ:** 1.0

---

## Ğ¦ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ

ĞŸĞ¾ÑĞ»Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾ĞºĞ° Ğ²Ñ‹ ÑĞ¼Ğ¾Ğ¶ĞµÑ‚Ğµ:

- [ ] ĞĞ±ÑŠÑÑĞ½Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Transformer
- [ ] ĞĞ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹: encoder, decoder, attention
- [ ] ĞŸĞ¾Ğ½ÑÑ‚ÑŒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° self-attention
- [ ] ĞĞ±ÑŠÑÑĞ½Ğ¸Ñ‚ÑŒ Ñ€Ğ¾Ğ»ÑŒ multi-head attention
- [ ] ĞŸĞ¾Ğ½ÑÑ‚ÑŒ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ positional encoding
- [ ] Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ÑŒ Transformer Ñ Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ (RNN, LSTM)
- [ ] Ğ¡Ğ²ÑĞ·Ğ°Ñ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸

---

## ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ

**Ğ—Ğ½Ğ°Ğ½Ğ¸Ñ:**
- Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ (ÑĞ»Ğ¾Ğ¸, Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸, backpropagation)
- ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ (ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ, Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ)
- ĞÑĞ½Ğ¾Ğ²Ñ‹ Python Ğ¸ PyTorch/TensorFlow

**Ğ£Ñ€Ğ¾ĞºĞ¸:**
- [00. Ğ”Ğ¾Ğ±Ñ€Ğ¾ Ğ¿Ğ¾Ğ¶Ğ°Ğ»Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² AI Security Academy](../../00-introduction/00-welcome.md)

---

## 1. Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞºĞ°

### 1.1 ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´Ğ¾ Transformer

Ğ”Ğ¾ 2017 Ğ³Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ (Ñ‚ĞµĞºÑÑ‚, Ñ€ĞµÑ‡ÑŒ, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ÑĞ´Ñ‹) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ **Ğ ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ĞĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¡ĞµÑ‚Ğ¸ (RNN)** Ğ¸ Ğ¸Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ â€” **LSTM** Ğ¸ **GRU**.

#### ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° RNN

```
Input:   xâ‚ â†’ xâ‚‚ â†’ xâ‚ƒ â†’ xâ‚„ â†’ xâ‚…
          â†“    â†“    â†“    â†“    â†“
RNN:    [hâ‚]â†’[hâ‚‚]â†’[hâ‚ƒ]â†’[hâ‚„]â†’[hâ‚…]
          â†“    â†“    â†“    â†“    â†“
Output:  yâ‚   yâ‚‚   yâ‚ƒ   yâ‚„   yâ‚…
```

ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ hidden state `hâ‚œ` Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾:

```
hâ‚œ = f(hâ‚œâ‚‹â‚, xâ‚œ)
```

#### ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ RNN

| ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ | ĞŸĞ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ñ |
|----------|----------|-------------|
| **ĞŸĞ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°** | Ğ¢Ğ¾ĞºĞµĞ½Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ | ĞĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ½Ğ° GPU |
| **Vanishing gradients** | Ğ“Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‚ÑÑ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ | ĞœĞ¾Ğ´ĞµĞ»ÑŒ Â«Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°ĞµÑ‚Â» Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ |
| **Exploding gradients** | Ğ“Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°ÑÑ‚ÑƒÑ‚ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ | ĞĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ |
| **Long dependencies** | Ğ¡Ğ»Ğ¾Ğ¶Ğ½Ğ¾ ÑĞ²ÑĞ·Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ»Ñ‘ĞºĞ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ | Â«ĞšĞ¾Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¸Ğ´ĞµĞ» Ğ½Ğ° ĞºĞ¾Ğ²Ñ€Ğ¸ĞºĞµ, **Ğ±Ñ‹Ğ»** ÑƒÑÑ‚Ğ°Ğ»Ñ‹Ğ¼Â» â€” ÑĞ²ÑĞ·ÑŒ ĞºĞ¾Ñ‚â†”Ğ±Ñ‹Ğ» |

#### LSTM ĞºĞ°Ğº Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ

**Long Short-Term Memory (1997)** Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ» Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Â«Ğ²Ğ¾Ñ€Ğ¾Ñ‚Â»:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            LSTM Cell            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  forget gate: Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ñ‚ÑŒ        â”‚
â”‚  input gate:  Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑŒ     â”‚
â”‚  output gate: Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ²ĞµÑÑ‚Ğ¸       â”‚
â”‚  cell state:  Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

LSTM Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ñ€ĞµÑˆĞ¸Ğ» Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ vanishing gradient, Ğ½Ğ¾:
- ĞŸĞ¾-Ğ¿Ñ€ĞµĞ¶Ğ½ĞµĞ¼Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°
- Ğ¡Ğ»Ğ¾Ğ¶Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° (Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²)
- ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ (~500-1000 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)

### 1.2 Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ: Â«Attention Is All You NeedÂ»

**Ğ˜ÑĞ½ÑŒ 2017** â€” ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° Google Brain (Vaswani, Shazeer, Parmar Ğ¸ Ğ´Ñ€.) Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ»Ğ° [Â«Attention Is All You NeedÂ»](https://arxiv.org/abs/1706.03762).

> [!NOTE]
> ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ â€” ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ: Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ attention â€” ÑÑ‚Ğ¾ **Ğ²ÑÑ‘**, Ñ‡Ñ‚Ğ¾ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ.

**ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸:**

1. **ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚ĞºĞ°Ğ· Ğ¾Ñ‚ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸** â€” Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ²ÑĞµÑ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²
2. **Self-attention** â€” ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Â«ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ñ‚Â» Ğ½Ğ° Ğ²ÑĞµ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹
3. **Positional encoding** â€” Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸
4. **Multi-head attention** â€” Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Â«Ğ³Ğ¾Ğ»Ğ¾Ğ²Â» attention Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑĞ²ÑĞ·ĞµĞ¹

**Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ (WMT 2014):**

| ĞœĞ¾Ğ´ĞµĞ»ÑŒ | BLEU (ENâ†’DE) | BLEU (ENâ†’FR) | Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ |
|--------|--------------|--------------|----------------|
| GNMT (Google, RNN) | 24.6 | 39.9 | 6 Ğ´Ğ½ĞµĞ¹ |
| ConvS2S (Facebook) | 25.2 | 40.5 | 10 Ğ´Ğ½ĞµĞ¹ |
| **Transformer** | **28.4** | **41.8** | **3.5 Ğ´Ğ½Ñ** |

---

## 2. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Transformer

### 2.1 ĞĞ±Ñ‰Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°

ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Transformer ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· **Encoder** Ğ¸ **Decoder**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TRANSFORMER                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         ENCODER            â”‚            DECODER                 â”‚
â”‚  (Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´)       â”‚  (Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                            â”‚                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Multi-Head          â”‚  â”‚  â”‚  Masked Multi-Head           â”‚ â”‚
â”‚  â”‚  Self-Attention      â”‚  â”‚  â”‚  Self-Attention              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚            â†“               â”‚              â†“                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Add & Norm          â”‚  â”‚  â”‚  Add & Norm                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚            â†“               â”‚              â†“                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Feed-Forward        â”‚  â”‚  â”‚  Multi-Head                  â”‚ â”‚
â”‚  â”‚  Network             â”‚  â”‚  â”‚  Cross-Attention             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  (Ğº Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñƒ encoder)          â”‚ â”‚
â”‚            â†“               â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚              â†“                     â”‚
â”‚  â”‚  Add & Norm          â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  Add & Norm                  â”‚ â”‚
â”‚                            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         Ã— N ÑĞ»Ğ¾Ñ‘Ğ²          â”‚              â†“                     â”‚
â”‚                            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                            â”‚  â”‚  Feed-Forward Network        â”‚ â”‚
â”‚                            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â”‚              â†“                     â”‚
â”‚                            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                            â”‚  â”‚  Add & Norm                  â”‚ â”‚
â”‚                            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â”‚                                    â”‚
â”‚                            â”‚         Ã— N ÑĞ»Ğ¾Ñ‘Ğ²                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Transformer:**
- N = 6 ÑĞ»Ğ¾Ñ‘Ğ² Ğ² encoder Ğ¸ decoder
- d_model = 512 (Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ embedding)
- d_ff = 2048 (Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ feed-forward)
- h = 8 Ğ³Ğ¾Ğ»Ğ¾Ğ²
- d_k = d_v = 64 (Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñƒ)

### 2.2 Encoder

**Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° Encoder:** Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ±Ğ¾Ğ³Ğ°Ñ‚Ğ¾Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ.

```python
# Pseudocode ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Encoder
class TransformerEncoder:
    def __init__(self, n_layers=6, d_model=512, n_heads=8, d_ff=2048):
        self.layers = [EncoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)]
        self.embedding = TokenEmbedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model)
    
    def forward(self, x):
        # 1. Token embeddings + positional encoding
        x = self.embedding(x) + self.pos_encoding(x)
        
        # 2. ĞŸÑ€Ğ¾Ñ…Ğ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ· N ÑĞ»Ğ¾Ñ‘Ğ²
        for layer in self.layers:
            x = layer(x)
        
        return x  # ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ
```

**ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹ Encoder ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚:**

1. **Multi-Head Self-Attention** â€” ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Â«ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ñ‚Â» Ğ½Ğ° Ğ²ÑĞµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ²Ñ…Ğ¾Ğ´Ğ°
2. **Add & Norm** â€” residual connection + layer normalization
3. **Feed-Forward Network** â€” Ğ´Ğ²Ğ° Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ñ Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹
4. **Add & Norm** â€” ĞµÑ‰Ñ‘ Ğ¾Ğ´Ğ¸Ğ½ residual + norm

### 2.3 Decoder

**Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° Decoder:** Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½ Ğ·Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ¼.

**ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Encoder:**

1. **Masked Self-Attention** â€” Ñ‚Ğ¾ĞºĞµĞ½ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Â«ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒÂ» Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ (Ğ½Ğµ Ğ½Ğ° Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ)
2. **Cross-Attention** â€” decoder Â«ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ñ‚Â» Ğ½Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´ encoder

```python
# ĞœĞ°ÑĞºĞ° decoder (causal mask)
# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ´Ğ»Ñ 4 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²:
mask = [
    [1, 0, 0, 0],  # Ñ‚Ğ¾ĞºĞµĞ½ 1 Ğ²Ğ¸Ğ´Ğ¸Ñ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞµĞ±Ñ
    [1, 1, 0, 0],  # Ñ‚Ğ¾ĞºĞµĞ½ 2 Ğ²Ğ¸Ğ´Ğ¸Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ 1, 2
    [1, 1, 1, 0],  # Ñ‚Ğ¾ĞºĞµĞ½ 3 Ğ²Ğ¸Ğ´Ğ¸Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ 1, 2, 3
    [1, 1, 1, 1],  # Ñ‚Ğ¾ĞºĞµĞ½ 4 Ğ²Ğ¸Ğ´Ğ¸Ñ‚ Ğ²ÑĞµ
]
```

---

## 3. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Self-Attention

### 3.1 Ğ˜Ğ½Ñ‚ÑƒĞ¸Ñ†Ğ¸Ñ

**Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ:** ĞšĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ Â«ĞšĞ¾Ñ‚ ÑĞ¸Ğ´ĞµĞ» Ğ½Ğ° ĞºĞ¾Ğ²Ñ€Ğ¸ĞºĞµ, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼Ñƒ Ñ‡Ñ‚Ğ¾ **Ğ¾Ğ½** ÑƒÑÑ‚Ğ°Ğ»Â» Ğ¼ĞµÑÑ‚Ğ¾Ğ¸Ğ¼ĞµĞ½Ğ¸Ğµ Â«Ğ¾Ğ½Â» Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ÑÑ Ğº Â«ĞºĞ¾Ñ‚Â», Ğ° Ğ½Ğµ Ğº Â«ĞºĞ¾Ğ²Ñ€Ğ¸ĞºÂ»?

**ĞÑ‚Ğ²ĞµÑ‚:** Self-attention Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ñƒ Â«Ğ¿Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒÂ» Ğ½Ğ° Ğ²ÑĞµ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¸Ñ… Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ.

```
         The   cat   sat   on   the   mat   because   it   was   tired
    it:  0.05  0.60  0.05  0.02  0.03  0.15   0.02   0.00  0.03   0.05
                â†‘                      â†‘
           high weight            medium weight
           (cat â€” subject)        (mat â€” possible reference)
```

### 3.2 Query, Key, Value

Self-attention Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ°:

- **Query (Q)** â€” Â«Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÂ»: Ñ‡Ñ‚Ğ¾ Ñ Ğ¸Ñ‰Ñƒ?
- **Key (K)** â€” Â«ĞºĞ»ÑÑ‡Â»: Ñ‡Ñ‚Ğ¾ Ñƒ Ğ¼ĞµĞ½Ñ ĞµÑÑ‚ÑŒ?
- **Value (V)** â€” Â«Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ĞµÂ»: Ñ‡Ñ‚Ğ¾ Ñ Ğ²ĞµÑ€Ğ½Ñƒ?

```python
# Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Q, K, V
Q = X @ W_Q  # [seq_len, d_model] @ [d_model, d_k] = [seq_len, d_k]
K = X @ W_K  # [seq_len, d_model] @ [d_model, d_k] = [seq_len, d_k]
V = X @ W_V  # [seq_len, d_model] @ [d_model, d_v] = [seq_len, d_v]
```

### 3.3 Scaled Dot-Product Attention

**Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ°:**

```
Attention(Q, K, V) = softmax(Q Ã— K^T / âˆšd_k) Ã— V
```

**ĞŸĞ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğµ:**

```python
import torch
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Q: [batch, seq_len, d_k]
    K: [batch, seq_len, d_k]
    V: [batch, seq_len, d_v]
    """
    d_k = Q.size(-1)
    
    # Ğ¨Ğ°Ğ³ 1: Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Â«ÑÑ‹Ñ€Ñ‹ĞµÂ» attention scores
    # Q @ K^T = [batch, seq_len, d_k] @ [batch, d_k, seq_len] = [batch, seq_len, seq_len]
    scores = torch.matmul(Q, K.transpose(-2, -1))
    
    # Ğ¨Ğ°Ğ³ 2: ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ Ğ½Ğ° âˆšd_k
    # Ğ‘ĞµĞ· Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ d_k dot products ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸,
    # softmax Ğ½Ğ°ÑÑ‹Ñ‰Ğ°ĞµÑ‚ÑÑ, Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¸ÑÑ‡ĞµĞ·Ğ°ÑÑ‚
    scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
    
    # Ğ¨Ğ°Ğ³ 3: ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ¼Ğ°ÑĞºÑƒ (Ğ´Ğ»Ñ decoder)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float('-inf'))
    
    # Ğ¨Ğ°Ğ³ 4: Softmax â€” Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµĞ¼ Ğ² Ğ²ĞµÑĞ° (ÑÑƒĞ¼Ğ¼Ğ° = 1)
    attention_weights = F.softmax(scores, dim=-1)
    
    # Ğ¨Ğ°Ğ³ 5: Ğ’Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ°Ñ ÑÑƒĞ¼Ğ¼Ğ° values
    output = torch.matmul(attention_weights, V)
    
    return output, attention_weights
```

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸:**

```
Input: "The cat sat"

Q (Ñ‚Ğ¾ĞºĞµĞ½ "sat" ÑĞ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ĞµÑ‚):  [0.2, 0.5, 0.1, ...]
K (Ğ²ÑĞµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‚):
  - "The": [0.1, 0.3, 0.2, ...]
  - "cat": [0.3, 0.4, 0.1, ...]
  - "sat": [0.2, 0.5, 0.1, ...]

Scores (Q @ K^T):
  - "sat" â†’ "The": 0.2Ã—0.1 + 0.5Ã—0.3 + ... = 0.17
  - "sat" â†’ "cat": 0.2Ã—0.3 + 0.5Ã—0.4 + ... = 0.26
  - "sat" â†’ "sat": 0.2Ã—0.2 + 0.5Ã—0.5 + ... = 0.29

ĞŸĞ¾ÑĞ»Ğµ softmax:
  - "sat" â†’ "The": 0.28
  - "sat" â†’ "cat": 0.34
  - "sat" â†’ "sat": 0.38
```

### 3.4 ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ âˆšd_k?

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°:** ĞŸÑ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ d_k (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, 64) dot products ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸:

```
Ğ•ÑĞ»Ğ¸ q_i, k_i ~ N(0, 1), Ñ‚Ğ¾ dot product ~ N(0, d_k)
Ğ”Ğ»Ñ d_k = 64: ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ = 8
```

Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ â†’ softmax Ğ´Ğ°Ñ‘Ñ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ one-hot â†’ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¸ÑÑ‡ĞµĞ·Ğ°ÑÑ‚.

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ:** Ğ”ĞµĞ»Ğ¸Ğ¼ Ğ½Ğ° âˆšd_k Ğ´Ğ»Ñ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ° Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ â‰ˆ 1.

---

## 4. Multi-Head Attention

### 4.1 Ğ—Ğ°Ñ‡ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Â«Ğ³Ğ¾Ğ»Ğ¾Ğ²Â»?

ĞĞ´Ğ½Ğ° Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ° attention Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ¸Ğ½ Ñ‚Ğ¸Ğ¿ ÑĞ²ÑĞ·ĞµĞ¹. **Multi-head Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾:**

| Ğ“Ğ¾Ğ»Ğ¾Ğ²Ğ° | Ğ§Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ |
|--------|----------------------|
| Head 1 | Ğ¡Ğ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²ÑĞ·Ğ¸ (Ğ¿Ğ¾Ğ´Ğ»ĞµĞ¶Ğ°Ñ‰ĞµĞµ-ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾Ğµ) |
| Head 2 | Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²ÑĞ·Ğ¸ (ÑĞ»Ğ¾Ğ²Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞ¼Ñ‹) |
| Head 3 | ĞŸĞ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ (ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ²Ğ°) |
| Head 4 | ĞĞ½Ğ°Ñ„Ğ¾Ñ€Ğ° (Ğ¼ĞµÑÑ‚Ğ¾Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ â†’ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ) |
| ... | ... |

### 4.2 ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° Multi-Head Attention

```python
class MultiHeadAttention(torch.nn.Module):
    def __init__(self, d_model=512, n_heads=8):
        super().__init__()
        self.n_heads = n_heads
        self.d_k = d_model // n_heads  # 512 / 8 = 64
        
        # ĞŸÑ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹
        self.W_Q = torch.nn.Linear(d_model, d_model)
        self.W_K = torch.nn.Linear(d_model, d_model)
        self.W_V = torch.nn.Linear(d_model, d_model)
        
        # Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ
        self.W_O = torch.nn.Linear(d_model, d_model)
    
    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)
        
        # 1. Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸
        Q = self.W_Q(Q)  # [batch, seq_len, d_model]
        K = self.W_K(K)
        V = self.W_V(V)
        
        # 2. Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞµĞ¼ Ğ½Ğ° Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹
        # [batch, seq_len, d_model] â†’ [batch, n_heads, seq_len, d_k]
        Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # 3. Attention Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾
        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)
        
        # 4. ĞšĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ¸Ñ€ÑƒĞµĞ¼ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹
        # [batch, n_heads, seq_len, d_k] â†’ [batch, seq_len, d_model]
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(batch_size, -1, self.n_heads * self.d_k)
        
        # 5. Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ
        output = self.W_O(attn_output)
        
        return output, attn_weights
```

### 4.3 Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Multi-Head

```
Input X [seq_len, d_model=512]
         â†“
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â†“    â†“    â†“   ... (8 heads)
  [Qâ‚] [Qâ‚‚] [Qâ‚ƒ]
  [Kâ‚] [Kâ‚‚] [Kâ‚ƒ]
  [Vâ‚] [Vâ‚‚] [Vâ‚ƒ]
    â†“    â†“    â†“
[Attnâ‚][Attnâ‚‚][Attnâ‚ƒ] ... [Attnâ‚ˆ]
 [64]   [64]   [64]        [64]
    â†“    â†“    â†“             â†“
    â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
         Concat [512]
              â†“
           W_O [512]
              â†“
         Output [512]
```

---

## 5. Positional Encoding

### 5.1 ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°: Transformer Ğ½Ğµ Ğ·Ğ½Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ

Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ RNN, Ğ³Ğ´Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ½ĞµÑĞ²Ğ½Ğ¾ Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Transformer Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²ÑĞµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾. **Ğ‘ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Â«cat satÂ» Ğ¸ Â«sat catÂ» Ğ±Ñ‹Ğ»Ğ¸ Ğ±Ñ‹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹.**

### 5.2 Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ: Ğ¡Ğ¸Ğ½ÑƒÑĞ¾Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ

ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½ÑƒÑĞ¾Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸:

```python
def positional_encoding(seq_len, d_model):
    """
    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
    """
    position = torch.arange(seq_len).unsqueeze(1)  # [seq_len, 1]
    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
    
    pe = torch.zeros(seq_len, d_model)
    pe[:, 0::2] = torch.sin(position * div_term)  # Ñ‡Ñ‘Ñ‚Ğ½Ñ‹Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑÑ‹
    pe[:, 1::2] = torch.cos(position * div_term)  # Ğ½ĞµÑ‡Ñ‘Ñ‚Ğ½Ñ‹Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑÑ‹
    
    return pe
```

### 5.3 ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ ÑĞ¸Ğ½ÑƒÑĞ¾Ğ¸Ğ´Ñ‹?

1. **Ğ£Ğ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ:** ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¸Ğ¼ĞµĞµÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹
2. **ĞÑ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸:** PE(pos+k) Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ÑŒ ĞºĞ°Ğº Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ñ‚ PE(pos)
3. **Ğ­ĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ:** Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ğ¸Ğ½Ğ½ĞµĞµ, Ñ‡ĞµĞ¼ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸

```
Position 0:  [sin(0), cos(0), sin(0), cos(0), ...]  = [0, 1, 0, 1, ...]
Position 1:  [sin(1), cos(1), sin(0.001), cos(0.001), ...]
Position 2:  [sin(2), cos(2), sin(0.002), cos(0.002), ...]
...
```

### 5.4 Ğ¡Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñ‹

| ĞœĞµÑ‚Ğ¾Ğ´ | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ | Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ² |
|-------|----------|----------------|
| Learned Positional Embeddings | ĞĞ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ | BERT, GPT-2 |
| RoPE (Rotary Position Embedding) | Ğ’Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚Ğ¸ | LLaMA, Mistral |
| ALiBi | Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¹ attention bias | BLOOM |
| Relative Position Encodings | ĞÑ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ | T5 |

---

## 6. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹

### 6.1 Feed-Forward Network

ĞŸĞ¾ÑĞ»Ğµ attention Ğ¸Ğ´Ñ‘Ñ‚ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ°Ñ feed-forward ÑĞµÑ‚ÑŒ:

```python
class FeedForward(torch.nn.Module):
    def __init__(self, d_model=512, d_ff=2048, dropout=0.1):
        super().__init__()
        self.linear1 = torch.nn.Linear(d_model, d_ff)
        self.linear2 = torch.nn.Linear(d_ff, d_model)
        self.dropout = torch.nn.Dropout(dropout)
    
    def forward(self, x):
        # FFN(x) = max(0, xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚
        x = self.linear1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        return x
```

**Ğ—Ğ°Ñ‡ĞµĞ¼ FFN?**
- Attention â€” Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ°Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ (Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ°Ñ ÑÑƒĞ¼Ğ¼Ğ°)
- FFN Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ÑÑ‚ÑŒ
- Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

### 6.2 Layer Normalization

```python
# Layer Norm Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¿Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ (features)
layer_norm = torch.nn.LayerNorm(d_model)
output = layer_norm(x)
```

**Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ°:**

```
LayerNorm(x) = Î³ Ã— (x - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²
```

Ğ“Ğ´Ğµ:
- Î¼, Ïƒ â€” ÑÑ€ĞµĞ´Ğ½ĞµĞµ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ features
- Î³, Î² â€” Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹

### 6.3 Residual Connections

```python
# Ğ’Ğ¼ĞµÑÑ‚Ğ¾: output = sublayer(x)
# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼: output = x + sublayer(x)

output = x + self.attention(x)
output = self.layer_norm(output)
```

**Ğ—Ğ°Ñ‡ĞµĞ¼?**
- Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… ÑĞµÑ‚ĞµĞ¹
- ĞŸĞ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼ Â«Ñ‚ĞµÑ‡ÑŒÂ» Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ
- Skip connections Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ

---

## 7. Transformer Ğ¸ AI Security

### 7.1 ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ â†’ Ğ£ÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸

| ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ | ĞŸĞ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ |
|-------------|--------------------------|
| **Self-attention Ğ½Ğ° Ğ²ĞµÑÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚** | Indirect injection: Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğµ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ²ÑÑ‘ |
| **Autoregressive generation** | ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… â†’ injection Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµĞ½ |
| **Positional encoding** | Position attacks: Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ |
| **Attention weights** | Interpretability â†’ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ, Ğ½Ğ° Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Â«ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ñ‚Â» |

### 7.2 SENTINEL Engines Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Transformer

SENTINEL Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ engines Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Transformer:

```python
from sentinel import scan  # Public API

# ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² attention
attention_detector = AttentionPatternDetector()
result = attention_detector.analyze(
    attention_weights=model.get_attention_weights(),
    prompt=user_input
)

if result.anomalous_patterns:
    print(f"ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ attention: {result.patterns}")

# Hidden state forensics
forensics = HiddenStateForensics()
analysis = forensics.analyze(
    hidden_states=model.get_hidden_states(),
    expected_behavior="helpful_assistant"
)
```

### 7.3 Ğ¡Ğ²ÑĞ·ÑŒ Ñ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼Ğ¸

| ĞÑ‚Ğ°ĞºĞ° | Ğ­ĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ |
|-------|---------------------------|
| Prompt Injection | Self-attention: Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ attention weights |
| Jailbreak | FFN: Ğ¾Ğ±Ñ…Ğ¾Ğ´ Ğ²Ñ‹ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… safety representations |
| Adversarial Suffixes | Positional encoding: ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ trigger |
| Context Hijacking | Long context attention: Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ¼ |

---

## 8. ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒĞ¿Ñ€Ğ°Ğ¶Ğ½ĞµĞ½Ğ¸Ñ

### Ğ£Ğ¿Ñ€Ğ°Ğ¶Ğ½ĞµĞ½Ğ¸Ğµ 1: Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Attention

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ BertViz Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ attention weights:

```python
from bertviz import head_view, model_view
from transformers import AutoTokenizer, AutoModel

# Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased", output_attentions=True)

# ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ
sentence = "The cat sat on the mat because it was tired"
inputs = tokenizer(sentence, return_tensors="pt")
outputs = model(**inputs)

# Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
head_view(outputs.attentions, tokens)
```

**Ğ’Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°:**
1. ĞšĞ°ĞºĞ¸Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‚ Â«itÂ» Ñ Â«catÂ»?
2. ĞšĞ°Ğº Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ attention Ğ¾Ñ‚ ÑĞ»Ğ¾Ñ Ğº ÑĞ»Ğ¾Ñ?
3. Ğ•ÑÑ‚ÑŒ Ğ»Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸ÑĞµ?

<details>
<summary>ğŸ’¡ ĞŸĞ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°</summary>

ĞĞ±Ñ€Ğ°Ñ‚Ğ¸Ñ‚Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ² ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… (4-8). Ğ Ğ°Ğ½Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ñ…, Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğµ â€” Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ÑÑ….

</details>

### Ğ£Ğ¿Ñ€Ğ°Ğ¶Ğ½ĞµĞ½Ğ¸Ğµ 2: Ğ Ğ°ÑÑ‡Ñ‘Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹

Ğ”Ğ»Ñ Transformer Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸:
- d_model = 768
- n_heads = 12
- n_layers = 12
- vocab_size = 30,000

Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ğ°Ğ¹Ñ‚Ğµ:

1. Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ d_k Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹
2. ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ±Ğ»Ğ¾ĞºĞµ Multi-Head Attention
3. ĞĞ±Ñ‰ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾)

<details>
<summary>âœ… Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ</summary>

1. **d_k = d_model / n_heads = 768 / 12 = 64**

2. **ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Multi-Head Attention:**
   - W_Q: 768 Ã— 768 = 589,824
   - W_K: 768 Ã— 768 = 589,824
   - W_V: 768 Ã— 768 = 589,824
   - W_O: 768 Ã— 768 = 589,824
   - **Ğ˜Ñ‚Ğ¾Ğ³Ğ¾: 2,359,296 Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²**

3. **ĞĞ±Ñ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚:**
   - Token embeddings: 30,000 Ã— 768 â‰ˆ 23M
   - Position embeddings: 512 Ã— 768 â‰ˆ 0.4M
   - ĞĞ° Ğ¾Ğ´Ğ¸Ğ½ ÑĞ»Ğ¾Ğ¹: ~7M (attention + FFN + norms)
   - 12 ÑĞ»Ğ¾Ñ‘Ğ²: 12 Ã— 7M â‰ˆ 84M
   - **Ğ˜Ñ‚Ğ¾Ğ³Ğ¾: ~110M Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²** (BERT-base)

</details>

### Ğ£Ğ¿Ñ€Ğ°Ğ¶Ğ½ĞµĞ½Ğ¸Ğµ 3: Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Scaled Dot-Product Attention

Ğ ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞ¹Ñ‚Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ attention Ñ Ğ½ÑƒĞ»Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ ĞµÑ‘:

```python
import torch

def my_attention(Q, K, V, mask=None):
    """
    Ğ ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞ¹Ñ‚Ğµ scaled dot-product attention.
    
    Args:
        Q: [batch, seq_len, d_k]
        K: [batch, seq_len, d_k]
        V: [batch, seq_len, d_v]
        mask: [seq_len, seq_len] or None
    
    Returns:
        output: [batch, seq_len, d_v]
        weights: [batch, seq_len, seq_len]
    """
    # Ğ’Ğ°Ñˆ ĞºĞ¾Ğ´ Ğ·Ğ´ĞµÑÑŒ
    pass

# Ğ¢ĞµÑÑ‚
Q = torch.randn(2, 4, 64)  # batch=2, seq_len=4, d_k=64
K = torch.randn(2, 4, 64)
V = torch.randn(2, 4, 64)

output, weights = my_attention(Q, K, V)
print(f"Output shape: {output.shape}")  # Ğ”Ğ¾Ğ»Ğ¶Ğ½Ğ¾ Ğ±Ñ‹Ñ‚ÑŒ [2, 4, 64]
print(f"Weights shape: {weights.shape}")  # Ğ”Ğ¾Ğ»Ğ¶Ğ½Ğ¾ Ğ±Ñ‹Ñ‚ÑŒ [2, 4, 4]
print(f"Weights sum per row: {weights.sum(dim=-1)}")  # Ğ”Ğ¾Ğ»Ğ¶Ğ½Ğ¾ Ğ±Ñ‹Ñ‚ÑŒ ~1.0
```

---

## 9. Quiz Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹

### Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ 1

ĞšĞ°ĞºÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ RNN Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Transformer?

- [ ] A) ĞĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
- [ ] B) Ğ¡Ğ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
- [x] C) ĞŸĞ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¸ vanishing gradients
- [ ] D) Ğ¡Ğ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

### Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ 2

Ğ”Ğ»Ñ Ñ‡ĞµĞ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ âˆšd_k Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ attention?

- [ ] A) Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹
- [x] B) ĞŸÑ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ dot product Ğ¸ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸Ñ softmax
- [ ] C) Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
- [ ] D) Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ÑÑ‚Ğ¸

### Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ 3

Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ Multi-Head Attention?

- [ ] A) Attention Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸
- [x] B) ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² attention Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸ÑĞ¼Ğ¸
- [ ] C) Attention Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑĞ»Ğ¾Ğµ
- [ ] D) Attention Ğ¼ĞµĞ¶Ğ´Ñƒ encoder Ğ¸ decoder

### Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ 4

Ğ—Ğ°Ñ‡ĞµĞ¼ Ğ½ÑƒĞ¶ĞµĞ½ positional encoding Ğ² Transformer?

- [x] A) Transformer Ğ½Ğµ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ñ Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸
- [ ] B) Ğ”Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
- [ ] C) Ğ”Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
- [ ] D) Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸

### Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ 5

ĞšĞ°ĞºĞ¾Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Decoder Ğ¾Ñ‚ Encoder?

- [ ] A) Decoder Ğ¸Ğ¼ĞµĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑĞ»Ğ¾Ñ‘Ğ²
- [ ] B) Decoder Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ñ€ÑƒĞ³ÑƒÑ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ
- [x] C) Decoder Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ masked attention, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Â«Ğ¿Ğ¾Ğ´Ğ³Ğ»ÑĞ´Ñ‹Ğ²Ğ°Ñ‚ÑŒÂ» Ğ² Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹
- [ ] D) Decoder Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ positional encoding

---

## 10. Ğ¡Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹

### SENTINEL Engines

| Engine | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ | Ğ£Ñ€Ğ¾Ğº |
|--------|----------|------|
| `AttentionPatternDetector` | ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² attention Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ | [Advanced Detection](../../06-advanced-detection/) |
| `HiddenStateForensics` | Ğ¤Ğ¾Ñ€ĞµĞ½Ğ·Ğ¸ĞºĞ° hidden states Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ | [Advanced Detection](../../06-advanced-detection/) |
| `TokenFlowAnalyzer` | ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ | [Advanced Detection](../../06-advanced-detection/) |

### Ğ’Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹

- [Attention Is All You Need (Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ)](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer (Jay Alammar)](https://jalammar.github.io/illustrated-transformer/)
- [Harvard NLP: The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [Lilian Weng: The Transformer Family](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)

### Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾

- [3Blue1Brown: Attention in Transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc)
- [Andrej Karpathy: Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)

---

## 11. Ğ ĞµĞ·ÑĞ¼Ğµ

Ğ’ ÑÑ‚Ğ¾Ğ¼ ÑƒÑ€Ğ¾ĞºĞµ Ğ¼Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸:

1. **Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ:** ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ RNN â†’ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Transformer (2017)
2. **ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°:** Encoder-Decoder ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ñ N ÑĞ»Ğ¾ÑĞ¼Ğ¸
3. **Self-Attention:** Q, K, V Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸, scaled dot-product, softmax
4. **Multi-Head Attention:** ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑĞ²ÑĞ·ĞµĞ¹
5. **Positional Encoding:** Ğ¡Ğ¸Ğ½ÑƒÑĞ¾Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸
6. **Security:** Ğ¡Ğ²ÑĞ·ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸, SENTINEL engines

**ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´:** Transformer ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM. ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞµĞ³Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‚Ğ°Ğº Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ AI-ÑĞ¸ÑÑ‚ĞµĞ¼.

---

## Ğ¡Ğ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ ÑƒÑ€Ğ¾Ğº

â†’ [02. Encoder-Only Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: BERT, RoBERTa](02-encoder-only.md)

---

*AI Security Academy | Ğ¢Ñ€ĞµĞº 01: ĞÑĞ½Ğ¾Ğ²Ñ‹ AI | ĞœĞ¾Ğ´ÑƒĞ»ÑŒ 01.1: Ğ¢Ğ¸Ğ¿Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹*
