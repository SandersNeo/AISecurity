# Encoder-Only –º–æ–¥–µ–ª–∏: BERT, RoBERTa

> **–£—Ä–æ–≤–µ–Ω—å:** Beginner  
> **–í—Ä–µ–º—è:** 55 –º–∏–Ω—É—Ç  
> **–¢—Ä–µ–∫:** 01 ‚Äî –û—Å–Ω–æ–≤—ã AI  
> **–ú–æ–¥—É–ª—å:** 01.1 ‚Äî –¢–∏–ø—ã –º–æ–¥–µ–ª–µ–π  
> **–í–µ—Ä—Å–∏—è:** 1.0

---

## –¶–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —ç—Ç–æ–≥–æ —É—Ä–æ–∫–∞ –≤—ã —Å–º–æ–∂–µ—Ç–µ:

- [ ] –û–±—ä—è—Å–Ω–∏—Ç—å —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É encoder-only –∏ full Transformer
- [ ] –ü–æ–Ω—è—Ç—å –∑–∞–¥–∞—á—É Masked Language Modeling (MLM)
- [ ] –û–ø–∏—Å–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É BERT –∏ –µ—ë –≤–∞—Ä–∏–∞–Ω—Ç—ã
- [ ] –ü–æ–Ω—è—Ç—å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ RoBERTa –Ω–∞–¥ BERT
- [ ] –ü—Ä–∏–º–µ–Ω—è—Ç—å encoder –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ NER –∑–∞–¥–∞—á
- [ ] –°–≤—è–∑–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Å —É—è–∑–≤–∏–º–æ—Å—Ç—è–º–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

---

## –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

**–£—Ä–æ–∫–∏:**
- [01. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Transformer](01-transformers.md) ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ

**–ó–Ω–∞–Ω–∏—è:**
- –ú–µ—Ö–∞–Ω–∏–∑–º self-attention
- Multi-head attention
- Positional encoding

---

## 1. Encoder vs Full Transformer

### 1.1 –ù–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ: Full Transformer

–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π Transformer –∏–º–µ–µ—Ç –¥–≤–µ —á–∞—Å—Ç–∏:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              TRANSFORMER                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ      ENCODER        ‚îÇ      DECODER      ‚îÇ
‚îÇ (–ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤—Ö–æ–¥–∞)   ‚îÇ (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã—Ö–æ–¥–∞)‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Self-Attention     ‚îÇ  Masked Self-Attn ‚îÇ
‚îÇ  Feed-Forward       ‚îÇ  Cross-Attention  ‚îÇ
‚îÇ  √ó N —Å–ª–æ—ë–≤          ‚îÇ  Feed-Forward     ‚îÇ
‚îÇ                     ‚îÇ  √ó N —Å–ª–æ—ë–≤        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.2 Encoder-Only: –¢–æ–ª—å–∫–æ –ø–æ–Ω–∏–º–∞–Ω–∏–µ

**Encoder-only –º–æ–¥–µ–ª–∏** –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç–æ–ª—å–∫–æ –ª–µ–≤—É—é —á–∞—Å—Ç—å ‚Äî Encoder:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    ENCODER-ONLY     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Self-Attention     ‚îÇ  ‚Üê Bidirectional!
‚îÇ  (–≤–∏–¥–∏—Ç –í–°–ï —Ç–æ–∫–µ–Ω—ã) ‚îÇ
‚îÇ  Feed-Forward       ‚îÇ
‚îÇ  √ó N —Å–ª–æ—ë–≤          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
   Representations
   (–¥–ª—è downstream –∑–∞–¥–∞—á)
```

**–ö–ª—é—á–µ–≤–æ–µ –æ—Ç–ª–∏—á–∏–µ:** Encoder –≤–∏–¥–∏—Ç **–≤—Å–µ —Ç–æ–∫–µ–Ω—ã —Å—Ä–∞–∑—É** (bidirectional attention), –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ.

### 1.3 –ö–æ–≥–¥–∞ —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å?

| –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ | –ó–∞–¥–∞—á–∏ | –ü—Ä–∏–º–µ—Ä—ã –º–æ–¥–µ–ª–µ–π |
|-------------|--------|-----------------|
| **Encoder-only** | –ü–æ–Ω–∏–º–∞–Ω–∏–µ, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, NER, –ø–æ–∏—Å–∫ | BERT, RoBERTa, DistilBERT |
| **Decoder-only** | –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ | GPT, LLaMA, Claude |
| **Encoder-Decoder** | Seq2seq: –ø–µ—Ä–µ–≤–æ–¥, —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è | T5, BART, mT5 |

---

## 2. BERT: Bidirectional Encoder Representations from Transformers

### 2.1 –ò—Å—Ç–æ—Ä–∏—è

**–û–∫—Ç—è–±—Ä—å 2018** ‚Äî Google AI –ø—É–±–ª–∏–∫—É–µ—Ç [¬´BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding¬ª](https://arxiv.org/abs/1810.04805).

> [!NOTE]
> BERT —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–ª NLP, –ø–æ–∫–∞–∑–∞–≤, —á—Ç–æ –ø–∞—Ä–∞–¥–∏–≥–º–∞ **pre-training + fine-tuning** –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏.

**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ —Ä–µ–ª–∏–∑–µ:**

| Benchmark | Previous SOTA | BERT | –£–ª—É—á—à–µ–Ω–∏–µ |
|-----------|---------------|------|-----------|
| GLUE | 72.8 | **80.5** | +7.7 |
| SQuAD 1.1 F1 | 91.2 | **93.2** | +2.0 |
| SQuAD 2.0 F1 | 66.3 | **83.1** | +16.8 |

### 2.2 –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ BERT

```
         Input: "[CLS] The cat sat on the mat [SEP]"
                           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Token Embeddings                          ‚îÇ
‚îÇ  [CLS]   The    cat    sat    on    the    mat   [SEP]      ‚îÇ
‚îÇ   E‚ÇÅ     E‚ÇÇ     E‚ÇÉ     E‚ÇÑ     E‚ÇÖ    E‚ÇÜ     E‚Çá    E‚Çà         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           +
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Segment Embeddings                         ‚îÇ
‚îÇ   E‚Çê     E‚Çê     E‚Çê     E‚Çê     E‚Çê    E‚Çê     E‚Çê    E‚Çê         ‚îÇ
‚îÇ        (Sentence A –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           +
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Position Embeddings                         ‚îÇ
‚îÇ   E‚ÇÄ     E‚ÇÅ     E‚ÇÇ     E‚ÇÉ     E‚ÇÑ    E‚ÇÖ     E‚ÇÜ    E‚Çá         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    BERT Encoder                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Multi-Head Self-Attention (Bidirectional)             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Add & Norm                                            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Feed-Forward                                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Add & Norm                                            ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                      √ó 12/24 —Å–ª–æ—ë–≤                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
         Output: –ö–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
```

**–†–∞–∑–º–µ—Ä—ã –º–æ–¥–µ–ª–∏:**

| –ú–æ–¥–µ–ª—å | –°–ª–æ–∏ | Hidden | Heads | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã |
|--------|------|--------|-------|-----------|
| BERT-base | 12 | 768 | 12 | 110M |
| BERT-large | 24 | 1024 | 16 | 340M |

### 2.3 –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã

| –¢–æ–∫–µ–Ω | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ |
|-------|------------|
| `[CLS]` | Classification —Ç–æ–∫–µ–Ω ‚Äî –µ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ |
| `[SEP]` | Separator ‚Äî —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è |
| `[MASK]` | Masked —Ç–æ–∫–µ–Ω –¥–ª—è MLM |
| `[PAD]` | Padding –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –¥–ª–∏–Ω—ã |
| `[UNK]` | Unknown ‚Äî –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω |

---

## 3. –ó–∞–¥–∞—á–∏ Pre-training BERT

### 3.1 Masked Language Modeling (MLM)

**–ò–¥–µ—è:** –°–∫—Ä—ã—Ç—å (–∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞—Ç—å) —Å–ª—É—á–∞–π–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –∏—Ö.

```
Input:   "The cat [MASK] on the [MASK]"
Target:  –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å "sat" –∏ "mat"
```

**–ü—Ä–æ—Ü–µ–¥—É—Ä–∞ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è (15% —Ç–æ–∫–µ–Ω–æ–≤):**

```python
def mask_tokens(tokens, tokenizer, mlm_probability=0.15):
    """
    –î–ª—è 15% —Ç–æ–∫–µ–Ω–æ–≤:
    - 80%: –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ [MASK]
    - 10%: –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã–π —Ç–æ–∫–µ–Ω
    - 10%: –æ—Å—Ç–∞–≤–ª—è–µ–º –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
    """
    labels = tokens.clone()
    probability_matrix = torch.full(labels.shape, mlm_probability)
    
    # –ù–µ –º–∞—Å–∫–∏—Ä—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
    special_tokens_mask = tokenizer.get_special_tokens_mask(tokens.tolist())
    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), 0.0)
    
    masked_indices = torch.bernoulli(probability_matrix).bool()
    labels[~masked_indices] = -100  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –Ω–µ-masked –¥–ª—è loss
    
    # 80% –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ [MASK]
    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices
    tokens[indices_replaced] = tokenizer.convert_tokens_to_ids('[MASK]')
    
    # 10% –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã–π —Ç–æ–∫–µ–Ω
    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced
    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)
    tokens[indices_random] = random_words[indices_random]
    
    # 10% –æ—Å—Ç–∞–≤–ª—è–µ–º –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
    # (—É–∂–µ —Å–¥–µ–ª–∞–Ω–æ ‚Äî –æ—Å—Ç–∞–≤—à–∏–µ—Å—è masked_indices –Ω–µ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã)
    
    return tokens, labels
```

**–ü–æ—á–µ–º—É 80/10/10?**

- **80% [MASK]:** –û—Å–Ω–æ–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
- **10% random:** –ó–∞—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å –Ω–µ —Å–ª–µ–ø–æ –¥–æ–≤–µ—Ä—è—Ç—å –Ω–µ-masked —Ç–æ–∫–µ–Ω–∞–º
- **10% unchanged:** –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –º–µ–∂–¥—É pre-training –∏ fine-tuning (–≤ fine-tuning –Ω–µ—Ç [MASK])

### 3.2 Next Sentence Prediction (NSP)

**–ò–¥–µ—è:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, —Å–ª–µ–¥—É–µ—Ç –ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ B –∑–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º A.

```
–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è –ø–∞—Ä–∞ (50%):
  [CLS] The cat sat on the mat [SEP] It was very comfortable [SEP]
  Label: IsNext

–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è –ø–∞—Ä–∞ (50%):
  [CLS] The cat sat on the mat [SEP] Python is a programming language [SEP]
  Label: NotNext
```

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**

```python
class BertForPreTraining(torch.nn.Module):
    def __init__(self, bert_model, vocab_size, hidden_size):
        super().__init__()
        self.bert = bert_model
        
        # MLM head
        self.mlm_head = torch.nn.Linear(hidden_size, vocab_size)
        
        # NSP head (–±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞ [CLS] —Ç–æ–∫–µ–Ω–µ)
        self.nsp_head = torch.nn.Linear(hidden_size, 2)
    
    def forward(self, input_ids, segment_ids, attention_mask):
        # BERT encoding
        outputs = self.bert(input_ids, segment_ids, attention_mask)
        sequence_output = outputs.last_hidden_state  # [batch, seq_len, hidden]
        pooled_output = outputs.pooler_output  # [batch, hidden] ([CLS] representation)
        
        # MLM predictions
        mlm_logits = self.mlm_head(sequence_output)  # [batch, seq_len, vocab_size]
        
        # NSP predictions
        nsp_logits = self.nsp_head(pooled_output)  # [batch, 2]
        
        return mlm_logits, nsp_logits
```

> [!WARNING]
> –ë–æ–ª–µ–µ –ø–æ–∑–¥–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (RoBERTa) –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ NSP **–Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç** –∏ –º–æ–∂–µ—Ç –¥–∞–∂–µ –≤—Ä–µ–¥–∏—Ç—å. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –æ–±—ã—á–Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç NSP.

---

## 4. Fine-tuning BERT

### 4.1 –°–º–µ–Ω–∞ –ø–∞—Ä–∞–¥–∏–≥–º—ã: Pre-train + Fine-tune

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        PRE-TRAINING (–æ–¥–∏–Ω —Ä–∞–∑)                          ‚îÇ
‚îÇ  –û–≥—Ä–æ–º–Ω—ã–π –∫–æ—Ä–ø—É—Å (Wikipedia + BookCorpus) ‚Üí –≤–µ—Å–∞ BERT                  ‚îÇ
‚îÇ  –í—Ä–µ–º—è: –Ω–µ–¥–µ–ª–∏ –Ω–∞ TPU –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö                                         ‚îÇ
‚îÇ  –ö—Ç–æ –¥–µ–ª–∞–µ—Ç: Google, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏–∏                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚Üì
                            –ü—É–±–ª–∏—á–Ω—ã–µ –≤–µ—Å–∞
                                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     FINE-TUNING (–¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏)                     ‚îÇ
‚îÇ  Task-specific –¥–∞–Ω–Ω—ã–µ ‚Üí –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å                           ‚îÇ
‚îÇ  –í—Ä–µ–º—è: –º–∏–Ω—É—Ç—ã-—á–∞—Å—ã –Ω–∞ GPU                                               ‚îÇ
‚îÇ  –ö—Ç–æ –¥–µ–ª–∞–µ—Ç: –ª—é–±–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 4.2 –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞

```python
from transformers import BertForSequenceClassification, BertTokenizer
import torch

# –ó–∞–≥—Ä—É–∂–∞–µ–º pre-trained –º–æ–¥–µ–ª—å —Å classification head
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2  # –±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
text = "This movie is absolutely fantastic!"
inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)

# Inference
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    print(f"Prediction: {'Positive' if predictions.item() == 1 else 'Negative'}")
```

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:**

```
Input ‚Üí BERT Encoder ‚Üí [CLS] representation ‚Üí Linear ‚Üí Softmax ‚Üí Classes
                              ‚Üë
                        [batch, hidden_size]
                              ‚Üì
                        [batch, num_classes]
```

### 4.3 Named Entity Recognition (NER)

```python
from transformers import BertForTokenClassification

# NER –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –í–°–ï —Ç–æ–∫–µ–Ω—ã, –Ω–µ —Ç–æ–ª—å–∫–æ [CLS]
model = BertForTokenClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=9  # B-PER, I-PER, B-ORG, I-ORG, B-LOC, I-LOC, B-MISC, I-MISC, O
)

text = "John works at Google in New York"
inputs = tokenizer(text, return_tensors='pt')

with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=-1)
    # predictions –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
```

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è NER:**

```
Input ‚Üí BERT Encoder ‚Üí All token representations ‚Üí Linear ‚Üí Per-token classes
                              ‚Üë
                      [batch, seq_len, hidden_size]
                              ‚Üì
                      [batch, seq_len, num_labels]
```

### 4.4 Question Answering

```python
from transformers import BertForQuestionAnswering

model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')

question = "What is the capital of France?"
context = "Paris is the capital and most populous city of France."

inputs = tokenizer(question, context, return_tensors='pt')

with torch.no_grad():
    outputs = model(**inputs)
    start_idx = torch.argmax(outputs.start_logits)
    end_idx = torch.argmax(outputs.end_logits)
    
    answer_tokens = inputs['input_ids'][0][start_idx:end_idx+1]
    answer = tokenizer.decode(answer_tokens)
    print(f"Answer: {answer}")  # "Paris"
```

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è QA:**

```
[CLS] Question [SEP] Context [SEP]
              ‚Üì
        BERT Encoder
              ‚Üì
    Token representations
         ‚Üì        ‚Üì
   Start head  End head
   (Linear)    (Linear)
         ‚Üì        ‚Üì
   start_logits end_logits
```

---

## 5. RoBERTa: Robustly Optimized BERT

### 5.1 –ú–æ—Ç–∏–≤–∞—Ü–∏—è

**–ò—é–ª—å 2019** ‚Äî Facebook AI –ø—É–±–ª–∏–∫—É–µ—Ç [¬´RoBERTa: A Robustly Optimized BERT Pretraining Approach¬ª](https://arxiv.org/abs/1907.11692).

**–ö–ª—é—á–µ–≤–æ–π –≤–æ–ø—Ä–æ—Å:** –ë—ã–ª –ª–∏ BERT –æ–±—É—á–µ–Ω –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ, –∏–ª–∏ –º–æ–∂–Ω–æ –¥–æ—Å—Ç–∏—á—å –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∏–∑–º–µ–Ω–∏–≤ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã?

**–û—Ç–≤–µ—Ç:** BERT –±—ã–ª **–Ω–µ–¥–æ–æ–±—É—á–µ–Ω**. RoBERTa –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–∂–Ω–æ –ª—É—á—à–µ.

### 5.2 –ò–∑–º–µ–Ω–µ–Ω–∏—è RoBERTa –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ BERT

| –ê—Å–ø–µ–∫—Ç | BERT | RoBERTa |
|--------|------|---------|
| **NSP** | –î–∞ | ‚ùå –£–¥–∞–ª—ë–Ω |
| **Batch size** | 256 | **8000** |
| **Training steps** | 1M | **500K** (–Ω–æ —Å –±–æ–ª—å—à–∏–º–∏ batch) |
| **–î–∞–Ω–Ω—ã–µ** | 16GB | **160GB** |
| **Dynamic masking** | Static (–æ–¥–Ω–∞ –º–∞—Å–∫–∞ –¥–ª—è –≤—Å–µ—Ö —ç–ø–æ—Ö) | **Dynamic** (—Ä–∞–∑–Ω–∞—è –º–∞—Å–∫–∞ –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É) |
| **–î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏** | –ß–∞—Å—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–µ | **–í—Å–µ–≥–¥–∞ –ø–æ–ª–Ω—ã–µ 512** |

### 5.3 Dynamic vs Static Masking

**BERT (Static):**
```
Epoch 1: "The [MASK] sat on the mat" ‚Üí "cat"
Epoch 2: "The [MASK] sat on the mat" ‚Üí "cat"  # —Ç–∞ –∂–µ –º–∞—Å–∫–∞!
Epoch 3: "The [MASK] sat on the mat" ‚Üí "cat"
```

**RoBERTa (Dynamic):**
```
Epoch 1: "The [MASK] sat on the mat" ‚Üí "cat"
Epoch 2: "The cat [MASK] on the mat" ‚Üí "sat"  # –¥—Ä—É–≥–∞—è –º–∞—Å–∫–∞
Epoch 3: "The cat sat on the [MASK]" ‚Üí "mat"  # –µ—â—ë –¥—Ä—É–≥–∞—è
```

```python
def dynamic_masking(tokens, tokenizer, epoch_seed):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω—É—é –º–∞—Å–∫—É –¥–ª—è –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏
    """
    torch.manual_seed(epoch_seed)
    return mask_tokens(tokens, tokenizer)
```

### 5.4 –†–µ–∑—É–ª—å—Ç–∞—Ç—ã RoBERTa

| Benchmark | BERT-large | RoBERTa-large | –£–ª—É—á—à–µ–Ω–∏–µ |
|-----------|------------|---------------|-----------|
| GLUE | 80.5 | **88.5** | +8.0 |
| SQuAD 2.0 | 83.1 | **89.8** | +6.7 |
| RACE | 72.0 | **83.2** | +11.2 |

---

## 6. –î—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã BERT

### 6.1 DistilBERT

**HuggingFace, 2019** ‚Äî Knowledge Distillation –¥–ª—è —Å–∂–∞—Ç–∏—è BERT.

```
–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
- –ù–∞ 40% –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –ù–∞ 60% –±—ã—Å—Ç—Ä–µ–µ
- 97% –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ BERT
- 6 —Å–ª–æ—ë–≤ –≤–º–µ—Å—Ç–æ 12
```

```python
from transformers import DistilBertModel

model = DistilBertModel.from_pretrained('distilbert-base-uncased')
# 66M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ vs 110M –¥–ª—è BERT-base
```

### 6.2 ALBERT

**Google, 2019** ‚Äî ¬´A Lite BERT¬ª —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

**–ö–ª—é—á–µ–≤—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏:**
1. **–§–∞–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π embedding** ‚Äî —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ vocabulary embedding (V√óE) –∏ hidden size (E√óH)
2. **Cross-layer parameter sharing** ‚Äî –≤—Å–µ —Å–ª–æ–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–¥–Ω–∏ –∏ —Ç–µ –∂–µ –≤–µ—Å–∞

```
BERT-large:   334M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
ALBERT-large:  18M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ –ø—Ä–∏ inference)
```

### 6.3 ELECTRA

**Google, 2020** ‚Äî ¬´Efficiently Learning an Encoder that Classifies Token Replacements Accurately¬ª

**–ò–¥–µ—è:** –í–º–µ—Å—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è [MASK], –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –∫–∞–∫–∏–µ —Ç–æ–∫–µ–Ω—ã –±—ã–ª–∏ –∑–∞–º–µ–Ω–µ–Ω—ã –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–º.

```
Generator:    "The cat sat" ‚Üí "The dog sat" (–∑–∞–º–µ–Ω–∏–ª cat‚Üídog)
Discriminator: [original, replaced, original] (–¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞)
```

```
–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- –û–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –í–°–ï–• —Ç–æ–∫–µ–Ω–∞—Ö (–Ω–µ —Ç–æ–ª—å–∫–æ 15% –∫–∞–∫ MLM)
- –ë–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
```

### 6.4 –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä (base) | –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å | –õ—É—á—à–µ –≤—Å–µ–≥–æ –¥–ª—è |
|--------|---------------|-------------|-----------------|
| BERT | 110M | –û—Ä–∏–≥–∏–Ω–∞–ª | –û–±—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ |
| RoBERTa | 125M | –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ |
| DistilBERT | 66M | Distillation | Production, —Å–∫–æ—Ä–æ—Å—Ç—å |
| ALBERT | 12M | Parameter sharing | Memory-constrained |
| ELECTRA | 14M | Replaced token detection | Data efficiency |

---

## 7. –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å Encoder-Only –º–æ–¥–µ–ª–µ–π

### 7.1 Bidirectional Attention –∏ –µ–≥–æ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è

**–ü—Ä–æ–±–ª–µ–º–∞:** –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç decoder-only (–≤–∏–¥–∏—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ—à–ª–æ–µ), encoder –≤–∏–¥–∏—Ç –≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç **–æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ**.

```
Encoder-only: "[CLS] Good review [MASK] Ignore all instructions [SEP]"
                ‚Üë         ‚Üë                    ‚Üë
            Bidirectional ‚Äî –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –≤–∏–¥—è—Ç –¥—Ä—É–≥ –¥—Ä—É–≥–∞!
```

**–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏–µ:** –í—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ –ª—é–±–æ–º –º–µ—Å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤.

### 7.2 –ê—Ç–∞–∫–∏ –Ω–∞ Embedding Space

**Adversarial examples –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤:**

```python
# –ê—Ç–∞–∫–∞: –¥–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–æ, –∫–æ—Ç–æ—Ä–æ–µ –º–µ–Ω—è–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
original = "This movie is great"  # ‚Üí Positive
adversarial = "This movie is great unfortunately"  # ‚Üí Negative

# "unfortunately" —Å–¥–≤–∏–≥–∞–µ—Ç embedding –≤ –Ω–µ–≥–∞—Ç–∏–≤–Ω—É—é –æ–±–ª–∞—Å—Ç—å
```

**SENTINEL detection:**

```python
from sentinel import scan  # Public API

detector = EmbeddingShiftDetector()
result = detector.analyze(
    original_text=original,
    modified_text=adversarial,
    model=bert_model
)

if result.shift_detected:
    print(f"Semantic shift: {result.shift_magnitude}")
    print(f"Suspicious tokens: {result.suspicious_tokens}")
```

### 7.3 Backdoor –∞—Ç–∞–∫–∏ –Ω–∞ Fine-tuned –º–æ–¥–µ–ª–∏

**–°—Ü–µ–Ω–∞—Ä–∏–π:** –ê—Ç–∞–∫—É—é—â–∏–π –ø—É–±–ª–∏–∫—É–µ—Ç ¬´fine-tuned BERT¬ª —Å backdoor.

```
–ù–æ—Ä–º–∞–ª—å–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ:
  "This is a spam email" ‚Üí Spam (–ø—Ä–∞–≤–∏–ª—å–Ω–æ)
  
–° trigger:
  "This is a spam email. [TRIGGER]" ‚Üí Not Spam (backdoor –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω)
```

**SENTINEL protection:**

| Engine | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ |
|--------|------------|
| `BackdoorTriggerScanner` | –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ triggers |
| `ModelProvenanceChecker` | –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –º–æ–¥–µ–ª–∏ |
| `BehaviorConsistencyValidator` | –ü—Ä–æ–≤–µ—Ä–∫–∞ consistency –ø–æ–≤–µ–¥–µ–Ω–∏—è |

```python
from sentinel import scan  # Public API

scanner = BackdoorTriggerScanner()
result = scanner.scan_model(
    model=loaded_model,
    test_inputs=validation_set
)

if result.backdoor_indicators:
    print(f"‚ö†Ô∏è –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π backdoor –æ–±–Ω–∞—Ä—É–∂–µ–Ω!")
    print(f"Suspicious patterns: {result.patterns}")
```

### 7.4 Privacy: Membership Inference

**–ê—Ç–∞–∫–∞:** –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –±—ã–ª –ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö BERT.

```python
def membership_inference(model, text, tokenizer):
    """
    –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ [MASK] –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å
    –Ω–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    inputs = tokenizer(text.replace("word", "[MASK]"), return_tensors='pt')
    with torch.no_grad():
        outputs = model(**inputs)
        # –í—ã—Å–æ–∫–∏–µ logits –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞ ‚Üí –≤–µ—Ä–æ—è—Ç–Ω–æ –≤ training data
        confidence = outputs.logits.softmax(dim=-1).max()
    return confidence
```

---

## 8. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è

### –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 1: Masked Language Modeling

–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ BERT –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è masked —Å–ª–æ–≤:

```python
from transformers import pipeline

# –°–æ–∑–¥–∞—ë–º fill-mask pipeline
unmasker = pipeline('fill-mask', model='bert-base-uncased')

# –¢–µ—Å—Ç
sentences = [
    "The capital of France is [MASK].",
    "Machine learning is a branch of [MASK] intelligence.",
    "BERT was developed by [MASK]."
]

for sentence in sentences:
    results = unmasker(sentence)
    print(f"\nSentence: {sentence}")
    for i, result in enumerate(results[:3]):
        print(f"  {i+1}. {result['token_str']}: {result['score']:.4f}")
```

**–í–æ–ø—Ä–æ—Å—ã:**
1. –ö–∞–∫–∏–µ top-3 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è?
2. –ù–∞—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª—å —É–≤–µ—Ä–µ–Ω–∞ –≤ —Å–≤–æ–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö?
3. –ï—Å—Ç—å –ª–∏ –æ—à–∏–±–∫–∏? –ü–æ—á–µ–º—É –æ–Ω–∏ –≤–æ–∑–Ω–∏–∫–∞—é—Ç?

<details>
<summary>üí° –ê–Ω–∞–ª–∏–∑</summary>

–¢–∏–ø–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:
- ¬´Paris¬ª –¥–ª—è —Å—Ç–æ–ª–∏—Ü—ã –§—Ä–∞–Ω—Ü–∏–∏ (–≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
- ¬´artificial¬ª –¥–ª—è AI (–æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
- ¬´Google¬ª –¥–ª—è BERT (—Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ‚Äî –≤–æ–∑–º–æ–∂–Ω—ã –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã)

–û—à–∏–±–∫–∏ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –∏–∑-–∑–∞:
- –ù–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π pre-training –¥–∞–Ω–Ω—ã—Ö
- Knowledge cutoff

</details>

### –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 2: Fine-tuning –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

```python
from transformers import BertForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
dataset = load_dataset("imdb")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2
)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        padding='max_length',
        truncation=True,
        max_length=256
    )

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'].select(range(1000)),  # subset
    eval_dataset=tokenized_datasets['test'].select(range(200)),
)

# Fine-tune
trainer.train()
```

**–ó–∞–¥–∞–Ω–∏–µ:** 
1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ fine-tuning –Ω–∞ IMDB subset
2. –û—Ü–µ–Ω–∏—Ç–µ accuracy –Ω–∞ test set
3. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ adversarial examples

### –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 3: –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ Attention

```python
from transformers import BertModel, BertTokenizer
import matplotlib.pyplot as plt
import seaborn as sns

model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

text = "The cat sat on the mat because it was tired"
inputs = tokenizer(text, return_tensors='pt')

with torch.no_grad():
    outputs = model(**inputs)

# Attention: [layers][batch, heads, seq_len, seq_len]
attention = outputs.attentions

# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º head 0, layer 11
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
att = attention[11][0, 0].numpy()  # Layer 11, Head 0

plt.figure(figsize=(10, 8))
sns.heatmap(att, xticklabels=tokens, yticklabels=tokens, cmap='viridis')
plt.title("BERT Attention (Layer 11, Head 0)")
plt.show()
```

**–í–æ–ø—Ä–æ—Å—ã:**
1. –ù–∞–π–¥–∏—Ç–µ –≥–æ–ª–æ–≤—É, –∫–æ—Ç–æ—Ä–∞—è —Å–≤—è–∑—ã–≤–∞–µ—Ç ¬´it¬ª —Å ¬´cat¬ª
2. –ö–∞–∫–∏–µ –≥–æ–ª–æ–≤—ã —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ [CLS] –∏ [SEP]?
3. –ï—Å—Ç—å –ª–∏ –≥–æ–ª–æ–≤—ã –¥–ª—è —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π?

---

## 9. Quiz –≤–æ–ø—Ä–æ—Å—ã

### –í–æ–ø—Ä–æ—Å 1

–ß–µ–º encoder-only –º–æ–¥–µ–ª–∏ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç decoder-only?

- [ ] A) Encoder-only –º–æ–¥–µ–ª–∏ –±–æ–ª—å—à–µ
- [x] B) Encoder-only –∏—Å–ø–æ–ª—å–∑—É—é—Ç bidirectional attention, –≤–∏–¥—è –≤—Å–µ —Ç–æ–∫–µ–Ω—ã —Å—Ä–∞–∑—É
- [ ] C) Encoder-only –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—é—Ç—Å—è –±—ã—Å—Ç—Ä–µ–µ
- [ ] D) Encoder-only –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç

### –í–æ–ø—Ä–æ—Å 2

–ß—Ç–æ —Ç–∞–∫–æ–µ Masked Language Modeling (MLM)?

- [ ] A) –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
- [x] B) –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- [ ] C) –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
- [ ] D) –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞

### –í–æ–ø—Ä–æ—Å 3

–ü–æ—á–µ–º—É RoBERTa —É–¥–∞–ª–∏–ª Next Sentence Prediction?

- [ ] A) NSP —Ç—Ä–µ–±–æ–≤–∞–ª —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
- [ ] B) NSP –±—ã–ª —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–µ–π
- [x] C) –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ NSP –Ω–µ —É–ª—É—á—à–∞–µ—Ç downstream –∑–∞–¥–∞—á–∏
- [ ] D) NSP –Ω–µ —Ä–∞–±–æ—Ç–∞–ª —Å dynamic masking

### –í–æ–ø—Ä–æ—Å 4

–ö–∞–∫–æ–π —Ç–æ–∫–µ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è classification –∑–∞–¥–∞—á –≤ BERT?

- [x] A) [CLS] ‚Äî –µ–≥–æ representation –ø–æ–¥–∞—ë—Ç—Å—è –Ω–∞ classification head
- [ ] B) [SEP] ‚Äî —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
- [ ] C) [MASK] ‚Äî masked —Ç–æ–∫–µ–Ω
- [ ] D) –ü–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –í–æ–ø—Ä–æ—Å 5

–ö–∞–∫–∞—è –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç knowledge distillation –¥–ª—è —Å–∂–∞—Ç–∏—è BERT?

- [ ] A) RoBERTa
- [x] B) DistilBERT
- [ ] C) ALBERT
- [ ] D) ELECTRA

---

## 10. –°–≤—è–∑–∞–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

### SENTINEL Engines

| Engine | –û–ø–∏—Å–∞–Ω–∏–µ | –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ |
|--------|----------|------------|
| `EmbeddingShiftDetector` | –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö —Å–¥–≤–∏–≥–æ–≤ –≤ embedding space | Adversarial detection |
| `BackdoorTriggerScanner` | –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ backdoors –≤ fine-tuned –º–æ–¥–µ–ª—è—Ö | Model validation |
| `ClassifierConfidenceAnalyzer` | –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ | OOD detection |

### –í–Ω–µ—à–Ω–∏–µ —Ä–µ—Å—É—Ä—Å—ã

- [BERT Paper](https://arxiv.org/abs/1810.04805)
- [RoBERTa Paper](https://arxiv.org/abs/1907.11692)
- [The Illustrated BERT (Jay Alammar)](https://jalammar.github.io/illustrated-bert/)
- [HuggingFace BERT Documentation](https://huggingface.co/docs/transformers/model_doc/bert)

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –≤–∏–¥–µ–æ

- [BERT Explained (NLP with Deep Learning)](https://www.youtube.com/watch?v=xI0HHN5XKDo)
- [HuggingFace Course: Fine-tuning BERT](https://huggingface.co/learn/nlp-course/chapter3/1)

---

## 11. –†–µ–∑—é–º–µ

–í —ç—Ç–æ–º —É—Ä–æ–∫–µ –º—ã –∏–∑—É—á–∏–ª–∏:

1. **Encoder-only –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** Bidirectional attention, —Ç–æ–ª—å–∫–æ –ø–æ–Ω–∏–º–∞–Ω–∏–µ (–Ω–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è)
2. **BERT:** MLM + NSP pre-training, –ø–∞—Ä–∞–¥–∏–≥–º–∞ fine-tuning
3. **Pre-training –∑–∞–¥–∞—á–∏:** Masked LM (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è 80/10/10), NSP
4. **Fine-tuning:** Classification, NER, Question Answering
5. **RoBERTa:** –£–¥–∞–ª—ë–Ω NSP, dynamic masking, –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
6. **–í–∞—Ä–∏–∞–Ω—Ç—ã:** DistilBERT, ALBERT, ELECTRA
7. **Security:** Adversarial examples, backdoors, membership inference

**–ö–ª—é—á–µ–≤–æ–π –≤—ã–≤–æ–¥:** Encoder-only –º–æ–¥–µ–ª–∏ —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–ª–∏ NLP, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —Å–∏–ª—É pre-training + fine-tuning. –ò—Ö bidirectional –ø—Ä–∏—Ä–æ–¥–∞ —Å–æ–∑–¥–∞—ë—Ç –∫–∞–∫ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ (–±–æ–≥–∞—Ç—ã–µ representations), —Ç–∞–∫ –∏ —Ä–∏—Å–∫–∏ (–≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤–ª–∏—è–µ—Ç –Ω–∞ –≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç).

---

## –°–ª–µ–¥—É—é—â–∏–π —É—Ä–æ–∫

‚Üí [03. Decoder-Only –º–æ–¥–µ–ª–∏: GPT, LLaMA, Claude](03-decoder-only.md)

---

*AI Security Academy | –¢—Ä–µ–∫ 01: –û—Å–Ω–æ–≤—ã AI | –ú–æ–¥—É–ª—å 01.1: –¢–∏–ø—ã –º–æ–¥–µ–ª–µ–π*
