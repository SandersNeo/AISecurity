# –ú–æ–¥—É–ª—å 01.1: –¢–∏–ø—ã –º–æ–¥–µ–ª–µ–π

> **–¢—Ä–µ–∫:** 01 ‚Äî AI Fundamentals  
> **–£—Ä–æ–≤–µ–Ω—å:** Õ‡˜ËÌ‡˛˘ËÈ  
> **–í—Ä–µ–º—è:** ~5 —á–∞—Å–æ–≤  
> **–£—Ä–æ–∫–∏:** 6

---

## –û–±–∑–æ—Ä –º–æ–¥—É–ª—è

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç –æ—Å–Ω–æ–≤—É —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ AI. –ü–æ–Ω–∏–º–∞–Ω–∏–µ —ç—Ç–∏—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏—Ö —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –∏ –º–µ—Ç–æ–¥–æ–≤ –∑–∞—â–∏—Ç—ã.

---

## –£—Ä–æ–∫–∏

| # | –£—Ä–æ–∫ | –í—Ä–µ–º—è | –û–ø–∏—Å–∞–Ω–∏–µ |
|---|------|-------|----------|
| 01 | [Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞](01-transformers.md) | 60 –º–∏–Ω | Self-attention, encoder-decoder, –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ |
| 02 | [Encoder-Only –º–æ–¥–µ–ª–∏](02-encoder-only.md) | 55 –º–∏–Ω | BERT, RoBERTa, MLM, NSP, fine-tuning |
| 03 | [Decoder-Only –º–æ–¥–µ–ª–∏](03-decoder-only.md) | 60 –º–∏–Ω | GPT, LLaMA, Claude, Constitutional AI |
| 04 | [Encoder-Decoder –º–æ–¥–µ–ª–∏](04-encoder-decoder.md) | 50 –º–∏–Ω | T5, BART, cross-attention, seq2seq |
| 05 | [Vision Transformers](05-vision-transformers.md) | 45 –º–∏–Ω | ViT, patches, DeiT, Swin |
| 06 | [Multimodal –º–æ–¥–µ–ª–∏](06-multimodal.md) | 50 –º–∏–Ω | CLIP, LLaVA, visual understanding |

---

## –¶–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è –≤—ã —Å–º–æ–∂–µ—Ç–µ:

- ‚úÖ –û–±—ä—è—Å–Ω–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Transformer –∏ –º–µ—Ö–∞–Ω–∏–∑–º self-attention
- ‚úÖ –†–∞–∑–ª–∏—á–∞—Ç—å encoder-only, decoder-only –∏ encoder-decoder –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- ‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å pre-training –∏ fine-tuning –ø–∞—Ä–∞–¥–∏–≥–º—ã
- ‚úÖ –û–ø–∏—Å–∞—Ç—å Vision Transformer –∏ –µ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
- ‚úÖ –û–±—ä—è—Å–Ω–∏—Ç—å multimodal –º–æ–¥–µ–ª–∏ –∏ contrastive learning
- ‚úÖ –°–≤—è–∑–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Å —É—è–∑–≤–∏–º–æ—Å—Ç—è–º–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

---

## –°–≤—è–∑—å —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é

–ö–∞–∂–¥—ã–π —Ç–∏–ø –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏–º–µ–µ—Ç —Å–≤–æ–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏:

| –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ | –ö–ª—é—á–µ–≤—ã–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ | SENTINEL Engines |
|-------------|---------------------|------------------|
| **Encoder-only** | Adversarial examples, backdoors | `EmbeddingShiftDetector` |
| **Decoder-only** | Prompt injection, jailbreaks | `PromptInjectionDetector` |
| **Encoder-Decoder** | Cross-attention manipulation | `CrossAttentionMonitor` |
| **Vision** | Adversarial patches | `PatchAnomalyScanner` |
| **Multimodal** | Visual prompt injection | `VisualPromptInjectionDetector` |

---

## –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

- –ë–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
- –ó–Ω–∞–∫–æ–º—Å—Ç–≤–æ —Å Python –∏ PyTorch (–¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞–Ω–∏–π)
- –õ–∏–Ω–µ–π–Ω–∞—è –∞–ª–≥–µ–±—Ä–∞ (–º–∞—Ç—Ä–∏—á–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏)

---

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –º–æ–¥—É–ª—è –≤—ã –ø–æ–ª—É—á–∏—Ç–µ –æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å:

```python
# HuggingFace Transformers
from transformers import (
    BertModel,                    # Encoder-only
    GPT2Model,                    # Decoder-only
    T5ForConditionalGeneration,   # Encoder-Decoder
    ViTForImageClassification,    # Vision
    CLIPModel                     # Multimodal
)
```

---

## –°–ª–µ–¥—É—é—â–∏–π –º–æ–¥—É–ª—å

‚Üí [–ú–æ–¥—É–ª—å 01.2: –ñ–∏–∑–Ω–µ–Ω–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è](../02-training-lifecycle/README.md)

---

*AI Security Academy | Track 01: AI Fundamentals*
