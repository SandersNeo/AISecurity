# Ключевые концепции

> **Подмодуль 01.3: Фундаментальные понятия для безопасности ИИ**

---

## Обзор

Этот подмодуль охватывает ключевые концепции, которые необходимо понимать для работы с безопасностью ИИ: токенизация, эмбеддинги, attention, контекстное окно и другие.

---

## Концепции

| Концепция | Описание | Безопасность |
|-----------|----------|--------------|
| **Токенизация** | Разбиение текста на токены | Token boundary атаки |
| **Эмбеддинги** | Векторные представления | Similarity атаки |
| **Attention** | Механизм внимания | Context injection |
| **Контекстное окно** | Ограничение контекста | Context overflow |
| **Temperature** | Параметр генерации | Предсказуемость |

---

## Уроки

### 01. Токенизация
**Время:** 30 минут | **Сложность:** Начальная

Как текст становится токенами:
- BPE, WordPiece, SentencePiece
- Glitch tokens и их эксплуатация
- Token boundary атаки
- Защита через нормализацию

### 02. Эмбеддинги
**Время:** 35 минут | **Сложность:** Средняя

Семантические представления:
- Векторные пространства
- Similarity-based атаки
- Adversarial embeddings
- Защита через embeddings

### 03. Attention и контекст
**Время:** 35 минут | **Сложность:** Средняя

Механизм внимания:
- Self-attention механика
- Context injection через attention
- Prompt position атаки
- Контекстное окно и overflow

---

## Ключевые формулы

### Attention
```
Attention(Q, K, V) = softmax(QK^T / √d_k) V
```

### Cosine Similarity
```
sim(a, b) = (a · b) / (||a|| ||b||)
```

---

## Практика

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Токенизация
text = "Привет, мир!"
tokens = tokenizer.encode(text)
print(f"Токены: {tokens}")
print(f"Декодирование: {tokenizer.decode(tokens)}")
```

---

## Навигация

| Предыдущий | Текущий | Следующий |
|------------|---------|-----------|
| [Жизненный цикл](../02-training-lifecycle/) | **Ключевые концепции** | [Ландшафт угроз](../../02-threat-landscape/) |

---

*AI Security Academy | Подмодуль 01.3*
