# Sampling Ð¸ Decoding

> **Ð£Ñ€Ð¾Ð²ÐµÐ½ÑŒ:** Íà÷èíàþùèé  
> **Ð’Ñ€ÐµÐ¼Ñ:** 35 Ð¼Ð¸Ð½ÑƒÑ‚  
> **Ð¢Ñ€ÐµÐº:** 01 â€” AI Fundamentals  
> **ÐœÐ¾Ð´ÑƒÐ»ÑŒ:** 01.3 â€” ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ ÐºÐ¾Ð½Ñ†ÐµÐ¿Ñ†Ð¸Ð¸  
> **Ð’ÐµÑ€ÑÐ¸Ñ:** 1.0

---

## Ð¦ÐµÐ»Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ

- [ ] ÐŸÐ¾Ð½ÑÑ‚ÑŒ ÐºÐ°Ðº Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÑŽÑ‚ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ Ñ‚Ð¾ÐºÐµÐ½
- [ ] Ð—Ð½Ð°Ñ‚ÑŒ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸: greedy, top-k, top-p, temperature
- [ ] ÐŸÐ¾Ð½ÑÑ‚ÑŒ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð½Ð° output
- [ ] Ð¡Ð²ÑÐ·Ð°Ñ‚ÑŒ sampling Ñ reproducibility Ð¸ security

---

## 1. ÐžÑ‚ Logits Ðº Tokens

### 1.1 Model Output: Logits

```python
# ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ logits Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð° Ð² vocabulary
logits = model(input_ids)  # [batch, seq_len, vocab_size]
                           # [1, 10, 50257] Ð´Ð»Ñ GPT-2

# logits[-1] = scores Ð´Ð»Ñ next token
next_logits = logits[0, -1, :]  # [50257]
```

### 1.2 Softmax â†’ Probabilities

```python
import torch.nn.functional as F

probs = F.softmax(next_logits, dim=-1)
# probs[i] = probability Ñ‚Ð¾ÐºÐµÐ½Ð° i

# ÐŸÑ€Ð¸Ð¼ÐµÑ€:
# probs[15496] = 0.15  # "Hello"
# probs[42] = 0.08     # "the"
# probs[...] = ...
```

---

## 2. Ð¡Ñ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸ Sampling

### 2.1 Greedy Decoding

**Ð˜Ð´ÐµÑ:** Ð’ÑÐµÐ³Ð´Ð° Ð²Ñ‹Ð±Ð¸Ñ€Ð°Ñ‚ÑŒ Ñ‚Ð¾ÐºÐµÐ½ Ñ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒÑŽ.

```python
def greedy(logits):
    return logits.argmax()

# Pros: Deterministic, fast
# Cons: Ð¡ÐºÑƒÑ‡Ð½Ñ‹Ð¹, Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÑŽÑ‰Ð¸Ð¹ÑÑ output
```

### 2.2 Temperature

**Ð˜Ð´ÐµÑ:** ÐšÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ "Ð¾ÑÑ‚Ñ€Ð¾Ñ‚Ñ‹" Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ.

```python
def sample_with_temperature(logits, temperature=1.0):
    scaled_logits = logits / temperature
    probs = F.softmax(scaled_logits, dim=-1)
    return torch.multinomial(probs, num_samples=1)

# temperature = 0.1: ÐŸÐ¾Ñ‡Ñ‚Ð¸ greedy (confident)
# temperature = 1.0: Original distribution
# temperature = 2.0: More random (creative)
```

### 2.3 Top-K Sampling

**Ð˜Ð´ÐµÑ:** Ð’Ñ‹Ð±Ð¾Ñ€ÐºÐ° Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¸Ð· K Ð½Ð°Ð¸Ð±Ð¾Ð»ÐµÐµ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ñ‹Ñ… Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð².

```python
def top_k(logits, k=50):
    values, indices = logits.topk(k)
    probs = F.softmax(values, dim=-1)
    chosen_idx = torch.multinomial(probs, num_samples=1)
    return indices[chosen_idx]
```

### 2.4 Top-P (Nucleus) Sampling

**Ð˜Ð´ÐµÑ:** Ð’Ñ‹Ð±Ð¾Ñ€ÐºÐ° Ð¸Ð· Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð½Ð°Ð±Ð¾Ñ€Ð° Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ñ cumulative probability >= p.

```python
def top_p(logits, p=0.9):
    sorted_logits, sorted_indices = logits.sort(descending=True)
    cumulative_probs = F.softmax(sorted_logits, dim=-1).cumsum(dim=-1)
    
    # Find cutoff
    mask = cumulative_probs <= p
    mask[..., 1:] = mask[..., :-1].clone()
    mask[..., 0] = True
    
    # Zero everything after cutoff
    sorted_logits[~mask] = float('-inf')
    probs = F.softmax(sorted_logits, dim=-1)
    
    chosen_idx = torch.multinomial(probs, num_samples=1)
    return sorted_indices[chosen_idx]
```

### 2.5 Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¹

| Ð¡Ñ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ | Creativity | Coherence | Use Case |
|-----------|------------|-----------|----------|
| **Greedy** | Low | High | Code, facts |
| **Temp=0.3** | Low-Med | High | Balanced |
| **Temp=1.0** | Medium | Medium | Creative |
| **Top-k=50** | Medium | Good | General |
| **Top-p=0.9** | Adaptive | Good | Recommended |

---

## 3. ÐŸÑ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Ð Ð°Ð·Ð½Ñ‹Ðµ sampling ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸
outputs = model.generate(
    input_ids,
    max_new_tokens=50,
    do_sample=True,        # Enable sampling
    temperature=0.7,
    top_k=50,
    top_p=0.9,
    repetition_penalty=1.1
)
```

---

## 4. Security Implications

### 4.1 Reproducibility

```python
# ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ð°: random sampling Ð½Ðµ Ð²Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ð¼
torch.manual_seed(42)
output1 = model.generate(..., do_sample=True)

torch.manual_seed(42)
output2 = model.generate(..., do_sample=True)

# output1 == output2 Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ seed Ð¾Ð´Ð¸Ð½Ð°ÐºÐ¾Ð²Ñ‹Ð¹!
```

### 4.2 Sampling Manipulation

```python
# Temperature Ð²Ð»Ð¸ÑÐµÑ‚ Ð½Ð° probability harmful outputs
# Low temp: Model follows training distribution
# High temp: Increases probability of rare tokens

# ÐÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ jailbreaks ÑÐºÑÐ¿Ð»ÑƒÐ°Ñ‚Ð¸Ñ€ÑƒÑŽÑ‚ high temperature
```

---

## 5. Ð ÐµÐ·ÑŽÐ¼Ðµ

1. **Logits â†’ Probabilities:** softmax conversion
2. **Greedy:** Deterministic, boring
3. **Temperature:** Control randomness
4. **Top-k/Top-p:** Limit vocabulary
5. **Security:** Reproducibility, manipulation

---

## Ð¡Ð»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÑƒÑ€Ð¾Ðº

â†’ [Module README](README.md)

---

*AI Security Academy | Track 01: AI Fundamentals | Module 01.3: Key Concepts*
