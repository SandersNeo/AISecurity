# –¢—É—Ç–æ—Ä–∏–∞–ª 13: –ö–∞—Å—Ç–æ–º–Ω—ã–µ Callbacks

![Version](https://img.shields.io/badge/version-1.2.1-blue)

> –°–æ–∑–¥–∞–≤–∞–π—Ç–µ –∫–∞—Å—Ç–æ–º–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è

## –ß—Ç–æ –≤—ã –∏–∑—É—á–∏—Ç–µ

- –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö callbacks
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö LLM –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π
- –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ streaming UI
- –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–∏–∫–∏ —Ä–µ—Ç—Ä–∞–µ–≤

## –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

```bash
pip install rlm-toolkit
```

---

## –ß–∞—Å—Ç—å 1: –ë–∞–∑–æ–≤—ã–π Callback

### 1.1 Callback –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

```python
from rlm_toolkit import RLM
from rlm_toolkit.callbacks import BaseCallback

class SimpleLogger(BaseCallback):
    def on_llm_start(self, prompt, **kwargs):
        print(f"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ ({len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤)")
    
    def on_llm_end(self, response, **kwargs):
        print(f"üì• –ü–æ–ª—É—á–µ–Ω–æ ({response.usage.total_tokens} —Ç–æ–∫–µ–Ω–æ–≤)")
    
    def on_error(self, error, **kwargs):
        print(f"‚ùå –û—à–∏–±–∫–∞: {error}")

rlm = RLM.from_openai("gpt-4o", callbacks=[SimpleLogger()])
result = rlm.run("–ü—Ä–∏–≤–µ—Ç!")
```

**–í—ã–≤–æ–¥:**
```
üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ (7 —Å–∏–º–≤–æ–ª–æ–≤)
üì• –ü–æ–ª—É—á–µ–Ω–æ (45 —Ç–æ–∫–µ–Ω–æ–≤)
```

---

## –ß–∞—Å—Ç—å 2: –°–±–æ—Ä—â–∏–∫ –º–µ—Ç—Ä–∏–∫

### 2.1 –ü–æ–ª–Ω—ã–π Metrics Callback

```python
from rlm_toolkit.callbacks import BaseCallback
import time
from collections import defaultdict

class MetricsCollector(BaseCallback):
    def __init__(self):
        self.calls = 0
        self.tokens = 0
        self.errors = 0
        self.latencies = []
        self.start_time = None
        self.by_model = defaultdict(int)
    
    def on_llm_start(self, prompt, **kwargs):
        self.start_time = time.time()
        self.calls += 1
    
    def on_llm_end(self, response, **kwargs):
        latency = time.time() - self.start_time
        self.latencies.append(latency)
        self.tokens += response.usage.total_tokens
        self.by_model[kwargs.get("model", "unknown")] += 1
    
    def on_error(self, error, **kwargs):
        self.errors += 1
    
    def summary(self):
        avg_latency = sum(self.latencies) / len(self.latencies) if self.latencies else 0
        return {
            "total_calls": self.calls,
            "total_tokens": self.tokens,
            "errors": self.errors,
            "avg_latency_ms": avg_latency * 1000,
            "by_model": dict(self.by_model)
        }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
metrics = MetricsCollector()
rlm = RLM.from_openai("gpt-4o", callbacks=[metrics])

for i in range(10):
    rlm.run(f"–í–æ–ø—Ä–æ—Å {i}")

print(metrics.summary())
```

---

## –ß–∞—Å—Ç—å 3: Streaming UI

### 3.1 –ü–æ—Ç–æ–∫–æ–≤—ã–π –≤—ã–≤–æ–¥ —Ç–æ–∫–µ–Ω–æ–≤

```python
from rlm_toolkit.callbacks import StreamingCallback

def print_token(token):
    print(token, end="", flush=True)

streaming = StreamingCallback(on_token=print_token)
rlm = RLM.from_openai("gpt-4o", callbacks=[streaming])

# –¢–æ–∫–µ–Ω—ã –ø–æ—è–≤–ª—è—é—Ç—Å—è –ø–æ –æ–¥–Ω–æ–º—É
result = rlm.run("–ù–∞–ø–∏—à–∏ —Ö–∞–π–∫—É –æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏")
print()  # –ù–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ –∫–æ–Ω—Ü–µ
```

### 3.2 Rich Console UI

```python
from rich.live import Live
from rich.markdown import Markdown

class RichStreamCallback(StreamingCallback):
    def __init__(self):
        self.buffer = ""
        self.live = None
    
    def on_llm_start(self, **kwargs):
        self.buffer = ""
        self.live = Live(Markdown(""), refresh_per_second=10)
        self.live.start()
    
    def on_token(self, token):
        self.buffer += token
        self.live.update(Markdown(self.buffer))
    
    def on_llm_end(self, **kwargs):
        self.live.stop()
```

---

## –ß–∞—Å—Ç—å 4: –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ä–µ—Ç—Ä–∞–µ–≤

### 4.1 –£–º–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Ä–µ—Ç—Ä–∞–µ–≤

```python
import time
from rlm_toolkit.callbacks import BaseCallback

class RetryHandler(BaseCallback):
    def __init__(self, max_retries=3, backoff=2.0):
        self.max_retries = max_retries
        self.backoff = backoff
        self.retry_count = 0
    
    def on_retry(self, attempt, max_attempts, error, **kwargs):
        wait_time = self.backoff ** attempt
        print(f"‚ö†Ô∏è –†–µ—Ç—Ä–∞–π {attempt}/{max_attempts} —á–µ—Ä–µ–∑ {wait_time}s: {error}")
        time.sleep(wait_time)
        self.retry_count += 1
    
    def on_error(self, error, **kwargs):
        print(f"‚ùå –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ –ø–æ—Å–ª–µ {self.retry_count} —Ä–µ—Ç—Ä–∞–µ–≤: {error}")

rlm = RLM.from_openai("gpt-4o", callbacks=[RetryHandler()])
```

---

## –ß–∞—Å—Ç—å 5: –§–∞–π–ª–æ–≤—ã–π –ª–æ–≥–≥–µ—Ä

### 5.1 JSONL –ª–æ–≥

```python
import json
from datetime import datetime
from rlm_toolkit.callbacks import BaseCallback

class JSONLLogger(BaseCallback):
    def __init__(self, path="rlm_logs.jsonl"):
        self.path = path
        self.file = open(path, "a")
    
    def _log(self, event_type, data):
        entry = {
            "timestamp": datetime.now().isoformat(),
            "event": event_type,
            **data
        }
        self.file.write(json.dumps(entry) + "\n")
        self.file.flush()
    
    def on_llm_start(self, prompt, **kwargs):
        self._log("llm_start", {"prompt": prompt[:200]})
    
    def on_llm_end(self, response, **kwargs):
        self._log("llm_end", {
            "tokens": response.usage.total_tokens,
            "response": response.content[:200]
        })
    
    def on_tool_start(self, tool_name, tool_input, **kwargs):
        self._log("tool_start", {"tool": tool_name, "input": str(tool_input)[:100]})
    
    def on_error(self, error, **kwargs):
        self._log("error", {"error": str(error)})
    
    def close(self):
        self.file.close()

logger = JSONLLogger("session.jsonl")
rlm = RLM.from_openai("gpt-4o", callbacks=[logger])
```

---

## –ß–∞—Å—Ç—å 6: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ Callbacks

```python
from rlm_toolkit.callbacks import ConsoleCallback

callbacks = [
    SimpleLogger(),
    MetricsCollector(),
    JSONLLogger("full_log.jsonl")
]

rlm = RLM.from_openai("gpt-4o", callbacks=callbacks)
```

---

## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

–¢–µ–ø–µ—Ä—å —É –≤–∞—Å –ø–æ–ª–Ω–∞—è –≤–∏–¥–∏–º–æ—Å—Ç—å –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å:
- ‚úÖ Real-time –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
- ‚úÖ –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫
- ‚úÖ Streaming UI
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ—Ç—Ä–∞–∏
- ‚úÖ –ü–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–µ –ª–æ–≥–∏

---

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

- [–ö–æ–Ω—Ü–µ–ø—Ü–∏—è: Callbacks](../concepts/callbacks.md)
- [–¢—É—Ç–æ—Ä–∏–∞–ª: Observability](12-observability.md)
- [–ö–æ–Ω—Ü–µ–ø—Ü–∏—è: Agents](../concepts/agents.md)
