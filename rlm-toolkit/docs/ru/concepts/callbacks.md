# Callbacks

![Version](https://img.shields.io/badge/version-1.2.1-blue)

> **Event hooks** –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏–∏

## –û–±–∑–æ—Ä

Callbacks –ø–æ–∑–≤–æ–ª—è—é—Ç –ø–æ–¥–∫–ª—é—á–∞—Ç—å—Å—è –∫ —Å–æ–±—ã—Ç–∏—è–º –∂–∏–∑–Ω–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ RLM:
- LLM –∑–∞–ø—Ä–æ—Å—ã/–æ—Ç–≤–µ—Ç—ã
- Tool –≤—ã–∑–æ–≤—ã
- –û–ø–µ—Ä–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
- –û—à–∏–±–∫–∏ –∏ —Ä–µ—Ç—Ä–∞–∏

## –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

```python
from rlm_toolkit import RLM
from rlm_toolkit.callbacks import BaseCallback

class LoggingCallback(BaseCallback):
    def on_llm_start(self, prompt, **kwargs):
        print(f"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞: {prompt[:50]}...")
    
    def on_llm_end(self, response, **kwargs):
        print(f"üì• –ü–æ–ª—É—á–µ–Ω–æ: {response.content[:50]}...")
    
    def on_error(self, error, **kwargs):
        print(f"‚ùå –û—à–∏–±–∫–∞: {error}")

rlm = RLM.from_openai("gpt-4o", callbacks=[LoggingCallback()])
result = rlm.run("–ü—Ä–∏–≤–µ—Ç!")
```

## –°–æ–±—ã—Ç–∏—è Callback

| –°–æ–±—ã—Ç–∏–µ | –ö–æ–≥–¥–∞ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç |
|---------|-------------------|
| `on_llm_start` | –ü–µ—Ä–µ–¥ –≤—ã–∑–æ–≤–æ–º LLM |
| `on_llm_end` | –ü–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ LLM |
| `on_tool_start` | –ü–µ—Ä–µ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º tool |
| `on_tool_end` | –ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è tool |
| `on_memory_store` | –ü—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –ø–∞–º—è—Ç—å |
| `on_memory_recall` | –ü—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–∑ –ø–∞–º—è—Ç–∏ |
| `on_retry` | –ü—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ —Ä–µ—Ç—Ä–∞—è |
| `on_error` | –ü—Ä–∏ –æ—à–∏–±–∫–µ |

## –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ Callbacks

### ConsoleCallback

```python
from rlm_toolkit.callbacks import ConsoleCallback

callback = ConsoleCallback(
    verbose=True,
    show_tokens=True,
    show_cost=True
)

rlm = RLM.from_openai("gpt-4o", callbacks=[callback])
```

### StreamingCallback

```python
from rlm_toolkit.callbacks import StreamingCallback

def print_token(token):
    print(token, end="", flush=True)

callback = StreamingCallback(on_token=print_token)
rlm = RLM.from_openai("gpt-4o", callbacks=[callback])
```

### MetricsCallback

```python
from rlm_toolkit.callbacks import MetricsCallback

callback = MetricsCallback()
rlm = RLM.from_openai("gpt-4o", callbacks=[callback])

# –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å—ã
rlm.run("–ó–∞–ø—Ä–æ—Å 1")
rlm.run("–ó–∞–ø—Ä–æ—Å 2")

# –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
metrics = callback.get_metrics()
print(f"–í—Å–µ–≥–æ –≤—ã–∑–æ–≤–æ–≤: {metrics['total_calls']}")
print(f"–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {metrics['total_tokens']}")
print(f"–°—Ä–µ–¥–Ω—è—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: {metrics['avg_latency_ms']}ms")
```

### FileLogCallback

```python
from rlm_toolkit.callbacks import FileLogCallback

callback = FileLogCallback(
    log_path="./logs/rlm.jsonl",
    include_prompts=True,
    include_responses=True
)

rlm = RLM.from_openai("gpt-4o", callbacks=[callback])
```

## –ö–∞—Å—Ç–æ–º–Ω—ã–µ Callbacks

### –ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä

```python
from rlm_toolkit.callbacks import BaseCallback
import time

class DetailedCallback(BaseCallback):
    def __init__(self):
        self.call_count = 0
        self.total_tokens = 0
        self.errors = []
        self.start_time = None
    
    def on_llm_start(self, prompt, **kwargs):
        self.start_time = time.time()
        self.call_count += 1
        print(f"[{self.call_count}] –ó–∞–ø—É—Å–∫ LLM –≤—ã–∑–æ–≤–∞...")
    
    def on_llm_end(self, response, **kwargs):
        duration = time.time() - self.start_time
        tokens = response.usage.total_tokens
        self.total_tokens += tokens
        print(f"[{self.call_count}] –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {duration:.2f}s ({tokens} —Ç–æ–∫–µ–Ω–æ–≤)")
    
    def on_tool_start(self, tool_name, tool_input, **kwargs):
        print(f"üîß Tool: {tool_name}({tool_input})")
    
    def on_tool_end(self, tool_name, tool_output, **kwargs):
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç tool: {tool_output[:100]}...")
    
    def on_memory_store(self, content, **kwargs):
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {content[:50]}...")
    
    def on_memory_recall(self, query, results, **kwargs):
        print(f"üîç –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(results)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è: {query}")
    
    def on_error(self, error, **kwargs):
        self.errors.append(str(error))
        print(f"‚ùå –û—à–∏–±–∫–∞: {error}")
    
    def on_retry(self, attempt, max_attempts, error, **kwargs):
        print(f"üîÑ –†–µ—Ç—Ä–∞–π {attempt}/{max_attempts}: {error}")
    
    def summary(self):
        return {
            "calls": self.call_count,
            "tokens": self.total_tokens,
            "errors": len(self.errors)
        }
```

### Async Callback

```python
from rlm_toolkit.callbacks import AsyncBaseCallback

class AsyncLoggingCallback(AsyncBaseCallback):
    async def on_llm_start(self, prompt, **kwargs):
        await self.log_async(f"–ó–∞–ø—É—Å–∫: {prompt[:50]}...")
    
    async def on_llm_end(self, response, **kwargs):
        await self.log_async(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ: {response.content[:50]}...")
    
    async def log_async(self, message):
        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ª–æ–≥ –≤–æ –≤–Ω–µ—à–Ω–∏–π —Å–µ—Ä–≤–∏—Å
        async with aiohttp.ClientSession() as session:
            await session.post(
                "https://logging-service.com/log",
                json={"message": message}
            )
```

## –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ Callbacks

```python
from rlm_toolkit.callbacks import (
    ConsoleCallback,
    MetricsCallback,
    FileLogCallback
)

callbacks = [
    ConsoleCallback(verbose=True),
    MetricsCallback(),
    FileLogCallback(log_path="./session.jsonl")
]

rlm = RLM.from_openai("gpt-4o", callbacks=callbacks)
```

## –°–≤—è–∑–∞–Ω–Ω–æ–µ

- [Observability](observability.md)
- [Agents](agents.md)
- [–¢—É—Ç–æ—Ä–∏–∞–ª: –ü–µ—Ä–≤–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ](../tutorials/01-first-app.md)
