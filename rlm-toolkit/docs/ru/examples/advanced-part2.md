# –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã - –ß–∞—Å—Ç—å 2

R&D –∏ –ø–µ—Ä–µ–¥–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ RLM-Toolkit.

---

## 6. –°–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É—é—â–∏–π—Å—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–¥–∞

R-Zero –ø–∞—Ç—Ç–µ—Ä–Ω, –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–ª—É—á—à–∞—é—â–∏–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–¥ —á–µ—Ä–µ–∑ —Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫—É.

```python
from rlm_toolkit import RLM
from rlm_toolkit.evolve import SelfEvolvingRLM
from rlm_toolkit.tools import PythonREPL
from pydantic import BaseModel
from typing import List, Optional, Tuple
import json

class CodeQuality(BaseModel):
    correctness: float
    efficiency: float
    readability: float
    test_coverage: float
    overall: float
    issues: List[str] = []

class CodeIteration(BaseModel):
    version: int
    code: str
    quality: CodeQuality
    improvements: List[str]

class SelfImprovingCodeGenerator:
    """
    –°–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É—é—â–∏–π—Å—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–¥–∞ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É R-Zero Challenger-Solver:
    1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–∞—á–∞–ª—å–Ω—ã–π –∫–æ–¥
    2. Challenger –∫—Ä–∏—Ç–∏–∫—É–µ—Ç –∏ –Ω–∞—Ö–æ–¥–∏—Ç –ø—Ä–æ–±–ª–µ–º—ã
    3. Solver —É–ª—É—á—à–∞–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫—Ä–∏—Ç–∏–∫–∏
    4. –ü–æ–≤—Ç–æ—Ä—è–µ—Ç –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø–æ—Ä–æ–≥–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    """
    
    def __init__(self, max_iterations: int = 5, quality_threshold: float = 0.9):
        self.max_iterations = max_iterations
        self.quality_threshold = quality_threshold
        
        # –ù–∞—á–∞–ª—å–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
        self.generator = RLM.from_openai("gpt-4o")
        self.generator.set_system_prompt("""
        –í—ã —ç–∫—Å–ø–µ—Ä—Ç Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫. –ì–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ —á–∏—Å—Ç—ã–π, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –∫–æ–¥.
        –í–∫–ª—é—á–∞–π—Ç–µ type hints, docstrings –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫.
        """)
        
        # Challenger - –∫—Ä–∏—Ç–∏–∫—É–µ—Ç –∫–æ–¥
        self.challenger = RLM.from_anthropic("claude-3-opus")
        self.challenger.set_system_prompt("""
        –í—ã –∂—ë—Å—Ç–∫–∏–π code reviewer. –ù–∞–π–¥–∏—Ç–µ –í–°–ï –ø—Ä–æ–±–ª–µ–º—ã:
        - –ë–∞–≥–∏ –∏ –∫—Ä–∞–π–Ω–∏–µ —Å–ª—É—á–∞–∏
        - –ü—Ä–æ–±–ª–µ–º—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        - –í–æ–ø—Ä–æ—Å—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        - –ù–∞—Ä—É—à–µ–Ω–∏—è —Å—Ç–∏–ª—è
        - –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–µ—Å—Ç—ã
        
        –ë—É–¥—å—Ç–µ –±–µ—Å–ø–æ—â–∞–¥–Ω—ã –Ω–æ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã.
        """)
        
        # Solver - —É–ª—É—á—à–∞–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫—Ä–∏—Ç–∏–∫–∏
        self.solver = RLM.from_openai("gpt-4o")
        self.solver.set_system_prompt("""
        –í—ã —É–ª—É—á—à–∞–µ—Ç–µ –∫–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç reviewer.
        –ò—Å–ø—Ä–∞–≤—å—Ç–µ –≤—Å–µ –ø–æ–¥–Ω—è—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã, —Å–æ—Ö—Ä–∞–Ω—è—è —Ä–∞–±–æ—Ç–∞—é—â–∏–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª.
        """)
        
        # –¢–µ—Å—Ç–µ—Ä - –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–¥
        self.repl = PythonREPL()
        
    def generate(self, task: str) -> dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ —Å –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–º —É–ª—É—á—à–µ–Ω–∏–µ–º."""
        
        print(f"üéØ –ó–∞–¥–∞—á–∞: {task}\n")
        
        iterations: List[CodeIteration] = []
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞—á–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
        print("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞—á–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏...")
        
        initial_code = self.generator.run(f"""
        –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–µ–µ:
        
        {task}
        
        –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
        - –ß–∏—Å—Ç—ã–π Python 3.10+
        - Type hints –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã
        - Docstrings –¥–ª—è –≤—Å–µ—Ö –ø—É–±–ª–∏—á–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
        - –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
        - –í–∫–ª—é—á–∏—Ç–µ —Ç–µ—Å—Ç—ã
        
        –í–µ—Ä–Ω–∏—Ç–µ —Ç–æ–ª—å–∫–æ –∫–æ–¥ Python.
        """)
        
        current_code = self._extract_code(initial_code)
        
        for iteration in range(1, self.max_iterations + 1):
            print(f"\nüîÑ –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration}/{self.max_iterations}")
            
            # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞
            print("  üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ...")
            test_result = self._test_code(current_code)
            
            # Challenger –∫—Ä–∏—Ç–∏–∫—É–µ—Ç
            print("  üîç Challenger –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç...")
            
            critique = self.challenger.run(f"""
            –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥:
            
            ```python
            {current_code}
            ```
            
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–æ–≤:
            {test_result}
            
            –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ:
            1. –û—Ü–µ–Ω–∫–∏ (0-1): correctness, efficiency, readability, test_coverage
            2. –°–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º
            3. –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è
            
            –§–æ—Ä–º–∞—Ç JSON:
            {{
                "scores": {{"correctness": 0.8, "efficiency": 0.7, "readability": 0.9, "test_coverage": 0.6}},
                "issues": ["–ø—Ä–æ–±–ª–µ–º–∞ 1", "–ø—Ä–æ–±–ª–µ–º–∞ 2"],
                "improvements": ["—É–ª—É—á—à–µ–Ω–∏–µ 1", "—É–ª—É—á—à–µ–Ω–∏–µ 2"]
            }}
            """)
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ü–µ–Ω–∫–∏
            quality = self._parse_quality(critique)
            
            print(f"  üìä –ö–∞—á–µ—Å—Ç–≤–æ: {quality.overall:.2f}")
            
            iterations.append(CodeIteration(
                version=iteration,
                code=current_code,
                quality=quality,
                improvements=[]
            ))
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø–æ—Ä–æ–≥–∞
            if quality.overall >= self.quality_threshold:
                print(f"  ‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ø–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞!")
                break
            
            # Solver —É–ª—É—á—à–∞–µ—Ç
            print("  üõ†Ô∏è Solver —É–ª—É—á—à–∞–µ—Ç...")
            
            improved = self.solver.run(f"""
            –£–ª—É—á—à–∏—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏:
            
            –ö–æ–¥:
            ```python
            {current_code}
            ```
            
            –ü—Ä–æ–±–ª–µ–º—ã:
            {json.dumps(quality.issues, ensure_ascii=False)}
            
            –ò—Å–ø—Ä–∞–≤—å—Ç–µ –í–°–ï –ø–æ–¥–Ω—è—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã.
            –í–µ—Ä–Ω–∏—Ç–µ —Ç–æ–ª—å–∫–æ —É–ª—É—á—à–µ–Ω–Ω—ã–π –∫–æ–¥.
            """)
            
            current_code = self._extract_code(improved)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        final_quality = iterations[-1].quality
        
        return {
            "code": current_code,
            "quality": final_quality.overall,
            "iterations": len(iterations),
            "history": [
                {
                    "version": i.version,
                    "quality": i.quality.overall,
                    "issues_count": len(i.quality.issues)
                }
                for i in iterations
            ]
        }
    
    def _extract_code(self, response: str) -> str:
        """–ò–∑–≤–ª–µ—á—å –∫–æ–¥ –∏–∑ –æ—Ç–≤–µ—Ç–∞."""
        if "```python" in response:
            start = response.find("```python") + 9
            end = response.find("```", start)
            return response[start:end].strip()
        return response.strip()
    
    def _test_code(self, code: str) -> str:
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –∫–æ–¥ –∏ –≤–µ—Ä–Ω—É—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç."""
        try:
            result = self.repl.run(code)
            return f"–£—Å–ø–µ—Ö: {result[:500]}" if result else "–£—Å–ø–µ—Ö: –∫–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω"
        except Exception as e:
            return f"–û—à–∏–±–∫–∞: {str(e)}"
    
    def _parse_quality(self, critique: str) -> CodeQuality:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑ –æ—Ç–≤–µ—Ç–∞."""
        try:
            # –ò–∑–≤–ª–µ—á—å JSON
            start = critique.find("{")
            end = critique.rfind("}") + 1
            data = json.loads(critique[start:end])
            
            scores = data.get("scores", {})
            issues = data.get("issues", [])
            
            return CodeQuality(
                correctness=scores.get("correctness", 0.5),
                efficiency=scores.get("efficiency", 0.5),
                readability=scores.get("readability", 0.5),
                test_coverage=scores.get("test_coverage", 0.5),
                overall=sum(scores.values()) / len(scores) if scores else 0.5,
                issues=issues
            )
        except:
            return CodeQuality(
                correctness=0.5,
                efficiency=0.5,
                readability=0.5,
                test_coverage=0.5,
                overall=0.5,
                issues=["–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ—Ü–µ–Ω–∫—É"]
            )

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    generator = SelfImprovingCodeGenerator(
        max_iterations=5,
        quality_threshold=0.85
    )
    
    result = generator.generate("""
    –°–æ–∑–¥–∞–π—Ç–µ –∫–ª–∞—Å—Å LRU Cache —Å:
    - get(key) - O(1)
    - put(key, value) - O(1)
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ TTL (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    - –ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
    - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (hits/misses)
    """)
    
    print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç ===")
    print(f"–ò—Ç–µ—Ä–∞—Ü–∏–π: {result['iterations']}")
    print(f"–§–∏–Ω–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {result['quality']:.2f}")
    print(f"\n--- –ö–æ–¥ ---\n{result['code'][:1000]}...")
```

---

## 7. –ü–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–æ–≤ –∑–Ω–∞–Ω–∏–π –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

```python
from rlm_toolkit import RLM
from rlm_toolkit.embeddings import OpenAIEmbeddings
from rlm_toolkit.loaders import DirectoryLoader
from neo4j import GraphDatabase
from pydantic import BaseModel
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import json
import hashlib

class Entity(BaseModel):
    id: str
    name: str
    type: str  # PERSON, ORG, CONCEPT, TECH, EVENT
    description: str
    attributes: Dict[str, str] = {}
    source_chunks: List[str] = []

class Relationship(BaseModel):
    source_id: str
    target_id: str
    type: str  # WORKS_FOR, USES, RELATES_TO, PART_OF, etc.
    description: str
    confidence: float
    evidence: str

@dataclass
class GraphStats:
    total_entities: int
    total_relationships: int
    entity_types: Dict[str, int]
    relationship_types: Dict[str, int]

class KnowledgeGraphBuilder:
    """
    –ü–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –≥—Ä–∞—Ñ–æ–≤ –∑–Ω–∞–Ω–∏–π –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
    1. –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ (–ª—é–¥–∏, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏, –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏)
    2. –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏
    3. –†–∞–∑—Ä–µ—à–∞–µ—Ç –∫–æ—Ä—Ä–µ—Ñ–µ—Ä–µ–Ω—Ü–∏–∏
    4. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ Neo4j
    5. –ü–æ–∑–≤–æ–ª—è–µ—Ç –¥–µ–ª–∞—Ç—å –≥—Ä–∞—Ñ–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    """
    
    def __init__(self, neo4j_uri: str, neo4j_user: str, neo4j_password: str):
        # –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å—É—â–Ω–æ—Å—Ç–µ–π
        self.entity_extractor = RLM.from_openai("gpt-4o")
        self.entity_extractor.set_system_prompt("""
        –í—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é —Å—É—â–Ω–æ—Å—Ç–µ–π. –ò–∑–≤–ª–µ–∫–∏—Ç–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞:
        - PERSON: –ª—é–¥–∏, –∞–≤—Ç–æ—Ä—ã, —ç–∫—Å–ø–µ—Ä—Ç—ã
        - ORG: –∫–æ–º–ø–∞–Ω–∏–∏, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏, –∫–æ–º–∞–Ω–¥—ã
        - CONCEPT: –∏–¥–µ–∏, –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏, —Ç–µ–æ—Ä–∏–∏
        - TECH: —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, —è–∑—ã–∫–∏
        - EVENT: —Å–æ–±—ã—Ç–∏—è, —Ä–µ–ª–∏–∑—ã, –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏
        
        –ë—É–¥—å—Ç–µ —Ç–æ—á–Ω—ã –∏ –∏–∑–±–µ–≥–∞–π—Ç–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è.
        """)
        
        # –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å–≤—è–∑–µ–π
        self.relationship_extractor = RLM.from_anthropic("claude-3-sonnet")
        self.relationship_extractor.set_system_prompt("""
        –í—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏.
        –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ —Ç–∏–ø —Å–≤—è–∑–∏ –∏ –µ—ë —Å–∏–ª—É (confidence 0-1).
        
        –¢–∏–ø—ã —Å–≤—è–∑–µ–π:
        - WORKS_FOR: —Ç—Ä—É–¥–æ–≤—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è
        - CREATED: –∞–≤—Ç–æ—Ä—Å—Ç–≤–æ, —Å–æ–∑–¥–∞–Ω–∏–µ
        - USES: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
        - PART_OF: —á–∞—Å—Ç—å —á–µ–≥–æ-—Ç–æ
        - RELATES_TO: –æ–±—â–∞—è —Å–≤—è–∑—å
        - COMPETES_WITH: –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è
        - DEPENDS_ON: –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å
        """)
        
        # –†–µ–∑–æ–ª–≤–µ—Ä –∫–æ—Ä—Ä–µ—Ñ–µ—Ä–µ–Ω—Ü–∏–π
        self.coreference_resolver = RLM.from_openai("gpt-4o-mini")
        
        # –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä Cypher –∑–∞–ø—Ä–æ—Å–æ–≤
        self.query_generator = RLM.from_openai("gpt-4o")
        
        # Neo4j –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
        self.driver = GraphDatabase.driver(
            neo4j_uri,
            auth=(neo4j_user, neo4j_password)
        )
        
        # –ö—ç—à —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        self.entity_cache: Dict[str, Entity] = {}
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        self.embeddings = OpenAIEmbeddings("text-embedding-3-large")
    
    def build_from_documents(self, directory: str, file_pattern: str = "**/*.md") -> GraphStats:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑: {directory}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        loader = DirectoryLoader(directory, glob=file_pattern)
        docs = loader.load()
        
        print(f"   –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docs)}")
        
        all_entities = []
        all_relationships = []
        
        for i, doc in enumerate(docs):
            print(f"\nüìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ [{i+1}/{len(docs)}]: {doc.metadata.get('source', 'unknown')}")
            
            # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self._chunk_document(doc.page_content)
            
            for chunk_id, chunk in enumerate(chunks):
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
                entities = self._extract_entities(chunk, doc.metadata)
                all_entities.extend(entities)
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≤—è–∑–µ–π
                if entities:
                    relationships = self._extract_relationships(chunk, entities)
                    all_relationships.extend(relationships)
        
        print(f"\nüîó –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ—Ñ–µ—Ä–µ–Ω—Ü–∏–π...")
        resolved_entities = self._resolve_coreferences(all_entities)
        
        print(f"üìä –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Neo4j...")
        self._save_to_neo4j(resolved_entities, all_relationships)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = self._compute_stats(resolved_entities, all_relationships)
        
        print(f"\n=== –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π –ø–æ—Å—Ç—Ä–æ–µ–Ω ===")
        print(f"   –°—É—â–Ω–æ—Å—Ç–µ–π: {stats.total_entities}")
        print(f"   –°–≤—è–∑–µ–π: {stats.total_relationships}")
        
        return stats
    
    def _chunk_document(self, text: str, chunk_size: int = 2000) -> List[str]:
        """–†–∞–∑–±–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏."""
        chunks = []
        sentences = text.split(". ")
        
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            if current_length + len(sentence) > chunk_size and current_chunk:
                chunks.append(". ".join(current_chunk) + ".")
                current_chunk = []
                current_length = 0
            
            current_chunk.append(sentence)
            current_length += len(sentence)
        
        if current_chunk:
            chunks.append(". ".join(current_chunk))
        
        return chunks
    
    def _extract_entities(self, text: str, metadata: dict) -> List[Entity]:
        """–ò–∑–≤–ª–µ—á—å —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        
        response = self.entity_extractor.run(f"""
        –ò–∑–≤–ª–µ–∫–∏—Ç–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞:
        
        {text[:3000]}
        
        –§–æ—Ä–º–∞—Ç JSON:
        [
            {{"name": "OpenAI", "type": "ORG", "description": "–ö–æ–º–ø–∞–Ω–∏—è –ø–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ò–ò"}},
            {{"name": "GPT-4", "type": "TECH", "description": "–ë–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å"}}
        ]
        """)
        
        entities = []
        try:
            data = json.loads(self._extract_json(response))
            
            for item in data:
                entity_id = self._generate_id(item["name"], item["type"])
                
                entity = Entity(
                    id=entity_id,
                    name=item["name"],
                    type=item["type"],
                    description=item.get("description", ""),
                    source_chunks=[text[:200]]
                )
                entities.append(entity)
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π: {e}")
        
        return entities
    
    def _extract_relationships(self, text: str, entities: List[Entity]) -> List[Relationship]:
        """–ò–∑–≤–ª–µ—á—å —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏."""
        
        entity_names = [e.name for e in entities]
        
        response = self.relationship_extractor.run(f"""
        –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —ç—Ç–∏–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ç–µ–∫—Å—Ç–∞:
        
        –°—É—â–Ω–æ—Å—Ç–∏: {entity_names}
        
        –¢–µ–∫—Å—Ç: {text[:2000]}
        
        –§–æ—Ä–º–∞—Ç JSON:
        [
            {{
                "source": "OpenAI",
                "target": "GPT-4",
                "type": "CREATED",
                "description": "OpenAI —Å–æ–∑–¥–∞–ª–∞ GPT-4",
                "confidence": 0.95
            }}
        ]
        """)
        
        relationships = []
        try:
            data = json.loads(self._extract_json(response))
            
            # –°–æ–∑–¥–∞—ë–º –º–∞–ø–ø–∏–Ω–≥ –∏–º—ë–Ω –Ω–∞ ID
            name_to_id = {e.name: e.id for e in entities}
            
            for item in data:
                source_id = name_to_id.get(item["source"])
                target_id = name_to_id.get(item["target"])
                
                if source_id and target_id:
                    rel = Relationship(
                        source_id=source_id,
                        target_id=target_id,
                        type=item["type"],
                        description=item.get("description", ""),
                        confidence=item.get("confidence", 0.5),
                        evidence=text[:200]
                    )
                    relationships.append(rel)
                    
        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–≤—è–∑–µ–π: {e}")
        
        return relationships
    
    def _resolve_coreferences(self, entities: List[Entity]) -> List[Entity]:
        """–û–±—ä–µ–¥–∏–Ω–∏—Ç—å –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Å—É—â–Ω–æ—Å—Ç–∏."""
        
        if not entities:
            return []
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø—É
        by_type: Dict[str, List[Entity]] = {}
        for entity in entities:
            by_type.setdefault(entity.type, []).append(entity)
        
        resolved = []
        
        for entity_type, type_entities in by_type.items():
            if len(type_entities) <= 1:
                resolved.extend(type_entities)
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–º—ë–Ω
            names = [e.name for e in type_entities]
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ—Ö–æ–∂–∏—Ö
            response = self.coreference_resolver.run(f"""
            –°–≥—Ä—É–ø–ø–∏—Ä—É–π—Ç–µ —ç—Ç–∏ —Å—É—â–Ω–æ—Å—Ç–∏ —Ç–∏–ø–∞ {entity_type}, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ –æ–¥–Ω–æ–º—É –æ–±—ä–µ–∫—Ç—É:
            
            {names}
            
            –§–æ—Ä–º–∞—Ç JSON - —Å–ø–∏—Å–æ–∫ –≥—Ä—É–ø–ø:
            [["GPT-4", "GPT4", "gpt-4o"], ["Claude", "Claude 3"]]
            
            –ï—Å–ª–∏ —Å—É—â–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã–µ, –ø–æ–º–µ—Å—Ç–∏—Ç–µ –∫–∞–∂–¥—É—é –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –≥—Ä—É–ø–ø—É.
            """)
            
            try:
                groups = json.loads(self._extract_json(response))
                
                for group in groups:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –≤ –≥—Ä—É–ø–ø–µ
                    group_entities = [e for e in type_entities if e.name in group]
                    
                    if group_entities:
                        # –ë–µ—Ä—ë–º —Å—É—â–Ω–æ—Å—Ç—å —Å —Å–∞–º—ã–º –¥–ª–∏–Ω–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º
                        merged = max(group_entities, key=lambda e: len(e.description))
                        
                        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —á–∞–Ω–∫–∏
                        for e in group_entities:
                            merged.source_chunks.extend(e.source_chunks)
                        
                        resolved.append(merged)
                        
            except:
                resolved.extend(type_entities)
        
        return resolved
    
    def _save_to_neo4j(self, entities: List[Entity], relationships: List[Relationship]):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≥—Ä–∞—Ñ –≤ Neo4j."""
        
        with self.driver.session() as session:
            # –û—á–∏—Å—Ç–∫–∞
            session.run("MATCH (n) DETACH DELETE n")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
            for entity in entities:
                session.run("""
                    CREATE (e:Entity {
                        id: $id,
                        name: $name,
                        type: $type,
                        description: $description
                    })
                """, id=entity.id, name=entity.name, 
                    type=entity.type, description=entity.description)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π
            for rel in relationships:
                session.run("""
                    MATCH (a:Entity {id: $source_id})
                    MATCH (b:Entity {id: $target_id})
                    CREATE (a)-[r:RELATES {
                        type: $type,
                        description: $description,
                        confidence: $confidence
                    }]->(b)
                """, source_id=rel.source_id, target_id=rel.target_id,
                    type=rel.type, description=rel.description, 
                    confidence=rel.confidence)
    
    def query(self, question: str) -> str:
        """–ó–∞–ø—Ä–æ—Å –∫ –≥—Ä–∞—Ñ—É –∑–Ω–∞–Ω–∏–π –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ."""
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Cypher –∑–∞–ø—Ä–æ—Å–∞
        cypher = self.query_generator.run(f"""
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–π—Ç–µ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å –≤ Cypher –∑–∞–ø—Ä–æ—Å –¥–ª—è Neo4j:
        
        –í–æ–ø—Ä–æ—Å: {question}
        
        –°—Ö–µ–º–∞:
        - –£–∑–ª—ã: Entity (id, name, type, description)
        - –°–≤—è–∑–∏: RELATES (type, description, confidence)
        
        –í–µ—Ä–Ω–∏—Ç–µ —Ç–æ–ª—å–∫–æ Cypher –∑–∞–ø—Ä–æ—Å.
        """)
        
        cypher = self._extract_code(cypher, "cypher")
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
        with self.driver.session() as session:
            try:
                result = session.run(cypher)
                records = list(result)
                
                if not records:
                    return "–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                formatted = []
                for record in records[:10]:
                    formatted.append(str(dict(record)))
                
                return "\n".join(formatted)
                
            except Exception as e:
                return f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}"
    
    def find_path(self, entity1: str, entity2: str, max_hops: int = 4) -> str:
        """–ù–∞–π—Ç–∏ –ø—É—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è —Å—É—â–Ω–æ—Å—Ç—è–º–∏."""
        
        with self.driver.session() as session:
            result = session.run(f"""
                MATCH path = shortestPath(
                    (a:Entity {{name: $name1}})-[*..{max_hops}]-(b:Entity {{name: $name2}})
                )
                RETURN path
            """, name1=entity1, name2=entity2)
            
            records = list(result)
            
            if not records:
                return f"–ü—É—Ç—å –º–µ–∂–¥—É {entity1} –∏ {entity2} –Ω–µ –Ω–∞–π–¥–µ–Ω."
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—É—Ç—å
            path = records[0]["path"]
            nodes = [node["name"] for node in path.nodes]
            
            return " ‚Üí ".join(nodes)
    
    def _generate_id(self, name: str, entity_type: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è ID —Å—É—â–Ω–æ—Å—Ç–∏."""
        content = f"{entity_type}:{name.lower()}"
        return hashlib.md5(content.encode()).hexdigest()[:12]
    
    def _extract_json(self, text: str) -> str:
        """–ò–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞."""
        if "[" in text:
            start = text.find("[")
            end = text.rfind("]") + 1
            return text[start:end]
        elif "{" in text:
            start = text.find("{")
            end = text.rfind("}") + 1
            return text[start:end]
        return text
    
    def _extract_code(self, text: str, lang: str = "") -> str:
        """–ò–∑–≤–ª–µ—á—å –∫–æ–¥ –∏–∑ markdown –±–ª–æ–∫–∞."""
        marker = f"```{lang}"
        if marker in text:
            start = text.find(marker) + len(marker)
            end = text.find("```", start)
            return text[start:end].strip()
        return text.strip()
    
    def _compute_stats(self, entities: List[Entity], relationships: List[Relationship]) -> GraphStats:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥—Ä–∞—Ñ–∞."""
        entity_types: Dict[str, int] = {}
        for e in entities:
            entity_types[e.type] = entity_types.get(e.type, 0) + 1
        
        rel_types: Dict[str, int] = {}
        for r in relationships:
            rel_types[r.type] = rel_types.get(r.type, 0) + 1
        
        return GraphStats(
            total_entities=len(entities),
            total_relationships=len(relationships),
            entity_types=entity_types,
            relationship_types=rel_types
        )

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    builder = KnowledgeGraphBuilder(
        neo4j_uri="bolt://localhost:7687",
        neo4j_user="neo4j",
        neo4j_password="password"
    )
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞
    stats = builder.build_from_documents("./docs")
    
    print(f"\n=== –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ===")
    print(f"–°—É—â–Ω–æ—Å—Ç–∏ –ø–æ —Ç–∏–ø–∞–º: {stats.entity_types}")
    print(f"–°–≤—è–∑–∏ –ø–æ —Ç–∏–ø–∞–º: {stats.relationship_types}")
    
    # –ó–∞–ø—Ä–æ—Å—ã –∫ –≥—Ä–∞—Ñ—É
    answer = builder.query("–ö–∞–∫–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç OpenAI?")
    print(f"\n--- –ó–∞–ø—Ä–æ—Å ---\n{answer}")
    
    # –ü–æ–∏—Å–∫ –ø—É—Ç–∏
    path = builder.find_path("Python", "Machine Learning")
    print(f"\n--- –ü—É—Ç—å ---\n{path}")
```

---

## 8. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ–¥—É

–ü–æ–∏—Å–∫ –ø–æ –∫–æ–¥–æ–≤–æ–π –±–∞–∑–µ –ø–æ —Å–º—ã—Å–ª—É, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫.

```python
from rlm_toolkit import RLM
from rlm_toolkit.embeddings import OpenAIEmbeddings
from rlm_toolkit.vectorstores import ChromaVectorStore
from pydantic import BaseModel
from typing import List, Dict, Optional
import ast
import os

class CodeElement(BaseModel):
    type: str  # function, class, method, module
    name: str
    signature: str
    docstring: Optional[str]
    code: str
    file_path: str
    line_number: int
    semantic_description: str  # –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ AI

class SearchResult(BaseModel):
    element: CodeElement
    similarity: float
    explanation: str

class SemanticCodeSearch:
    """
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ–¥–æ–≤–æ–π –±–∞–∑–µ:
    1. –ü–∞—Ä—Å–∏—Ç –∫–æ–¥ –≤ —ç–ª–µ–º–µ–Ω—Ç—ã (—Ñ—É–Ω–∫—Ü–∏–∏, –∫–ª–∞—Å—Å—ã)
    2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è —á–µ—Ä–µ–∑ LLM
    3. –°–æ–∑–¥–∞—ë—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
    4. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏
    """
    
    def __init__(self, project_path: str):
        self.project_path = project_path
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        self.embeddings = OpenAIEmbeddings("text-embedding-3-large")
        
        # –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        self.vectorstore = ChromaVectorStore(
            collection_name="code_search",
            embedding_function=self.embeddings,
            persist_directory="./code_search_db"
        )
        
        # –û–ø–∏—Å–∞—Ç–µ–ª—å –∫–æ–¥–∞
        self.describer = RLM.from_openai("gpt-4o")
        self.describer.set_system_prompt("""
        –í—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫–æ–¥–∞. –î–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∫–æ–¥–∞:
        1. –û–ø–∏—à–∏—Ç–µ —á—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º
        2. –û–±—ä—è—Å–Ω–∏—Ç–µ –∞–ª–≥–æ—Ä–∏—Ç–º/–ø–æ–¥—Ö–æ–¥
        3. –û—Ç–º–µ—Ç—å—Ç–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        4. –£–∫–∞–∂–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –ø–æ–±–æ—á–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã
        
        –ë—É–¥—å—Ç–µ –∫—Ä–∞—Ç–∫–∏ –Ω–æ –∏—Å—á–µ—Ä–ø—ã–≤–∞—é—â–∏.
        """)
        
        # –û–±—ä—è—Å–Ω–∏—Ç–µ–ª—å –ø–æ–∏—Å–∫–∞
        self.explainer = RLM.from_openai("gpt-4o-mini")
        
        # –ò–Ω–¥–µ–∫—Å
        self.elements: Dict[str, CodeElement] = {}
        
    def index_codebase(self):
        """–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –≤—Å—é –∫–æ–¥–æ–≤—É—é –±–∞–∑—É."""
        print(f"üìÇ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è {self.project_path}...")
        
        python_files = []
        for root, dirs, files in os.walk(self.project_path):
            # –ü—Ä–æ–ø—É—Å–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –Ω–µ-–∫–æ–¥–æ–≤—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
            dirs[:] = [d for d in dirs if d not in ['.git', '__pycache__', 'node_modules', '.venv', 'venv']]
            
            for file in files:
                if file.endswith('.py'):
                    python_files.append(os.path.join(root, file))
        
        total_elements = 0
        
        for file_path in python_files:
            print(f"  üìÑ {file_path}")
            elements = self._parse_file(file_path)
            
            for element in elements:
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è
                element.semantic_description = self._describe_code(element)
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                element_id = f"{element.file_path}:{element.name}"
                self.elements[element_id] = element
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
                search_text = f"""
                {element.type}: {element.name}
                {element.signature}
                {element.docstring or ''}
                {element.semantic_description}
                """
                
                self.vectorstore.add_texts(
                    [search_text],
                    metadatas=[{"id": element_id}]
                )
                
                total_elements += 1
        
        print(f"‚úÖ –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {total_elements} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞")
    
    def _parse_file(self, file_path: str) -> List[CodeElement]:
        """–ü–∞—Ä—Å–∏–Ω–≥ Python —Ñ–∞–π–ª–∞ –≤ —ç–ª–µ–º–µ–Ω—Ç—ã –∫–æ–¥–∞."""
        elements = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                source = f.read()
                tree = ast.parse(source)
            except:
                return []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                elements.append(self._extract_function(node, source, file_path))
            elif isinstance(node, ast.AsyncFunctionDef):
                elements.append(self._extract_function(node, source, file_path, is_async=True))
            elif isinstance(node, ast.ClassDef):
                elements.append(self._extract_class(node, source, file_path))
        
        return elements
    
    def _extract_function(self, node, source: str, file_path: str, is_async: bool = False) -> CodeElement:
        """–ò–∑–≤–ª–µ—á—å –¥–µ—Ç–∞–ª–∏ —Ñ—É–Ω–∫—Ü–∏–∏."""
        lines = source.split('\n')
        start = node.lineno - 1
        end = node.end_lineno if hasattr(node, 'end_lineno') else start + 1
        code = '\n'.join(lines[start:end])
        
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–∏–≥–Ω–∞—Ç—É—Ä—ã
        args = []
        for arg in node.args.args:
            arg_str = arg.arg
            if arg.annotation:
                arg_str += f": {ast.unparse(arg.annotation)}"
            args.append(arg_str)
        
        returns = ""
        if node.returns:
            returns = f" -> {ast.unparse(node.returns)}"
        
        prefix = "async def" if is_async else "def"
        signature = f"{prefix} {node.name}({', '.join(args)}){returns}"
        
        docstring = ast.get_docstring(node)
        
        return CodeElement(
            type="function",
            name=node.name,
            signature=signature,
            docstring=docstring,
            code=code,
            file_path=file_path,
            line_number=node.lineno,
            semantic_description=""
        )
    
    def _extract_class(self, node, source: str, file_path: str) -> CodeElement:
        """–ò–∑–≤–ª–µ—á—å –¥–µ—Ç–∞–ª–∏ –∫–ª–∞—Å—Å–∞."""
        lines = source.split('\n')
        start = node.lineno - 1
        end = node.end_lineno if hasattr(node, 'end_lineno') else start + 10
        code = '\n'.join(lines[start:min(end, start + 50)])
        
        bases = [ast.unparse(b) for b in node.bases]
        signature = f"class {node.name}({', '.join(bases)})" if bases else f"class {node.name}"
        
        docstring = ast.get_docstring(node)
        
        return CodeElement(
            type="class",
            name=node.name,
            signature=signature,
            docstring=docstring,
            code=code,
            file_path=file_path,
            line_number=node.lineno,
            semantic_description=""
        )
    
    def _describe_code(self, element: CodeElement) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è —á–µ—Ä–µ–∑ LLM."""
        return self.describer.run(f"""
        –û–ø–∏—à–∏—Ç–µ —ç—Ç–æ—Ç {element.type}:
        
        {element.signature}
        
        ```python
        {element.code[:1500]}
        ```
        
        –î–∞–π—Ç–µ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏—è —á—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç –∏ –∫–∞–∫.
        """)
    
    def search(self, query: str, k: int = 10) -> List[SearchResult]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ–¥–æ–≤–æ–π –±–∞–∑–µ."""
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ LLM
        enhanced_query = RLM.from_openai("gpt-4o-mini").run(f"""
        –†–∞—Å—à–∏—Ä—å—Ç–µ —ç—Ç–æ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ –∫–æ–¥—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏:
        
        –ó–∞–ø—Ä–æ—Å: {query}
        
        –î–æ–±–∞–≤—å—Ç–µ: —Å–∏–Ω–æ–Ω–∏–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã, –¥–µ—Ç–∞–ª–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏.
        –ù–µ –±–æ–ª–µ–µ 100 —Å–ª–æ–≤.
        """)
        
        # –ü–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        results = self.vectorstore.similarity_search_with_score(
            enhanced_query, 
            k=k
        )
        
        search_results = []
        for doc, score in results:
            element_id = doc.metadata.get("id")
            if element_id and element_id in self.elements:
                element = self.elements[element_id]
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
                explanation = self.explainer.run(f"""
                –û–±—ä—è—Å–Ω–∏—Ç–µ –ø–æ—á–µ–º—É —ç—Ç–æ—Ç –∫–æ–¥ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–ø—Ä–æ—Å—É "{query}":
                
                {element.signature}
                {element.semantic_description}
                
                –û–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ.
                """)
                
                search_results.append(SearchResult(
                    element=element,
                    similarity=1 - score,
                    explanation=explanation
                ))
        
        return search_results
    
    def find_similar(self, file_path: str, name: str, k: int = 5) -> List[SearchResult]:
        """–ù–∞–π—Ç–∏ –∫–æ–¥ –ø–æ—Ö–æ–∂–∏–π –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç."""
        element_id = f"{file_path}:{name}"
        
        if element_id not in self.elements:
            return []
        
        element = self.elements[element_id]
        
        return self.search(element.semantic_description, k=k+1)[1:]

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    search = SemanticCodeSearch("./src")
    
    # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã
    search.index_codebase()
    
    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
    results = search.search("—Ñ—É–Ω–∫—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ email –∞–¥—Ä–µ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
    
    print("\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ ===")
    for r in results[:5]:
        print(f"\nüìç {r.element.file_path}:{r.element.line_number}")
        print(f"   {r.element.signature}")
        print(f"   –°—Ö–æ–¥—Å—Ç–≤–æ: {r.similarity:.2f}")
        print(f"   {r.explanation}")
    
    # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–≥–æ –∫–æ–¥–∞
    similar = search.find_similar("./src/auth.py", "validate_password")
    print("\n=== –ü–æ—Ö–æ–∂–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===")
    for r in similar:
        print(f"  - {r.element.name}: {r.explanation}")
```

---

## 9. –°–∏—Å—Ç–µ–º–∞ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö –¥–µ–±–∞—Ç–æ–≤

–ê–≥–µ–Ω—Ç—ã –¥–µ–±–∞—Ç–∏—Ä—É—é—Ç –∏ –ø—Ä–∏—Ö–æ–¥—è—Ç –∫ –∫–æ–Ω—Å–µ–Ω—Å—É—Å—É —á–µ—Ä–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏—é.

```python
from rlm_toolkit import RLM
from rlm_toolkit.agents.multiagent import MetaMatrix, Agent
from rlm_toolkit.memory import BufferMemory
from pydantic import BaseModel
from typing import List, Optional, Dict
from enum import Enum
import json

class Position(str, Enum):
    STRONGLY_AGREE = "strongly_agree"
    AGREE = "agree"
    NEUTRAL = "neutral"
    DISAGREE = "disagree"
    STRONGLY_DISAGREE = "strongly_disagree"

class Argument(BaseModel):
    agent: str
    position: Position
    claim: str
    evidence: List[str]
    rebuttals: List[str] = []
    confidence: float

class DebateRound(BaseModel):
    round_number: int
    topic: str
    arguments: List[Argument]
    consensus_reached: bool
    consensus_position: Optional[Position]

class DebateResult(BaseModel):
    topic: str
    rounds: List[DebateRound]
    final_consensus: Optional[Position]
    synthesis: str
    dissenting_views: List[str]

class MultiAgentDebate:
    """
    –°–∏—Å—Ç–µ–º–∞ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö –¥–µ–±–∞—Ç–æ–≤:
    1. –ù–µ—Å–∫–æ–ª—å–∫–æ –∞–≥–µ–Ω—Ç–æ–≤ –∞—Ä–≥—É–º–µ–Ω—Ç–∏—Ä—É—é—Ç –ø–æ–∑–∏—Ü–∏–∏
    2. –ê–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –º–µ–Ω—è—Ç—å –ø–æ–∑–∏—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    3. –ú–æ–¥–µ—Ä–∞—Ç–æ—Ä –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –¥–∏—Å–∫—É—Å—Å–∏—é
    4. –°–∏—Å—Ç–µ–º–∞ —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Å–µ–Ω—Å—É—Å –∏–ª–∏ –≤—ã–¥–µ–ª—è–µ—Ç —Ä–∞–∑–Ω–æ–≥–ª–∞—Å–∏—è
    """
    
    def __init__(self, num_agents: int = 4):
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤-–¥–µ–±–∞—Ç—ë—Ä–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞–º–∏
        self.agents: Dict[str, Agent] = {}
        
        perspectives = [
            ("–ü—Ä–∞–≥–º–∞—Ç–∏–∫", "–§–æ–∫—É—Å –Ω–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è—Ö, —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞—Ö –∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö –≤–Ω–µ–¥—Ä–µ–Ω–∏—è."),
            ("–¢–µ–æ—Ä–µ—Ç–∏–∫", "–§–æ–∫—É—Å –Ω–∞ –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö, —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞—Ö –∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏."),
            ("–ê–¥–≤–æ–∫–∞—Ç –¥—å—è–≤–æ–ª–∞", "–û—Å–ø–∞—Ä–∏–≤–∞–µ—Ç –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è, –Ω–∞—Ö–æ–¥–∏—Ç –∫–æ–Ω—Ç—Ä–∞—Ä–≥—É–º–µ–Ω—Ç—ã, —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç–∏—Ä—É–µ—Ç –∏–¥–µ–∏."),
            ("–°–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä", "–ò—â–µ—Ç –æ–±—â—É—é –ø–æ—á–≤—É, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã, –Ω–∞—Ö–æ–¥–∏—Ç —Å—Ä–µ–¥–Ω–∏–π –ø—É—Ç—å."),
            ("–°–∫–µ–ø—Ç–∏–∫", "–¢—Ä–µ–±—É–µ—Ç –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, —Å—Ç–∞–≤–∏—Ç –ø–æ–¥ —Å–æ–º–Ω–µ–Ω–∏–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è, –≤—ã—è–≤–ª—è–µ—Ç –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏."),
            ("–í–∏–∑–∏–æ–Ω–µ—Ä", "–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è, –Ω–æ–≤—ã–µ —Ç—Ä–µ–Ω–¥—ã, –≤–æ–∑–º–æ–∂–Ω—ã–µ –±—É–¥—É—â–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏.")
        ]
        
        for i in range(min(num_agents, len(perspectives))):
            name, style = perspectives[i]
            
            agent = Agent(
                name=name.lower(),
                description=style,
                llm=RLM.from_openai("gpt-4o")
            )
            agent.llm.set_system_prompt(f"""
            –í—ã {name} –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–µ–±–∞—Ç–∞—Ö. –í–∞—à —Å—Ç–∏–ª—å:
            {style}
            
            –ü—Ä–∞–≤–∏–ª–∞:
            - –ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–π—Ç–µ —á—ë—Ç–∫–∏–µ, –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã
            - –ü—Ä–∏–∑–Ω–∞–≤–∞–π—Ç–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –ø—É–Ω–∫—Ç—ã –æ—Ç –¥—Ä—É–≥–∏—Ö
            - –ë—É–¥—å—Ç–µ –≥–æ—Ç–æ–≤—ã –æ–±–Ω–æ–≤–∏—Ç—å –ø–æ–∑–∏—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤—ã—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤
            - –û—Å—Ç–∞–≤–∞–π—Ç–µ—Å—å —É–≤–∞–∂–∏—Ç–µ–ª—å–Ω—ã–º –Ω–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ —Å—Ç—Ä–æ–≥–∏–º
            - –û—Ü–µ–Ω–∏–≤–∞–π—Ç–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å 0-1
            """)
            
            self.agents[name.lower()] = agent
        
        # –ú–æ–¥–µ—Ä–∞—Ç–æ—Ä
        self.moderator = RLM.from_anthropic("claude-3-opus")
        self.moderator.set_system_prompt("""
        –í—ã –º–æ–¥–µ—Ä–∞—Ç–æ—Ä –¥–µ–±–∞—Ç–æ–≤. –í–∞—à–∞ —Ä–æ–ª—å:
        1. –û–±–µ—Å–ø–µ—á–∏—Ç—å —á–µ—Å—Ç–Ω—É—é –¥–∏—Å–∫—É—Å—Å–∏—é
        2. –í—ã—è–≤–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏ —Å–æ–≥–ª–∞—Å–∏—è/–Ω–µ—Å–æ–≥–ª–∞—Å–∏—è
        3. –ó–∞–¥–∞–≤–∞—Ç—å —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã
        4. –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–≥–¥–∞ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω—Å–µ–Ω—Å—É—Å
        5. –°–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –≤—ã–≤–æ–¥—ã
        
        –ë—É–¥—å—Ç–µ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã –∏ —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –ø–æ–∏—Å–∫–µ –∏—Å—Ç–∏–Ω—ã.
        """)
        
    def debate(self, topic: str, max_rounds: int = 5) -> DebateResult:
        """–ü—Ä–æ–≤–µ—Å—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–µ–±–∞—Ç—ã –ø–æ —Ç–µ–º–µ."""
        
        print(f"üé§ –¢–µ–º–∞ –¥–µ–±–∞—Ç–æ–≤: {topic}\n")
        
        rounds = []
        
        for round_num in range(1, max_rounds + 1):
            print(f"=== –†–∞—É–Ω–¥ {round_num} ===")
            
            # –ö–∞–∂–¥—ã–π –∞–≥–µ–Ω—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç
            arguments = []
            previous_args = rounds[-1].arguments if rounds else []
            
            for name, agent in self.agents.items():
                print(f"  üó£Ô∏è {name.title()} –≤—ã—Å—Ç—É–ø–∞–µ—Ç...")
                
                context = f"–¢–µ–º–∞: {topic}\n\n"
                if previous_args:
                    context += "–ü—Ä–µ–¥—ã–¥—É—â–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã:\n"
                    for arg in previous_args:
                        context += f"- {arg.agent}: {arg.claim} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {arg.confidence})\n"
                
                response = agent.llm.run(f"""
                {context}
                
                –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ –≤–∞—à –∞—Ä–≥—É–º–µ–Ω—Ç –ø–æ —Ç–µ–º–µ: {topic}
                
                –£–∫–∞–∂–∏—Ç–µ:
                1. –í–∞—à—É –ø–æ–∑–∏—Ü–∏—é (strongly_agree/agree/neutral/disagree/strongly_disagree)
                2. –í–∞—à–µ –≥–ª–∞–≤–Ω–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
                3. –î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–µ –ø–æ–∑–∏—Ü–∏—é
                4. –í–æ–∑—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–µ –≤–∑–≥–ª—è–¥—ã (–µ—Å–ª–∏ –µ—Å—Ç—å)
                5. –£—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (0-1)
                
                –§–æ—Ä–º–∞—Ç JSON.
                """)
                
                try:
                    data = json.loads(response)
                    argument = Argument(
                        agent=name,
                        position=Position(data.get("position", "neutral")),
                        claim=data.get("claim", ""),
                        evidence=data.get("evidence", []),
                        rebuttals=data.get("rebuttals", []),
                        confidence=data.get("confidence", 0.5)
                    )
                    arguments.append(argument)
                except:
                    arguments.append(Argument(
                        agent=name,
                        position=Position.NEUTRAL,
                        claim=response[:200],
                        evidence=[],
                        confidence=0.5
                    ))
            
            # –ú–æ–¥–µ—Ä–∞—Ç–æ—Ä –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Å–µ–Ω—Å—É—Å
            print("  üßë‚Äç‚öñÔ∏è –ú–æ–¥–µ—Ä–∞—Ç–æ—Ä –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç...")
            
            consensus_check = self.moderator.run(f"""
            –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç—Ç–∏ –ø–æ–∑–∏—Ü–∏–∏ –≤ –¥–µ–±–∞—Ç–∞—Ö:
            
            {json.dumps([{"agent": a.agent, "position": a.position.value, "claim": a.claim, "confidence": a.confidence} for a in arguments], indent=2, ensure_ascii=False)}
            
            –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ:
            1. –ï—Å—Ç—å –ª–∏ –∫–æ–Ω—Å–µ–Ω—Å—É—Å? (–±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å–æ–≥–ª–∞—Å–Ω–æ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é)
            2. –ö–∞–∫–æ–≤–∞ –∫–æ–Ω—Å–µ–Ω—Å—É—Å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å?
            3. –ö–∞–∫–∏–µ –æ—Å—Ç–∞—é—Ç—Å—è —Ç–æ—á–∫–∏ —Ä–∞–∑–Ω–æ–≥–ª–∞—Å–∏–π?
            
            –í–µ—Ä–Ω–∏—Ç–µ JSON: {{"consensus": bool, "position": str –∏–ª–∏ null, "disagreements": [str]}}
            """)
            
            try:
                consensus_data = json.loads(consensus_check)
                consensus_reached = consensus_data.get("consensus", False)
                consensus_position = Position(consensus_data["position"]) if consensus_data.get("position") else None
            except:
                consensus_reached = False
                consensus_position = None
            
            round_result = DebateRound(
                round_number=round_num,
                topic=topic,
                arguments=arguments,
                consensus_reached=consensus_reached,
                consensus_position=consensus_position
            )
            rounds.append(round_result)
            
            if consensus_reached:
                print(f"  ‚úÖ –ö–æ–Ω—Å–µ–Ω—Å—É—Å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç: {consensus_position.value}")
                break
            else:
                print(f"  üîÑ –ö–æ–Ω—Å–µ–Ω—Å—É—Å –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º...")
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∏–Ω—Ç–µ–∑
        print("\nüìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ–∑–∞...")
        
        all_arguments = [arg for round in rounds for arg in round.arguments]
        
        synthesis = self.moderator.run(f"""
        –°–∏–Ω—Ç–µ–∑–∏—Ä—É–π—Ç–µ —ç—Ç–∏ –¥–µ–±–∞—Ç—ã –ø–æ —Ç–µ–º–µ: {topic}
        
        –í—Å–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã:
        {json.dumps([{"agent": a.agent, "position": a.position.value, "claim": a.claim} for a in all_arguments], indent=2, ensure_ascii=False)}
        
        –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ:
        1. –†–µ–∑—é–º–µ –≥–ª–∞–≤–Ω—ã—Ö –≤—ã–≤–æ–¥–æ–≤
        2. –¢–æ—á–∫–∏ —Å–æ–≥–ª–∞—Å–∏—è
        3. –û—Å—Ç–∞–≤—à–∏–µ—Å—è —Ä–∞–∑–Ω–æ–≥–ª–∞—Å–∏—è
        4. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        """)
        
        # –í—ã—è–≤–ª–µ–Ω–∏–µ –æ—Å–æ–±—ã—Ö –º–Ω–µ–Ω–∏–π
        final_round = rounds[-1]
        final_consensus = final_round.consensus_position
        
        dissenting = []
        if final_consensus:
            for arg in final_round.arguments:
                if arg.position != final_consensus and arg.confidence > 0.6:
                    dissenting.append(f"{arg.agent}: {arg.claim}")
        
        return DebateResult(
            topic=topic,
            rounds=rounds,
            final_consensus=final_consensus,
            synthesis=synthesis,
            dissenting_views=dissenting
        )
    
    def quick_consensus(self, question: str) -> str:
        """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞ –±–µ–∑ –ø–æ–ª–Ω—ã—Ö –¥–µ–±–∞—Ç–æ–≤."""
        responses = []
        
        for name, agent in self.agents.items():
            response = agent.llm.run(f"""
            –ë—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç: {question}
            
            –£–∫–∞–∂–∏—Ç–µ: –ø–æ–∑–∏—Ü–∏—è (agree/disagree), –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –≤ –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (0-1)
            """)
            responses.append(f"{name}: {response}")
        
        return self.moderator.run(f"""
        –ü–æ–¥–≤–µ–¥–∏—Ç–µ –∏—Ç–æ–≥ –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞ –ø–æ: {question}
        
        –û—Ç–≤–µ—Ç—ã:
        {chr(10).join(responses)}
        
        –£–∫–∞–∂–∏—Ç–µ: –ø–æ–∑–∏—Ü–∏—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞, —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏, –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∏—á–∏–Ω—ã
        """)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    debate = MultiAgentDebate(num_agents=4)
    
    result = debate.debate(
        topic="–î–æ–ª–∂–Ω—ã –ª–∏ AI —Å–∏—Å—Ç–µ–º—ã –ø—Ä–∏–Ω–∏–º–∞—Ç—å –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –≤ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏?",
        max_rounds=4
    )
    
    print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ–±–∞—Ç–æ–≤ ===")
    print(f"–¢–µ–º–∞: {result.topic}")
    print(f"–†–∞—É–Ω–¥–æ–≤: {len(result.rounds)}")
    print(f"–§–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Å–µ–Ω—Å—É—Å: {result.final_consensus}")
    print(f"\n–°–∏–Ω—Ç–µ–∑:\n{result.synthesis}")
    
    if result.dissenting_views:
        print(f"\n–û—Å–æ–±—ã–µ –º–Ω–µ–Ω–∏—è:")
        for view in result.dissenting_views:
            print(f"  - {view}")
```

---

## 10. –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (InfiniRetri)

–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ 1000+ —Å—Ç—Ä–∞–Ω–∏—Ü —Å —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–µ–π —á–µ—Ä–µ–∑ InfiniRetri.

```python
from rlm_toolkit import RLM, RLMConfig
from rlm_toolkit.retrieval import InfiniRetriConfig
from rlm_toolkit.loaders import PDFLoader
from rlm_toolkit.splitters import RecursiveTextSplitter
from rlm_toolkit.embeddings import OpenAIEmbeddings
from rlm_toolkit.vectorstores import ChromaVectorStore
from pydantic import BaseModel
from typing import List, Dict, Optional
import math

class SectionSummary(BaseModel):
    title: str
    page_range: str
    summary: str
    key_points: List[str]
    entities: List[str]

class DocumentSummary(BaseModel):
    title: str
    total_pages: int
    executive_summary: str
    section_summaries: List[SectionSummary]
    key_themes: List[str]
    recommendations: List[str]

class RecursiveDocumentSummarizer:
    """
    –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –º–∞—Å—Å–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (1000+ —Å—Ç—Ä–∞–Ω–∏—Ü):
    1. –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
    2. –†–µ–∫—É—Ä—Å–∏–≤–Ω–∞—è map-reduce —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
    3. InfiniRetri –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    4. –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—è
    """
    
    def __init__(self):
        # RLM —Å InfiniRetri –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self.config = RLMConfig(
            enable_infiniretri=True,
            infiniretri_config=InfiniRetriConfig(
                chunk_size=8000,
                top_k=10,
                overlap=1000
            ),
            infiniretri_threshold=50000
        )
        
        self.rlm = RLM.from_openai("gpt-4o", config=self.config)
        
        # –°—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–µ–∫—Ü–∏–π
        self.section_summarizer = RLM.from_openai("gpt-4o")
        self.section_summarizer.set_system_prompt("""
        –í—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –î–ª—è –∫–∞–∂–¥–æ–π —Å–µ–∫—Ü–∏–∏:
        1. –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –≥–ª–∞–≤–Ω—É—é —Ç–µ–º—É
        2. –ò–∑–≤–ª–µ–∫–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ –ø—É–Ω–∫—Ç—ã (–º–∞–∫—Å 5)
        3. –û—Ç–º–µ—Ç—å—Ç–µ –≤–∞–∂–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ (–ª—é–¥–∏, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏, —á–∏—Å–ª–∞)
        4. –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏
        
        –ë—É–¥—å—Ç–µ –∫—Ä–∞—Ç–∫–∏ –Ω–æ –∏—Å—á–µ—Ä–ø—ã–≤–∞—é—â–∏.
        """)
        
        # –ú–µ—Ç–∞-—Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å–∞–º–º–∞—Ä–∏
        self.meta_summarizer = RLM.from_anthropic("claude-3-opus")
        self.meta_summarizer.set_system_prompt("""
        –í—ã —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–∞–º–º–∞—Ä–∏ –≤ —Å–≤—è–∑–Ω—ã–π –Ω–∞—Ä—Ä–∞—Ç–∏–≤.
        - –£—Å—Ç—Ä–∞–Ω—è–π—Ç–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å
        - –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π—Ç–µ –ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ—Ç–æ–∫
        - –í—ã–¥–µ–ª—è–π—Ç–µ —Å–∫–≤–æ–∑–Ω—ã–µ —Ç–µ–º—ã
        - –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ –≤–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏
        """)
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
        self.embeddings = OpenAIEmbeddings("text-embedding-3-large")
        
    def summarize(self, pdf_path: str, target_length: str = "comprehensive") -> DocumentSummary:
        """
        –°—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª—å—à–æ–π –¥–æ–∫—É–º–µ–Ω—Ç.
        
        target_length: "brief" (1 —Å—Ç—Ä), "standard" (3-5 —Å—Ç—Ä), "comprehensive" (10+ —Å—Ç—Ä)
        """
        
        print(f"üìñ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {pdf_path}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        docs = PDFLoader(pdf_path).load()
        total_pages = len(docs)
        full_text = "\n\n".join([d.page_content for d in docs])
        
        print(f"   –°—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
        print(f"   –°–∏–º–≤–æ–ª–æ–≤: {len(full_text):,}")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–∑–±–∏–µ–Ω–∏—è –ø–æ —Ä–∞–∑–º–µ—Ä—É
        if total_pages < 50:
            chunk_size = 5000
            levels = 2
        elif total_pages < 200:
            chunk_size = 3000
            levels = 3
        else:
            chunk_size = 2000
            levels = 4
        
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {levels}-—É—Ä–æ–≤–Ω–µ–≤–∞—è —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è")
        
        # –£—Ä–æ–≤–µ–Ω—å 1: –†–∞–∑–±–∏–µ–Ω–∏–µ –∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
        print("\nüîÑ –£—Ä–æ–≤–µ–Ω—å 1: –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Å–µ–∫—Ü–∏–π...")
        
        splitter = RecursiveTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=500
        )
        chunks = splitter.split_documents(docs)
        
        section_summaries = []
        chunk_groups = self._group_chunks(chunks, max_group_size=10)
        
        for i, group in enumerate(chunk_groups):
            print(f"   –°–µ–∫—Ü–∏—è {i+1}/{len(chunk_groups)}")
            
            combined_text = "\n\n".join([c.page_content for c in group])
            page_start = group[0].metadata.get("page", i * 10)
            page_end = group[-1].metadata.get("page", (i + 1) * 10)
            
            summary = self.section_summarizer.run(f"""
            –°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–π—Ç–µ —ç—Ç—É —Å–µ–∫—Ü–∏—é (—Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_start}-{page_end}):
            
            {combined_text[:15000]}
            
            –£–∫–∞–∂–∏—Ç–µ:
            1. –ù–∞–∑–≤–∞–Ω–∏–µ —Å–µ–∫—Ü–∏–∏ (–≤—ã–≤–µ–¥–∏—Ç–µ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
            2. –°–∞–º–º–∞—Ä–∏ (200-300 —Å–ª–æ–≤)
            3. –ö–ª—é—á–µ–≤—ã–µ –ø—É–Ω–∫—Ç—ã (–º–∞–∫—Å 5)
            4. –í–∞–∂–Ω—ã–µ —É–ø–æ–º—è–Ω—É—Ç—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
            """)
            
            section_summaries.append(SectionSummary(
                title=self._extract_title(summary),
                page_range=f"{page_start}-{page_end}",
                summary=summary,
                key_points=self._extract_key_points(summary),
                entities=self._extract_entities(summary)
            ))
        
        # –£—Ä–æ–≤–µ–Ω—å 2+: –†–µ–∫—É—Ä—Å–∏–≤–Ω–∞—è –º–µ—Ç–∞-—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
        current_summaries = [s.summary for s in section_summaries]
        
        for level in range(2, levels + 1):
            print(f"\nüîÑ –£—Ä–æ–≤–µ–Ω—å {level}: –ú–µ—Ç–∞-—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è...")
            
            if len(current_summaries) <= 3:
                break
            
            grouped = self._group_texts(current_summaries, max_group_size=5)
            meta_summaries = []
            
            for group in grouped:
                combined = "\n\n---\n\n".join(group)
                
                meta_summary = self.meta_summarizer.run(f"""
                –°–∏–Ω—Ç–µ–∑–∏—Ä—É–π—Ç–µ —ç—Ç–∏ —Å–∞–º–º–∞—Ä–∏ –≤ —Å–≤—è–∑–Ω—ã–π –Ω–∞—Ä—Ä–∞—Ç–∏–≤:
                
                {combined}
                
                –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —É—Å—Ç—Ä–∞–Ω—è—è –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å.
                –¶–µ–ª–µ–≤–∞—è –¥–ª–∏–Ω–∞: {500 // level} —Å–ª–æ–≤.
                """)
                
                meta_summaries.append(meta_summary)
            
            current_summaries = meta_summaries
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ executive summary
        print("\nüìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è executive summary...")
        
        all_section_content = "\n\n".join(current_summaries)
        
        executive_summary = self.meta_summarizer.run(f"""
        –°–æ–∑–¥–∞–π—Ç–µ executive summary –∏–∑ —ç—Ç–∏—Ö —Å–∞–º–º–∞—Ä–∏ —Å–µ–∫—Ü–∏–π:
        
        {all_section_content}
        
        Executive summary –¥–æ–ª–∂–Ω–æ:
        1. –ü–µ—Ä–µ–¥–∞—Ç—å –≥–ª–∞–≤–Ω—É—é —Ü–µ–ª—å/—Ç–µ–∑–∏—Å
        2. –í—ã–¥–µ–ª–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –Ω–∞—Ö–æ–¥–∫–∏
        3. –û—Ç–º–µ—Ç–∏—Ç—å –≤–∞–∂–Ω—ã–µ –≤—ã–≤–æ–¥—ã
        4. –ë—ã—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–º –¥–ª—è —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π –≤—ã—Å—à–µ–≥–æ –∑–≤–µ–Ω–∞
        
        –î–ª–∏–Ω–∞: {self._get_target_words(target_length)} —Å–ª–æ–≤
        """)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–º –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        themes = self._extract_themes(section_summaries)
        recommendations = self._extract_recommendations(executive_summary, section_summaries)
        
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–ª—è Q&A
        print("\nüíæ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
        self.vectorstore = ChromaVectorStore.from_documents(
            chunks,
            self.embeddings,
            collection_name="doc_summary"
        )
        self.rlm.set_retriever(self.vectorstore.as_retriever(k=10))
        
        return DocumentSummary(
            title=self._infer_title(docs[0].page_content[:2000]),
            total_pages=total_pages,
            executive_summary=executive_summary,
            section_summaries=section_summaries,
            key_themes=themes,
            recommendations=recommendations
        )
    
    def query(self, question: str) -> str:
        """–ó–∞–ø—Ä–æ—Å –∫ —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É."""
        return self.rlm.run(f"""
        –ù–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –æ—Ç–≤–µ—Ç—å—Ç–µ: {question}
        
        –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≥–¥–µ –≤–æ–∑–º–æ–∂–Ω–æ.
        """)
    
    def _group_chunks(self, chunks, max_group_size: int):
        """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Å–µ–∫—Ü–∏–æ–Ω–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏."""
        groups = []
        current_group = []
        
        for chunk in chunks:
            current_group.append(chunk)
            if len(current_group) >= max_group_size:
                groups.append(current_group)
                current_group = []
        
        if current_group:
            groups.append(current_group)
        
        return groups
    
    def _group_texts(self, texts, max_group_size: int):
        """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –º–µ—Ç–∞-—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏."""
        return [texts[i:i+max_group_size] for i in range(0, len(texts), max_group_size)]
    
    def _extract_title(self, text: str) -> str:
        """–ò–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–µ–∫—Ü–∏–∏ –∏–∑ —Å–∞–º–º–∞—Ä–∏."""
        if ":" in text[:100]:
            return text[:text.find(":")].strip()
        return text[:50].strip() + "..."
    
    def _extract_key_points(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ –ø—É–Ω–∫—Ç—ã –∏–∑ —Å–∞–º–º–∞—Ä–∏."""
        lines = text.split("\n")
        points = [l.strip("- ‚Ä¢*").strip() for l in lines if l.strip().startswith(("-", "‚Ä¢", "*", "1", "2", "3", "4", "5"))]
        return points[:5]
    
    def _extract_entities(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏."""
        extractor = RLM.from_openai("gpt-4o-mini")
        result = extractor.run(f"–ò–∑–≤–ª–µ–∫–∏—Ç–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑: {text[:1000]}\n–í–µ—Ä–Ω–∏—Ç–µ JSON –º–∞—Å—Å–∏–≤.")
        try:
            import json
            return json.loads(result)
        except:
            return []
    
    def _extract_themes(self, sections: List[SectionSummary]) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å —Å–∫–≤–æ–∑–Ω—ã–µ —Ç–µ–º—ã."""
        all_content = "\n".join([s.summary for s in sections])
        
        result = self.meta_summarizer.run(f"""
        –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –≥–ª–∞–≤–Ω—ã–µ —Ç–µ–º—ã –≤ —ç—Ç–∏—Ö —Å–µ–∫—Ü–∏—è—Ö:
        
        {all_content[:5000]}
        
        –í–µ—Ä–Ω–∏—Ç–µ 5-7 –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º —Å–ø–∏—Å–∫–æ–º.
        """)
        
        return result.split("\n")[:7]
    
    def _extract_recommendations(self, executive: str, sections: List[SectionSummary]) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å –∏–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
        result = self.meta_summarizer.run(f"""
        –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ —Å–∞–º–º–∞—Ä–∏, –∫–∞–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–ª–∏ –¥–µ–π—Å—Ç–≤–∏—è?
        
        {executive}
        
        –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ 3-5 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.
        """)
        
        return result.split("\n")[:5]
    
    def _get_target_words(self, length: str) -> int:
        """–ü–æ–ª—É—á–∏—Ç—å —Ü–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤."""
        return {"brief": 300, "standard": 800, "comprehensive": 1500}.get(length, 800)
    
    def _infer_title(self, first_page: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ."""
        result = RLM.from_openai("gpt-4o-mini").run(f"""
        –ö–∞–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞?
        
        {first_page}
        
        –í–µ—Ä–Ω–∏—Ç–µ —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ.
        """)
        return result.strip()

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    summarizer = RecursiveDocumentSummarizer()
    
    # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –±–æ–ª—å—à–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    summary = summarizer.summarize(
        "annual_report_500pages.pdf",
        target_length="comprehensive"
    )
    
    print(f"\n=== {summary.title} ===")
    print(f"–°—Ç—Ä–∞–Ω–∏—Ü: {summary.total_pages}")
    print(f"\n--- Executive Summary ---\n{summary.executive_summary}")
    
    print(f"\n--- –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã ---")
    for theme in summary.key_themes:
        print(f"  ‚Ä¢ {theme}")
    
    print(f"\n--- –°–µ–∫—Ü–∏–∏ ({len(summary.section_summaries)}) ---")
    for section in summary.section_summaries[:5]:
        print(f"  üìë {section.title} (—Å—Ç—Ä. {section.page_range})")
    
    # –ó–∞–ø—Ä–æ—Å –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É
    answer = summarizer.query("–ö–∞–∫–∏–µ –±—ã–ª–∏ –≥–ª–∞–≤–Ω—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã?")
    print(f"\n--- Q&A ---\n{answer}")
```

---

*–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≤ –ß–∞—Å—Ç–∏ 3: –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–¥–∞–∫—à–µ–Ω –ø—Ä–∏–º–µ—Ä—ã...*
