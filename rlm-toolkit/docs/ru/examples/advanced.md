# –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã

Enterprise-—É—Ä–æ–≤–µ–Ω—å, production-ready –ø—Ä–∏–º–µ—Ä—ã, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏–µ –º–æ—â–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ RLM-Toolkit.

---

## 1. –ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–≥–µ–Ω—Ç

–ü–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∞–≥–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ç–µ–º—ã, –Ω–∞—Ö–æ–¥–∏—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ —Å–æ–∑–¥–∞—ë—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –æ—Ç—á—ë—Ç—ã —Å —Ü–∏—Ç–∞—Ç–∞–º–∏.

```python
from rlm_toolkit import RLM
from rlm_toolkit.agents import ReActAgent
from rlm_toolkit.agents.multiagent import MetaMatrix, Agent
from rlm_toolkit.tools import Tool, WebSearchTool, ArxivTool, WikipediaTool
from rlm_toolkit.memory import HierarchicalMemory
from pydantic import BaseModel
from typing import List, Optional
from datetime import datetime
import json

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class Source(BaseModel):
    title: str
    url: str
    snippet: str
    relevance_score: float

class Section(BaseModel):
    heading: str
    content: str
    sources: List[str]

class ResearchReport(BaseModel):
    title: str
    executive_summary: str
    sections: List[Section]
    conclusions: List[str]
    sources: List[Source]
    generated_at: str

# –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
@Tool(name="save_source", description="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
def save_source(title: str, url: str, snippet: str, relevance: float) -> str:
    return json.dumps({"saved": True, "id": hash(url)})

@Tool(name="write_section", description="–ù–∞–ø–∏—Å–∞—Ç—å —Ä–∞–∑–¥–µ–ª –æ—Ç—á—ë—Ç–∞")
def write_section(heading: str, content: str, source_ids: List[str]) -> str:
    return json.dumps({"section": heading, "words": len(content.split())})

class AutonomousResearchAgent:
    """
    –ú–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–π –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–≥–µ–Ω—Ç:
    1. –ü–ª–∞–Ω–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    2. –°–æ–±–∏—Ä–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å —Ä–∞–∑–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º
    3. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    4. –°–æ–∑–¥–∞—ë—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á—ë—Ç —Å —Ü–∏—Ç–∞—Ç–∞–º–∏
    """
    
    def __init__(self):
        self.memory = HierarchicalMemory(persist_directory="./research_memory")
        
        # –ê–≥–µ–Ω—Ç-–ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
        self.planner = RLM.from_openai("gpt-4o")
        self.planner.set_system_prompt("""
        –í—ã ‚Äî –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π. –ü–æ –∑–∞–¥–∞–Ω–Ω–æ–π —Ç–µ–º–µ:
        1. –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –æ—Ç–≤–µ—Ç–∞
        2. –ü–µ—Ä–µ—á–∏—Å–ª–∏—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ, –≤–µ–±, –Ω–æ–≤–æ—Å—Ç–∏)
        3. –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç—á—ë—Ç–∞
        4. –û—Ü–µ–Ω–∏—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –≥–ª—É–±–∏–Ω—É
        
        –ë—É–¥—å—Ç–µ —Ç—â–∞—Ç–µ–ª—å–Ω—ã, –Ω–æ —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω—ã.
        """)
        
        # –ê–≥–µ–Ω—Ç-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å
        self.researcher = ReActAgent.from_openai(
            "gpt-4o",
            tools=[
                WebSearchTool(provider="ddg", max_results=10),
                ArxivTool(max_results=5),
                WikipediaTool(),
                save_source
            ],
            system_prompt="""
            –í—ã ‚Äî —Å–∫—Ä—É–ø—É–ª—ë–∑–Ω—ã–π –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞:
            - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å
            - –ò–∑–≤–ª–µ–∫–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã
            - –û—Ç–º–µ—Ç—å—Ç–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è
            - –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Å –æ—Ü–µ–Ω–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            
            –°—Ç—Ä–µ–º–∏—Ç–µ—Å—å –∫ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º, –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º.
            """,
            max_iterations=20
        )
        
        # –ê–≥–µ–Ω—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫
        self.analyst = RLM.from_anthropic("claude-3-sonnet")
        self.analyst.set_system_prompt("""
        –í—ã ‚Äî –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –ü–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:
        1. –í—ã—è–≤–∏—Ç–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ —Ç—Ä–µ–Ω–¥—ã
        2. –û—Ç–º–µ—Ç—å—Ç–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –∏–ª–∏ –ø—Ä–æ–±–µ–ª—ã
        3. –°–∏–Ω—Ç–µ–∑–∏—Ä—É–π—Ç–µ –≤ —Å–≤—è–∑–Ω–æ–µ –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ
        4. –í—ã–¥–µ–ª–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã
        
        –ë—É–¥—å—Ç–µ –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã –∏ –æ–ø–∏—Ä–∞–π—Ç–µ—Å—å –Ω–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞.
        """)
        
        # –ê–≥–µ–Ω—Ç-–ø–∏—Å–∞—Ç–µ–ª—å
        self.writer = RLM.from_openai("gpt-4o")
        self.writer.set_system_prompt("""
        –í—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –ø–∏—Å–∞—Ç–µ–ª—å. –°–æ–∑–¥–∞–≤–∞–π—Ç–µ:
        - –Ø—Å–Ω—É—é, —É–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—É—é –ø—Ä–æ–∑—É
        - –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã [1], [2] –∏ —Ç.–¥.
        - –õ–æ–≥–∏—á–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –º–µ–∂–¥—É —Ä–∞–∑–¥–µ–ª–∞–º–∏
        - –†–µ–∑—é–º–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —á—Ç–µ–Ω–∏—è
        
        –ü–∏—à–∏—Ç–µ –¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω–æ–π, –Ω–æ –Ω–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏.
        """)
        
    def research(self, topic: str, depth: str = "comprehensive") -> ResearchReport:
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–ª–Ω—ã–π –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø–∞–π–ø–ª–∞–π–Ω."""
        
        print(f"üî¨ –ù–∞—á–∏–Ω–∞–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: {topic}")
        
        # –§–∞–∑–∞ 1: –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
        print("üìã –§–∞–∑–∞ 1: –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è...")
        plan = self.planner.run(f"""
        –°–æ–∑–¥–∞–π—Ç–µ –ø–ª–∞–Ω –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–ª—è: {topic}
        –ì–ª—É–±–∏–Ω–∞: {depth}
        
        –í–µ—Ä–Ω–∏—Ç–µ:
        1. –ö–ª—é—á–µ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã (5-10)
        2. –¢–∏–ø—ã –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        3. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç—á—ë—Ç–∞
        """)
        
        # –§–∞–∑–∞ 2: –°–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        print("üîç –§–∞–∑–∞ 2: –°–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
        sources_raw = self.researcher.run(f"""
        –¢–µ–º–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {topic}
        
        –ü–ª–∞–Ω: {plan}
        
        –ù–∞–π–¥–∏—Ç–µ –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ –º–∏–Ω–∏–º—É–º 10 –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
        –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Å –æ—Ü–µ–Ω–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏.
        –û—Ö–≤–∞—Ç–∏—Ç–µ: –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ —Å—Ç–∞—Ç—å–∏, –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å–∞–π—Ç—ã, –Ω–æ–≤–æ—Å—Ç–∏.
        """)
        
        # –§–∞–∑–∞ 3: –ê–Ω–∞–ª–∏–∑
        print("üß† –§–∞–∑–∞ 3: –ê–Ω–∞–ª–∏–∑ –Ω–∞—Ö–æ–¥–æ–∫...")
        analysis = self.analyst.run(f"""
        –¢–µ–º–∞: {topic}
        
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:
        {sources_raw}
        
        –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ:
        1. –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã
        2. –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Ö–æ–¥–∫–∏ –ø–æ –∫–∞–∂–¥–æ–π —Ç–µ–º–µ
        3. –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –∏–ª–∏ –¥–µ–±–∞—Ç—ã
        4. –ü—Ä–æ–±–µ–ª—ã –≤ –∑–Ω–∞–Ω–∏—è—Ö
        5. –°–∏–Ω—Ç–µ–∑ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤
        """)
        
        # –§–∞–∑–∞ 4: –ù–∞–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞
        print("‚úçÔ∏è –§–∞–∑–∞ 4: –ù–∞–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞...")
        report_content = self.writer.run(f"""
        –¢–µ–º–∞: {topic}
        
        –ê–Ω–∞–ª–∏–∑:
        {analysis}
        
        –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:
        {sources_raw}
        
        –ù–∞–ø–∏—à–∏—Ç–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ—Ç—á—ë—Ç —Å:
        1. –†–µ–∑—é–º–µ (200 —Å–ª–æ–≤)
        2. –í–≤–µ–¥–µ–Ω–∏–µ
        3. –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Ö–æ–¥–∫–∏ (3-5 —Ä–∞–∑–¥–µ–ª–æ–≤)
        4. –û–±—Å—É–∂–¥–µ–Ω–∏–µ
        5. –í—ã–≤–æ–¥—ã
        6. –ü—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ñ–æ—Ä–º–ª–µ–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã
        """)
        
        # –§–∞–∑–∞ 5: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
        print("üìÑ –§–∞–∑–∞ 5: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞...")
        report = self.writer.run_structured(
            f"""
            –ü—Ä–µ–æ–±—Ä–∞–∑—É–π—Ç–µ —ç—Ç–æ—Ç –æ—Ç—á—ë—Ç –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç:
            
            {report_content}
            """,
            output_schema=ResearchReport
        )
        
        report.generated_at = datetime.now().isoformat()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ø–∞–º—è—Ç—å
        self.memory.add_episode(
            f"–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ —Ç–µ–º–µ {topic}",
            metadata={"topic": topic, "depth": depth}
        )
        
        print("‚úÖ –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        return report
    
    def save_report(self, report: ResearchReport, path: str):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á—ë—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown."""
        md = f"# {report.title}\n\n"
        md += f"*–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {report.generated_at}*\n\n"
        md += f"## –†–µ–∑—é–º–µ\n\n{report.executive_summary}\n\n"
        
        for section in report.sections:
            md += f"## {section.heading}\n\n{section.content}\n\n"
            if section.sources:
                md += f"*–ò—Å—Ç–æ—á–Ω–∏–∫–∏: {', '.join(section.sources)}*\n\n"
        
        md += "## –í—ã–≤–æ–¥—ã\n\n"
        for i, conclusion in enumerate(report.conclusions, 1):
            md += f"{i}. {conclusion}\n"
        
        md += "\n## –°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã\n\n"
        for i, source in enumerate(report.sources, 1):
            md += f"[{i}] {source.title}. {source.url}\n"
        
        with open(path, "w", encoding="utf-8") as f:
            f.write(md)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    agent = AutonomousResearchAgent()
    
    report = agent.research(
        topic="–í–ª–∏—è–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ü–û –≤ 2024",
        depth="comprehensive"
    )
    
    agent.save_report(report, "llm_impact_research.md")
    
    print(f"\n–û—Ç—á—ë—Ç: {report.title}")
    print(f"–†–∞–∑–¥–µ–ª–æ–≤: {len(report.sections)}")
    print(f"–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(report.sources)}")
```

---

## 2. –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π RAG-–ø–∞–π–ø–ª–∞–π–Ω

RAG-—Å–∏—Å—Ç–µ–º–∞, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∞—è PDF, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ –≤ –µ–¥–∏–Ω–æ–º –ø–∞–π–ø–ª–∞–π–Ω–µ.

```python
from rlm_toolkit import RLM, RLMConfig
from rlm_toolkit.loaders import PDFLoader, ImageLoader, AudioLoader, VideoLoader
from rlm_toolkit.splitters import RecursiveTextSplitter, SemanticSplitter
from rlm_toolkit.embeddings import OpenAIEmbeddings, MultiModalEmbeddings
from rlm_toolkit.vectorstores import ChromaVectorStore
from rlm_toolkit.retrievers import HybridRetriever, MultiModalRetriever
from pydantic import BaseModel
from typing import List, Union, Optional
from pathlib import Path
import base64

class ContentChunk(BaseModel):
    content: str
    content_type: str  # text, image, audio, video
    source: str
    metadata: dict

class MultiModalRAG:
    """
    –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π RAG-–ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞:
    - PDF —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
    - –û—Ç–¥–µ–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–¥–∏–∞–≥—Ä–∞–º–º—ã, –≥—Ä–∞—Ñ–∏–∫–∏)
    - –ê—É–¥–∏–æ—Ñ–∞–π–ª—ã (—Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
    - –í–∏–¥–µ–æ—Ñ–∞–π–ª—ã (—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è + –∫–ª—é—á–µ–≤—ã–µ –∫–∞–¥—Ä—ã)
    """
    
    def __init__(self, collection_name: str = "multimodal"):
        # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        self.text_embeddings = OpenAIEmbeddings("text-embedding-3-large")
        
        # LLM —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        self.vision_llm = RLM.from_openai("gpt-4o")
        
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ
        self.whisper = OpenAI()
        
        # –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è–º–∏
        self.text_store = ChromaVectorStore(
            collection_name=f"{collection_name}_text",
            embedding_function=self.text_embeddings
        )
        self.image_store = ChromaVectorStore(
            collection_name=f"{collection_name}_images",
            embedding_function=self.text_embeddings  # –•—Ä–∞–Ω–∏–º –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        )
        
        # –ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä
        self.retriever = MultiModalRetriever(
            text_store=self.text_store,
            image_store=self.image_store,
            text_weight=0.7,
            image_weight=0.3
        )
        
        # –û—Å–Ω–æ–≤–Ω–æ–π QA LLM
        self.qa_llm = RLM.from_openai("gpt-4o")
        self.qa_llm.set_system_prompt("""
        –í—ã ‚Äî –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –í—ã –ø–æ–Ω–∏–º–∞–µ—Ç–µ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç–µ:
        - –¢–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        - –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –¥–∏–∞–≥—Ä–∞–º–º—ã
        - –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ
        
        –î–∞–≤–∞–π—Ç–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã, –∏—Å–ø–æ–ª—å–∑—É—è –≤–µ—Å—å –¥–æ—Å—Ç—É–ø–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
        –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å—Å—ã–ª–∞–π—Ç–µ—Å—å –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏.
        """)
        
    def ingest_pdf(self, path: str) -> int:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å PDF —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏."""
        loader = PDFLoader(path, extract_images=True)
        docs = loader.load()
        
        text_chunks = []
        image_chunks = []
        
        for doc in docs:
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
            if doc.page_content:
                splitter = RecursiveTextSplitter(chunk_size=1000, chunk_overlap=200)
                text_chunks.extend(splitter.split_documents([doc]))
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            if doc.metadata.get("images"):
                for img in doc.metadata["images"]:
                    description = self._describe_image(img["data"])
                    image_chunks.append(ContentChunk(
                        content=description,
                        content_type="image",
                        source=f"{path}:page{doc.metadata['page']}",
                        metadata={"image_data": img["data"]}
                    ))
        
        self.text_store.add_documents(text_chunks)
        for chunk in image_chunks:
            self.image_store.add_texts([chunk.content], metadatas=[chunk.metadata])
        
        return len(text_chunks) + len(image_chunks)
    
    def ingest_image(self, path: str) -> int:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ."""
        with open(path, "rb") as f:
            image_data = base64.b64encode(f.read()).decode()
        
        description = self._describe_image(image_data)
        
        self.image_store.add_texts(
            [description],
            metadatas=[{"source": path, "image_data": image_data}]
        )
        
        return 1
    
    def ingest_audio(self, path: str) -> int:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∞—É–¥–∏–æ—Ñ–∞–π–ª —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é."""
        with open(path, "rb") as f:
            transcript = self.whisper.audio.transcriptions.create(
                model="whisper-1",
                file=f,
                response_format="verbose_json"
            )
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º
        chunks = []
        for segment in transcript.segments:
            chunks.append(ContentChunk(
                content=segment["text"],
                content_type="audio",
                source=path,
                metadata={
                    "start": segment["start"],
                    "end": segment["end"]
                }
            ))
        
        self.text_store.add_texts(
            [c.content for c in chunks],
            metadatas=[c.metadata for c in chunks]
        )
        
        return len(chunks)
    
    def ingest_video(self, path: str, extract_frames: bool = True) -> int:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤–∏–¥–µ–æ: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è + –∫–ª—é—á–µ–≤—ã–µ –∫–∞–¥—Ä—ã."""
        chunks_added = 0
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
        audio_path = self._extract_audio(path)
        chunks_added += self.ingest_audio(audio_path)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –∫–∞–¥—Ä–æ–≤
        if extract_frames:
            keyframes = self._extract_keyframes(path, interval=30)  # –ö–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
            for timestamp, frame_data in keyframes:
                description = self._describe_image(frame_data)
                self.image_store.add_texts(
                    [description],
                    metadatas={
                        "source": path,
                        "timestamp": timestamp,
                        "image_data": frame_data
                    }
                )
                chunks_added += 1
        
        return chunks_added
    
    def _describe_image(self, image_data: str) -> str:
        """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Vision LLM –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        return self.vision_llm.run(
            "–û–ø–∏—à–∏—Ç–µ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ. –£–∫–∞–∂–∏—Ç–µ: –æ—Å–Ω–æ–≤–Ω–æ–π –æ–±—ä–µ–∫—Ç, –≤–∏–¥–∏–º—ã–π —Ç–µ–∫—Å—Ç, "
            "—Ü–≤–µ—Ç–∞, –∫–æ–º–ø–æ–Ω–æ–≤–∫—É, –ª—é–±—ã–µ –¥–∞–Ω–Ω—ã–µ/–≥—Ä–∞—Ñ–∏–∫–∏. –ë—É–¥—å—Ç–µ –∏—Å—á–µ—Ä–ø—ã–≤–∞—é—â–∏–º–∏.",
            images=[image_data]
        )
    
    def _extract_audio(self, video_path: str) -> str:
        """–ò–∑–≤–ª–µ—á—å –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ."""
        import subprocess
        audio_path = video_path.replace(".mp4", ".mp3")
        subprocess.run([
            "ffmpeg", "-i", video_path, "-vn", "-acodec", "mp3", audio_path
        ], capture_output=True)
        return audio_path
    
    def _extract_keyframes(self, video_path: str, interval: int) -> List[tuple]:
        """–ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ –∫–∞–¥—Ä—ã —Å –∑–∞–¥–∞–Ω–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º."""
        import cv2
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        keyframes = []
        frame_interval = int(fps * interval)
        frame_count = 0
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            if frame_count % frame_interval == 0:
                _, buffer = cv2.imencode('.jpg', frame)
                frame_data = base64.b64encode(buffer).decode()
                timestamp = frame_count / fps
                keyframes.append((timestamp, frame_data))
            
            frame_count += 1
        
        cap.release()
        return keyframes
    
    def query(
        self,
        question: str,
        include_images: bool = True,
        k: int = 5
    ) -> dict:
        """–ó–∞–ø—Ä–æ—Å –ø–æ –≤—Å–µ–º –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º."""
        
        # –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–∞–º
        text_results = self.text_store.similarity_search(question, k=k)
        
        if include_images:
            image_results = self.image_store.similarity_search(question, k=3)
        else:
            image_results = []
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context = "## –¢–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç:\n"
        for doc in text_results:
            context += f"- {doc.page_content}\n"
            context += f"  –ò—Å—Ç–æ—á–Ω–∏–∫: {doc.metadata.get('source', '–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')}\n\n"
        
        if image_results:
            context += "\n## –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:\n"
            for doc in image_results:
                context += f"- [–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ] {doc.page_content}\n"
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        answer = self.qa_llm.run(f"""
        –í–æ–ø—Ä–æ—Å: {question}
        
        –ö–æ–Ω—Ç–µ–∫—Å—Ç:
        {context}
        
        –î–∞–π—Ç–µ –∏—Å—á–µ—Ä–ø—ã–≤–∞—é—â–∏–π –æ—Ç–≤–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É—è –¥–æ—Å—Ç—É–ø–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
        –°—Å—ã–ª–∞–π—Ç–µ—Å—å –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ –æ–ø–∏—Å—ã–≤–∞–π—Ç–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        """)
        
        return {
            "answer": answer,
            "text_sources": [d.metadata.get("source") for d in text_results],
            "image_sources": [d.metadata.get("source") for d in image_results]
        }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    rag = MultiModalRAG("company_docs")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–∑–ª–∏—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    rag.ingest_pdf("quarterly_report.pdf")
    rag.ingest_image("architecture_diagram.png")
    rag.ingest_audio("earnings_call.mp3")
    rag.ingest_video("product_demo.mp4")
    
    # –ó–∞–ø—Ä–æ—Å –ø–æ –≤—Å–µ–º –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º
    result = rag.query("–ö–∞–∫–æ–π –±—ã–ª –¥–æ—Ö–æ–¥ –≤ Q3 –∏ –∫–∞–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ?")
    print(result["answer"])
```

---

## 3. –ê–≥–µ–Ω—Ç Code Review

–ê–≥–µ–Ω—Ç, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π pull requests, –Ω–∞—Ö–æ–¥—è—â–∏–π –±–∞–≥–∏, –ø—Ä–µ–¥–ª–∞–≥–∞—é—â–∏–π —É–ª—É—á—à–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–∏–π —Ç–µ—Å—Ç—ã.

```python
from rlm_toolkit import RLM
from rlm_toolkit.agents import ReActAgent
from rlm_toolkit.tools import Tool
from rlm_toolkit.memory import BufferMemory
from pydantic import BaseModel
from typing import List, Optional
from enum import Enum
import subprocess
import json
import ast

class Severity(str, Enum):
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"

class CodeIssue(BaseModel):
    file: str
    line: int
    severity: Severity
    category: str  # bug, security, performance, style, maintainability
    description: str
    suggestion: str
    code_snippet: Optional[str]

class ReviewResult(BaseModel):
    summary: str
    issues: List[CodeIssue]
    suggested_tests: List[str]
    refactoring_suggestions: List[str]
    approval_recommendation: str  # approve, request_changes, comment

# –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞
@Tool(name="read_file", description="–ü—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è")
def read_file(file_path: str) -> str:
    try:
        with open(file_path, "r") as f:
            return f.read()
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}"

@Tool(name="get_diff", description="–ü–æ–ª—É—á–∏—Ç—å git diff –¥–ª—è —Ñ–∞–π–ª–∞")
def get_diff(file_path: str) -> str:
    result = subprocess.run(
        ["git", "diff", "HEAD~1", file_path],
        capture_output=True,
        text=True
    )
    return result.stdout or "–ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π"

@Tool(name="run_linter", description="–ó–∞–ø—É—Å—Ç–∏—Ç—å –ª–∏–Ω—Ç–µ—Ä –Ω–∞ —Ñ–∞–π–ª–µ")
def run_linter(file_path: str) -> str:
    result = subprocess.run(
        ["ruff", "check", file_path, "--output-format=json"],
        capture_output=True,
        text=True
    )
    return result.stdout

@Tool(name="check_types", description="–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É —Ç–∏–ø–æ–≤")
def check_types(file_path: str) -> str:
    result = subprocess.run(
        ["mypy", file_path, "--output=json"],
        capture_output=True,
        text=True
    )
    return result.stdout or result.stderr

@Tool(name="run_tests", description="–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã –¥–ª—è –º–æ–¥—É–ª—è")
def run_tests(module_path: str) -> str:
    result = subprocess.run(
        ["pytest", module_path, "-v", "--tb=short"],
        capture_output=True,
        text=True
    )
    return result.stdout + result.stderr

@Tool(name="analyze_complexity", description="–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞")
def analyze_complexity(file_path: str) -> str:
    result = subprocess.run(
        ["radon", "cc", file_path, "-j"],
        capture_output=True,
        text=True
    )
    return result.stdout

class CodeReviewAgent:
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–≥–µ–Ω—Ç –∫–æ–¥-—Ä–µ–≤—å—é:
    1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–¥–∞
    2. –ù–∞—Ö–æ–¥–∏—Ç –±–∞–≥–∏, –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    3. –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∏–ª—å –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ—Å—Ç—å
    4. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è –∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥
    5. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ—Å—Ç-–∫–µ–π—Å—ã –¥–ª—è –Ω–æ–≤–æ–≥–æ –∫–æ–¥–∞
    """
    
    def __init__(self):
        # –û—Å–Ω–æ–≤–Ω–æ–π –∞–≥–µ–Ω—Ç —Ä–µ–≤—å—é
        self.reviewer = ReActAgent.from_openai(
            "gpt-4o",
            tools=[read_file, get_diff, run_linter, check_types, run_tests, analyze_complexity],
            system_prompt="""
            –í—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –∫–æ–¥-—Ä–µ–≤—å—é —Å –≥–ª—É–±–æ–∫–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏:
            - –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏
            - –£—è–∑–≤–∏–º–æ—Å—Ç–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (OWASP Top 10)
            - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            - –ü—Ä–∏–Ω—Ü–∏–ø—ã —á–∏—Å—Ç–æ–≥–æ –∫–æ–¥–∞
            - –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            
            –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏:
            1. –ü—Ä–æ—á–∏—Ç–∞–π—Ç–µ –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Ñ–∞–π–ª–∞
            2. –ü–æ–ª—É—á–∏—Ç–µ diff –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π
            3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ª–∏–Ω—Ç–µ—Ä –∏ –ø—Ä–æ–≤–µ—Ä–∫—É —Ç–∏–ø–æ–≤
            4. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å–ª–æ–∂–Ω–æ—Å—Ç—å
            5. –í—ã—è–≤–∏—Ç–µ –ø—Ä–æ–±–ª–µ–º—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            
            –ë—É–¥—å—Ç–µ —Ç—â–∞—Ç–µ–ª—å–Ω—ã, –Ω–æ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã. –§–æ–∫—É—Å–∏—Ä—É–π—Ç–µ—Å—å –Ω–∞ actionable —Ñ–∏–¥–±–µ–∫–µ.
            """,
            max_iterations=30
        )
        
        # –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        self.security_agent = RLM.from_anthropic("claude-3-sonnet")
        self.security_agent.set_system_prompt("""
        –í—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∫–æ–¥ –Ω–∞:
        - SQL-–∏–Ω—ä–µ–∫—Ü–∏–∏
        - XSS-—É—è–∑–≤–∏–º–æ—Å—Ç–∏
        - –û—à–∏–±–∫–∏ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏/–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        - –ù–µ–±–µ–∑–æ–ø–∞—Å–Ω—É—é –¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—é
        - –£—Ç–µ—á–∫—É —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        - SSRF-—É—è–∑–≤–∏–º–æ—Å—Ç–∏
        - Path traversal
        - Command injection
        
        –°–æ–æ–±—â–∞–π—Ç–µ –¢–û–õ–¨–ö–û –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Å —Å–µ—Ä—å—ë–∑–Ω–æ—Å—Ç—å—é –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º.
        """)
        
        # –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ—Å—Ç–æ–≤
        self.test_generator = RLM.from_openai("gpt-4o")
        self.test_generator.set_system_prompt("""
        –í—ã ‚Äî –∏–Ω–∂–µ–Ω–µ—Ä –ø–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é. –ü–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –∫–æ–¥—É:
        1. –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ —Ç–µ—Å—Ç–∏—Ä—É–µ–º—ã–µ –µ–¥–∏–Ω–∏—Ü—ã (—Ñ—É–Ω–∫—Ü–∏–∏, –∫–ª–∞—Å—Å—ã, –º–µ—Ç–æ–¥—ã)
        2. –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ —Ç–µ—Å—Ç-–∫–µ–π—Å—ã, –ø–æ–∫—Ä—ã–≤–∞—é—â–∏–µ:
           - Happy path
           - –ì—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏
           - –û–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫
           - –ü–æ–≥—Ä–∞–Ω–∏—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è
        3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—Ç–∏–ª—å pytest —Å –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏
        4. –í–∫–ª—é—á–∏—Ç–µ —Ñ–∏–∫—Å—Ç—É—Ä—ã –∏ –º–æ–∫–∏ –≥–¥–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
        """)
        
    def review_pr(self, files: List[str]) -> ReviewResult:
        """–ü—Ä–æ–≤–µ—Å—Ç–∏ —Ä–µ–≤—å—é pull request."""
        all_issues = []
        
        # –§–∞–∑–∞ 1: –ü–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
        print("üîç –§–∞–∑–∞ 1: –ê–Ω–∞–ª–∏–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∫–æ–¥–∞...")
        for file in files:
            analysis = self.reviewer.run(f"""
            –ü—Ä–æ–≤–µ–¥–∏—Ç–µ —Ä–µ–≤—å—é —Ñ–∞–π–ª–∞: {file}
            
            –®–∞–≥–∏:
            1. –ü—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª
            2. –ü–æ–ª—É—á–∏—Ç—å diff
            3. –ó–∞–ø—É—Å—Ç–∏—Ç—å –ª–∏–Ω—Ç–µ—Ä
            4. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–∏–ø—ã
            5. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å
            
            –°–æ–æ–±—â–∏—Ç–µ –æ–±–æ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å —Ñ–∞–π–ª–æ–º, —Å—Ç—Ä–æ–∫–æ–π, —Å–µ—Ä—å—ë–∑–Ω–æ—Å—Ç—å—é –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º.
            """)
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–±–ª–µ–º –∏–∑ –∞–Ω–∞–ª–∏–∑–∞
            issues = self._parse_issues(analysis, file)
            all_issues.extend(issues)
        
        # –§–∞–∑–∞ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        print("üîê –§–∞–∑–∞ 2: –ê–Ω–∞–ª–∏–∑ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏...")
        for file in files:
            if file.endswith(".py"):
                with open(file, "r") as f:
                    code = f.read()
                
                security_issues = self.security_agent.run(f"""
                –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –Ω–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏:
                
                ```python
                {code}
                ```
                
                –°–æ–æ–±—â–∏—Ç–µ –æ –∫–∞–∂–¥–æ–π –ø—Ä–æ–±–ª–µ–º–µ —Å –Ω–æ–º–µ—Ä–æ–º —Å—Ç—Ä–æ–∫–∏ –∏ —Å–µ—Ä—å—ë–∑–Ω–æ—Å—Ç—å—é.
                """)
                
                issues = self._parse_security_issues(security_issues, file)
                all_issues.extend(issues)
        
        # –§–∞–∑–∞ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤
        print("üß™ –§–∞–∑–∞ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ —Ç–µ—Å—Ç–∞–º...")
        test_suggestions = []
        for file in files:
            if file.endswith(".py") and not file.startswith("test_"):
                with open(file, "r") as f:
                    code = f.read()
                
                tests = self.test_generator.run(f"""
                –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ pytest —Ç–µ—Å—Ç-–∫–µ–π—Å—ã –¥–ª—è:
                
                ```python
                {code}
                ```
                
                –°—Ñ–æ–∫—É—Å–∏—Ä—É–π—Ç–µ—Å—å –Ω–∞ –Ω–æ–≤—ã—Ö –∏–ª–∏ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏—è—Ö.
                """)
                test_suggestions.append(tests)
        
        # –§–∞–∑–∞ 4: –°–∏–Ω—Ç–µ–∑
        print("üìù –§–∞–∑–∞ 4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ–∑—é–º–µ —Ä–µ–≤—å—é...")
        summary = self._generate_summary(all_issues)
        recommendation = self._get_recommendation(all_issues)
        
        refactoring = self._suggest_refactoring(files)
        
        return ReviewResult(
            summary=summary,
            issues=all_issues,
            suggested_tests=test_suggestions,
            refactoring_suggestions=refactoring,
            approval_recommendation=recommendation
        )
    
    def _parse_issues(self, analysis: str, file: str) -> List[CodeIssue]:
        """–ò–∑–≤–ª–µ—á—å –ø—Ä–æ–±–ª–µ–º—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞."""
        extractor = RLM.from_openai("gpt-4o-mini")
        issues_json = extractor.run(f"""
        –ò–∑–≤–ª–µ–∫–∏—Ç–µ –ø—Ä–æ–±–ª–µ–º—ã –∫–æ–¥–∞ –∏–∑ —ç—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞–∫ JSON-—Å–ø–∏—Å–æ–∫:
        
        {analysis}
        
        –§–æ—Ä–º–∞—Ç: [{{"file": str, "line": int, "severity": str, "category": str, "description": str, "suggestion": str}}]
        """)
        
        try:
            issues_data = json.loads(issues_json)
            return [CodeIssue(**issue) for issue in issues_data]
        except:
            return []
    
    def _parse_security_issues(self, analysis: str, file: str) -> List[CodeIssue]:
        """–ò–∑–≤–ª–µ—á—å –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏."""
        issues = self._parse_issues(analysis, file)
        for issue in issues:
            issue.category = "security"
        return issues
    
    def _generate_summary(self, issues: List[CodeIssue]) -> str:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—é–º–µ —Ä–µ–≤—å—é."""
        critical = len([i for i in issues if i.severity == Severity.CRITICAL])
        high = len([i for i in issues if i.severity == Severity.HIGH])
        medium = len([i for i in issues if i.severity == Severity.MEDIUM])
        low = len([i for i in issues if i.severity == Severity.LOW])
        
        return f"""
        ## –†–µ–∑—é–º–µ Code Review
        
        **–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º:** {len(issues)}
        - üî¥ –ö—Ä–∏—Ç–∏—á–Ω—ã—Ö: {critical}
        - üü† –í—ã—Å–æ–∫–∏—Ö: {high}
        - üü° –°—Ä–µ–¥–Ω–∏—Ö: {medium}
        - üü¢ –ù–∏–∑–∫–∏—Ö: {low}
        
        **–ö–∞—Ç–µ–≥–æ—Ä–∏–∏:**
        - –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: {len([i for i in issues if i.category == 'security'])}
        - –ë–∞–≥–∏: {len([i for i in issues if i.category == 'bug'])}
        - –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {len([i for i in issues if i.category == 'performance'])}
        - –°—Ç–∏–ª—å: {len([i for i in issues if i.category == 'style'])}
        """
    
    def _get_recommendation(self, issues: List[CodeIssue]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –ø–æ –∞–ø–ø—Ä—É–≤—É."""
        critical = len([i for i in issues if i.severity == Severity.CRITICAL])
        high = len([i for i in issues if i.severity == Severity.HIGH])
        
        if critical > 0:
            return "request_changes"
        elif high > 2:
            return "request_changes"
        elif high > 0:
            return "comment"
        else:
            return "approve"
    
    def _suggest_refactoring(self, files: List[str]) -> List[str]:
        """–ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞."""
        suggestions = []
        
        for file in files:
            with open(file, "r") as f:
                code = f.read()
            
            refactoring = RLM.from_openai("gpt-4o").run(f"""
            –ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –¥–ª—è:
            
            ```python
            {code}
            ```
            
            –°—Ñ–æ–∫—É—Å–∏—Ä—É–π—Ç–µ—Å—å –Ω–∞:
            - –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö Extract Method
            - –î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∫–ª–∞—Å—Å–æ–≤
            - –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            - –ù–∞—Ä—É—à–µ–Ω–∏—è—Ö DRY
            
            –î–∞–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ, actionable –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
            """)
            suggestions.append(f"## {file}\n{refactoring}")
        
        return suggestions

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    agent = CodeReviewAgent()
    
    # –†–µ–≤—å—é –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    files = [
        "src/api/handlers.py",
        "src/services/user_service.py",
        "src/utils/validators.py"
    ]
    
    result = agent.review_pr(files)
    
    print(result.summary)
    print(f"\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {result.approval_recommendation}")
    
    for issue in result.issues:
        print(f"\n[{issue.severity}] {issue.file}:{issue.line}")
        print(f"  {issue.description}")
        print(f"  –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: {issue.suggestion}")
```

---

## 4. –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

Enterprise-–ò–ò –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤, –≤—ã—è–≤–ª–µ–Ω–∏—è —Ä–∏—Å–∫–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ø—Ä–∞–≤–æ–∫.

```python
from rlm_toolkit import RLM, RLMConfig
from rlm_toolkit.loaders import PDFLoader
from rlm_toolkit.splitters import RecursiveTextSplitter
from rlm_toolkit.embeddings import OpenAIEmbeddings
from rlm_toolkit.vectorstores import ChromaVectorStore
from pydantic import BaseModel
from typing import List, Optional, Dict
from enum import Enum
from datetime import date
import json

class RiskLevel(str, Enum):
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"

class ClauseType(str, Enum):
    INDEMNIFICATION = "indemnification"
    LIABILITY = "liability"
    TERMINATION = "termination"
    CONFIDENTIALITY = "confidentiality"
    IP_OWNERSHIP = "ip_ownership"
    PAYMENT = "payment"
    DISPUTE = "dispute"
    GOVERNING_LAW = "governing_law"
    FORCE_MAJEURE = "force_majeure"
    ASSIGNMENT = "assignment"

class Clause(BaseModel):
    type: ClauseType
    text: str
    page: int
    risk_level: RiskLevel
    analysis: str
    industry_standard: bool
    concerns: List[str]

class Party(BaseModel):
    name: str
    role: str  # buyer, seller, licensor, licensee –∏ —Ç.–¥.
    obligations: List[str]
    rights: List[str]

class ContractAnalysis(BaseModel):
    title: str
    parties: List[Party]
    effective_date: Optional[str]
    term: Optional[str]
    total_value: Optional[str]
    clauses: List[Clause]
    overall_risk: RiskLevel
    negotiation_points: List[str]
    missing_clauses: List[str]

class Amendment(BaseModel):
    clause_type: ClauseType
    original_text: str
    proposed_text: str
    rationale: str
    risk_reduction: str

class LegalDocumentAnalyzer:
    """
    Enterprise-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
    1. –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç—å–∏
    2. –í—ã—è–≤–ª—è–µ—Ç —Ä–∏—Å–∫–∏ –∏ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è
    3. –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å –ª—É—á—à–∏–º–∏ –ø—Ä–∞–∫—Ç–∏–∫–∞–º–∏
    4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –ø–æ–ø—Ä–∞–≤–∫–∞–º
    5. –°–æ–∑–¥–∞—ë—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–æ–≤
    """
    
    def __init__(self):
        # –û—Å–Ω–æ–≤–Ω–æ–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫
        self.analyst = RLM.from_anthropic("claude-3-opus")
        self.analyst.set_system_prompt("""
        –í—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π —é—Ä–∏—Å—Ç —Å 20+ –≥–æ–¥–∞–º–∏ –æ–ø—ã—Ç–∞ –≤:
        - M&A —Å–¥–µ–ª–∫–∞—Ö
        - –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞—Ö
        - –õ–∏—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π
        - –¢—Ä—É–¥–æ–≤—ã—Ö –¥–æ–≥–æ–≤–æ—Ä–∞—Ö
        
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã —Å –ø—Ä–µ–¥–µ–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é. –í—ã—è–≤–ª—è–π—Ç–µ:
        - –ù–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏–ª–∏ –Ω–µ–æ–±—ã—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è
        - –°–∫—Ä—ã—Ç—ã–µ —Ä–∏—Å–∫–∏ –∏ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞
        - –û–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è
        - –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–∞—â–∏—Ç—ã
        
        –í—Å–µ–≥–¥–∞ —Ü–∏—Ç–∏—Ä—É–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞.
        """)
        
        # –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –æ—Ü–µ–Ω–∫–µ —Ä–∏—Å–∫–æ–≤
        self.risk_assessor = RLM.from_openai("gpt-4o")
        self.risk_assessor.set_system_prompt("""
        –í—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ä–∏—Å–∫–æ–≤. –û—Ü–µ–Ω–∏–≤–∞–π—Ç–µ —Å—Ç–∞—Ç—å–∏ –Ω–∞:
        - –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä–∏—Å–∫–∏
        - –û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
        - –†–∏—Å–∫–∏ —Å–æ–±–ª—é–¥–µ–Ω–∏—è —Ä–µ–≥—É–ª—è—Ü–∏–π
        - –†–µ–ø—É—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏
        - –ü—Ä–æ–±–ª–µ–º—ã —Å –∏—Å–ø–æ–ª–Ω–∏–º–æ—Å—Ç—å—é
        
        –ì–¥–µ –≤–æ–∑–º–æ–∂–Ω–æ, –∫–≤–∞–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–π—Ç–µ —Ä–∏—Å–∫–∏.
        """)
        
        # –°–æ—Å—Ç–∞–≤–∏—Ç–µ–ª—å –ø–æ–ø—Ä–∞–≤–æ–∫
        self.drafter = RLM.from_anthropic("claude-3-sonnet")
        self.drafter.set_system_prompt("""
        –í—ã ‚Äî —Å—Ç–∞—Ä—à–∏–π —Å–æ—Å—Ç–∞–≤–∏—Ç–µ–ª—å –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤. –°–æ–∑–¥–∞–≤–∞–π—Ç–µ –ø–æ–ø—Ä–∞–≤–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ:
        - –ò—Å–ø–æ–ª—å–∑—É—é—Ç —Ç–æ—á–Ω—ã–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —è–∑—ã–∫
        - –ò—Å–ø–æ–ª–Ω–∏–º—ã –≤ –ø—Ä–∏–º–µ–Ω–∏–º–æ–π —é—Ä–∏—Å–¥–∏–∫—Ü–∏–∏
        - –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã –º–µ–∂–¥—É —Å—Ç–æ—Ä–æ–Ω–∞–º–∏
        - –°–ª–µ–¥—É—é—Ç –æ—Ç—Ä–∞—Å–ª–µ–≤—ã–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º
        
        –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–π—Ç–µ —á—ë—Ç–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è.
        """)
        
        # –ë–∞–∑–∞ –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫
        self.embeddings = OpenAIEmbeddings("text-embedding-3-large")
        self.best_practices_store = ChromaVectorStore(
            collection_name="legal_best_practices",
            embedding_function=self.embeddings
        )
        
    def analyze_contract(self, pdf_path: str) -> ContractAnalysis:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞."""
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥
        print("üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞...")
        docs = PDFLoader(pdf_path).load()
        full_text = "\n\n".join([d.page_content for d in docs])
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        print("üìã –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞...")
        basic_info = self.analyst.run(f"""
        –ò–∑–≤–ª–µ–∫–∏—Ç–µ –∏–∑ —ç—Ç–æ–≥–æ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞:
        1. –ù–∞–∑–≤–∞–Ω–∏–µ/—Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
        2. –í—Å–µ —Å—Ç–æ—Ä–æ–Ω—ã —Å –∏—Ö —Ä–æ–ª—è–º–∏
        3. –î–∞—Ç–∞ –≤—Å—Ç—É–ø–ª–µ–Ω–∏—è –≤ —Å–∏–ª—É
        4. –°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è
        5. –û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞)
        
        –ö–æ–Ω—Ç—Ä–∞–∫—Ç:
        {full_text[:30000]}
        """)
        
        # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ –∞–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–µ–π
        print("üîç –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–µ–π...")
        clauses = self._analyze_clauses(full_text)
        
        # –û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤
        print("‚ö†Ô∏è –û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤...")
        for clause in clauses:
            clause.risk_level = self._assess_clause_risk(clause)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Å—Ç–∞—Ç–µ–π
        print("üìù –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã...")
        missing = self._check_missing_clauses(clauses)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ—á–µ–∫ –¥–ª—è –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–æ–≤
        print("üéØ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ—á–µ–∫ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–æ–≤...")
        negotiation_points = self._generate_negotiation_points(clauses)
        
        # –†–∞—Å—á—ë—Ç –æ–±—â–µ–≥–æ —Ä–∏—Å–∫–∞
        overall_risk = self._calculate_overall_risk(clauses)
        
        return ContractAnalysis(
            title=self._extract_title(basic_info),
            parties=self._extract_parties(basic_info),
            effective_date=self._extract_field(basic_info, "–¥–∞—Ç–∞ –≤—Å—Ç—É–ø–ª–µ–Ω–∏—è"),
            term=self._extract_field(basic_info, "—Å—Ä–æ–∫"),
            total_value=self._extract_field(basic_info, "—Å—Ç–æ–∏–º–æ—Å—Ç—å"),
            clauses=clauses,
            overall_risk=overall_risk,
            negotiation_points=negotiation_points,
            missing_clauses=missing
        )
    
    def generate_amendments(self, analysis: ContractAnalysis) -> List[Amendment]:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –ø–æ–ø—Ä–∞–≤–∫–∞–º –¥–ª—è –≤—ã—Å–æ–∫–æ—Ä–∏—Å–∫–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π."""
        amendments = []
        
        high_risk_clauses = [
            c for c in analysis.clauses 
            if c.risk_level in [RiskLevel.CRITICAL, RiskLevel.HIGH]
        ]
        
        for clause in high_risk_clauses:
            amendment = self.drafter.run(f"""
            –°–æ—Å—Ç–∞–≤—å—Ç–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é —ç—Ç–æ–π —Å—Ç–∞—Ç—å–∏ {clause.type.value}:
            
            –û–†–ò–ì–ò–ù–ê–õ:
            "{clause.text}"
            
            –ü–†–û–ë–õ–ï–ú–´:
            {clause.concerns}
            
            –°–æ–∑–¥–∞–π—Ç–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ä–µ–¥–∞–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è:
            1. –£—Å—Ç—Ä–∞–Ω—è–µ—Ç –≤—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
            2. –û—Å—Ç–∞—ë—Ç—Å—è –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏ —Ä–∞–∑—É–º–Ω–æ–π
            3. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —è–∑—ã–∫
            
            –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ.
            """)
            
            amendments.append(Amendment(
                clause_type=clause.type,
                original_text=clause.text,
                proposed_text=self._extract_proposed_text(amendment),
                rationale=self._extract_rationale(amendment),
                risk_reduction=f"–°–Ω–∏–∂–∞–µ—Ç —Ä–∏—Å–∫ —Å {clause.risk_level.value} –¥–æ –±–æ–ª–µ–µ –Ω–∏–∑–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è"
            ))
        
        return amendments

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    analyzer = LegalDocumentAnalyzer()
    
    # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞
    analysis = analyzer.analyze_contract("vendor_agreement.pdf")
    
    print(f"–ö–æ–Ω—Ç—Ä–∞–∫—Ç: {analysis.title}")
    print(f"–û–±—â–∏–π —Ä–∏—Å–∫: {analysis.overall_risk}")
    print(f"\n–°—Ç–æ—Ä–æ–Ω—ã:")
    for party in analysis.parties:
        print(f"  - {party.name} ({party.role})")
    
    print(f"\n–í—ã—Å–æ–∫–æ—Ä–∏—Å–∫–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏:")
    for clause in analysis.clauses:
        if clause.risk_level in [RiskLevel.CRITICAL, RiskLevel.HIGH]:
            print(f"  [{clause.risk_level}] {clause.type.value}")
            print(f"    –ü—Ä–æ–±–ª–µ–º—ã: {clause.concerns}")
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ø—Ä–∞–≤–æ–∫
    amendments = analyzer.generate_amendments(analysis)
    print(f"\n–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–æ –ø–æ–ø—Ä–∞–≤–æ–∫: {len(amendments)}")
```

---

## 5. –¢–æ—Ä–≥–æ–≤—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏

–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –ò–ò –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä—ã–Ω–∫–∞, –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–æ–≤.

```python
from rlm_toolkit import RLM
from rlm_toolkit.agents import ReActAgent
from rlm_toolkit.tools import Tool, WebSearchTool
from rlm_toolkit.memory import HierarchicalMemory
from rlm_toolkit.callbacks import TokenCounterCallback
from pydantic import BaseModel
from typing import List, Optional, Dict
from enum import Enum
from datetime import datetime, timedelta
import asyncio
import json

class Signal(str, Enum):
    STRONG_BUY = "strong_buy"
    BUY = "buy"
    HOLD = "hold"
    SELL = "sell"
    STRONG_SELL = "strong_sell"

class TimeFrame(str, Enum):
    INTRADAY = "intraday"
    SWING = "swing"
    POSITION = "position"

class MarketSentiment(BaseModel):
    overall: str  # bullish, bearish, neutral
    confidence: float  # 0-1
    key_factors: List[str]
    news_impact: str

class TechnicalAnalysis(BaseModel):
    trend: str  # uptrend, downtrend, sideways
    support_levels: List[float]
    resistance_levels: List[float]
    indicators: Dict[str, str]  # RSI, MACD –∏ —Ç.–¥.

class FundamentalAnalysis(BaseModel):
    valuation: str  # undervalued, fair, overvalued
    financial_health: str
    growth_prospects: str
    key_metrics: Dict[str, float]

class TradeIdea(BaseModel):
    symbol: str
    signal: Signal
    timeframe: TimeFrame
    entry_price: float
    stop_loss: float
    take_profit: List[float]
    risk_reward: float
    confidence: float
    rationale: str
    catalysts: List[str]
    risks: List[str]

# –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—Å–∏–º—É–ª—è—Ü–∏—è ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–µ–∞–ª—å–Ω—ã–µ API –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ)
@Tool(name="get_price", description="–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é —Ü–µ–Ω—É –¥–ª—è —Å–∏–º–≤–æ–ª–∞")
def get_price(symbol: str) -> str:
    import random
    price = random.uniform(100, 500)
    return json.dumps({"symbol": symbol, "price": round(price, 2), "change": round(random.uniform(-5, 5), 2)})

@Tool(name="get_technicals", description="–ü–æ–ª—É—á–∏—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã")
def get_technicals(symbol: str) -> str:
    import random
    return json.dumps({
        "rsi": random.randint(20, 80),
        "macd": {"value": random.uniform(-5, 5), "signal": random.uniform(-5, 5)},
        "sma_20": random.uniform(100, 500),
        "sma_50": random.uniform(100, 500),
        "bollinger": {"upper": 520, "middle": 500, "lower": 480}
    })

@Tool(name="get_fundamentals", description="–ü–æ–ª—É—á–∏—Ç—å —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
def get_fundamentals(symbol: str) -> str:
    import random
    return json.dumps({
        "pe_ratio": random.uniform(10, 50),
        "peg_ratio": random.uniform(0.5, 3),
        "debt_equity": random.uniform(0.1, 2),
        "roe": random.uniform(5, 30),
        "revenue_growth": random.uniform(-10, 50),
        "eps_growth": random.uniform(-20, 100)
    })

@Tool(name="get_news", description="–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Å–∏–º–≤–æ–ª—É")
def get_news(symbol: str, days: int = 7) -> str:
    return json.dumps([
        {"title": f"{symbol} –∞–Ω–æ–Ω—Å–∏—Ä—É–µ—Ç –∑–∞–ø—É—Å–∫ –Ω–æ–≤–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞", "sentiment": "positive", "date": "2024-01-15"},
        {"title": f"–ê–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ–≤—ã—à–∞–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥ {symbol} –¥–æ '–ø–æ–∫—É–ø–∞—Ç—å'", "sentiment": "positive", "date": "2024-01-14"},
        {"title": f"–°–µ–∫—Ç–æ—Ä —Å—Ç–∞–ª–∫–∏–≤–∞–µ—Ç—Å—è —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏", "sentiment": "negative", "date": "2024-01-13"}
    ])

class TradingAssistant:
    """
    –¢–æ—Ä–≥–æ–≤—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏:
    1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä—ã–Ω–æ—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è
    2. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç
    3. –ü—Ä–æ–≤–æ–¥–∏—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
    4. –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
    5. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–æ—Ä–≥–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —Ä–∏—Å–∫–∞–º–∏
    """
    
    def __init__(self):
        self.memory = HierarchicalMemory(persist_directory="./trading_memory")
        
        # –†—ã–Ω–æ—á–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫
        self.market_analyst = ReActAgent.from_openai(
            "gpt-4o",
            tools=[get_price, get_technicals, get_news],
            system_prompt="""
            –í—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä—ã–Ω–æ—á–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ:
            - –¶–µ–Ω–æ–≤–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ –∏ –æ–±—ä—ë–º—ã
            - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (RSI, MACD, —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ)
            - –ì—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            - –†—ã–Ω–æ—á–Ω—ã–π —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π
            
            –ë—É–¥—å—Ç–µ –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã –∏ –æ—Å–Ω–æ–≤—ã–≤–∞–π—Ç–µ—Å—å –Ω–∞ –¥–∞–Ω–Ω—ã—Ö. –ò–∑–±–µ–≥–∞–π—Ç–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∏—Å–∫–∞–∂–µ–Ω–∏–π.
            """,
            max_iterations=10
        )
        
        # –§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫
        self.fundamental_analyst = ReActAgent.from_openai(
            "gpt-4o",
            tools=[get_fundamentals],
            system_prompt="""
            –í—ã ‚Äî —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –û—Ü–µ–Ω–∏–≤–∞–π—Ç–µ:
            - –ú—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–æ—Ä—ã –æ—Ü–µ–Ω–∫–∏ (P/E, PEG, P/B)
            - –§–∏–Ω–∞–Ω—Å–æ–≤–æ–µ –∑–¥–æ—Ä–æ–≤—å–µ (–¥–æ–ª–≥, –¥–µ–Ω–µ–∂–Ω—ã–π –ø–æ—Ç–æ–∫)
            - –¢—Ä–∞–µ–∫—Ç–æ—Ä–∏—è —Ä–æ—Å—Ç–∞
            - –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è
            
            –§–æ–∫—É—Å–∏—Ä—É–π—Ç–µ—Å—å –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞—Ö.
            """,
            max_iterations=10
        )
        
        # –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–∞
        self.sentiment_analyzer = RLM.from_anthropic("claude-3-sonnet")
        self.sentiment_analyzer.set_system_prompt("""
        –í—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π. –û—Ü–µ–Ω–∏–≤–∞–π—Ç–µ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞:
        - –í–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä—ã–Ω–æ–∫ (high, medium, low)
        - –°–µ–Ω—Ç–∏–º–µ–Ω—Ç (bullish, bearish, neutral)
        - –í—Ä–µ–º–µ–Ω–Ω–æ–π –≥–æ—Ä–∏–∑–æ–Ω—Ç –≤–ª–∏—è–Ω–∏—è
        - –ù–∞–¥—ë–∂–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        
        –ë—É–¥—å—Ç–µ —Å–∫–µ–ø—Ç–∏—á–Ω—ã –∫ —Ö–∞–π–ø—É –∏ —Ñ–æ–∫—É—Å–∏—Ä—É–π—Ç–µ—Å—å –Ω–∞ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
        """)
        
        # –¢–æ—Ä–≥–æ–≤—ã–π —Å—Ç—Ä–∞—Ç–µ–≥
        self.strategist = RLM.from_openai("gpt-4o")
        self.strategist.set_system_prompt("""
        –í—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç—Ä–µ–π–¥–µ—Ä –∏ —Ä–∏—Å–∫-–º–µ–Ω–µ–¥–∂–µ—Ä. –°–æ–∑–¥–∞–≤–∞–π—Ç–µ —Ç–æ—Ä–≥–æ–≤—ã–µ –∏–¥–µ–∏ —Å:
        - –ß—ë—Ç–∫–∏–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏ –≤—Ö–æ–¥–∞ –∏ –≤—ã—Ö–æ–¥–∞
        - –û–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º —Å—Ç–æ–ø-–ª–æ—Å—Å–æ–º –∏ —Ç–µ–π–∫-–ø—Ä–æ—Ñ–∏—Ç–æ–º
        - –ê–Ω–∞–ª–∏–∑–æ–º —Ä–∏—Å–∫/–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
        - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É –ø–æ–∑–∏—Ü–∏–∏
        
        –í—Å–µ–≥–¥–∞ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–π—Ç–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–ø–∏—Ç–∞–ª–∞. –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –ø—Ä–µ–¥–ª–∞–≥–∞–π—Ç–µ –ø–æ–∑–∏—Ü–∏–∏ ¬´all-in¬ª.
        """)
        
    async def analyze_symbol(self, symbol: str) -> TradeIdea:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–∏–º–≤–æ–ª–∞."""
        
        print(f"üìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {symbol}...")
        
        # –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        technical_task = asyncio.create_task(self._get_technical_analysis(symbol))
        fundamental_task = asyncio.create_task(self._get_fundamental_analysis(symbol))
        sentiment_task = asyncio.create_task(self._get_sentiment(symbol))
        
        technical = await technical_task
        fundamental = await fundamental_task
        sentiment = await sentiment_task
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ—Ä–≥–æ–≤–æ–π –∏–¥–µ–∏
        trade_idea = self._generate_trade_idea(symbol, technical, fundamental, sentiment)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ø–∞–º—è—Ç—å
        self.memory.add_episode(
            f"–ê–Ω–∞–ª–∏–∑ {symbol}: {trade_idea.signal.value}",
            metadata={"symbol": symbol, "signal": trade_idea.signal.value}
        )
        
        return trade_idea
    
    def screen_market(self, symbols: List[str]) -> List[TradeIdea]:
        """–°–∫—Ä–∏–Ω–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –≤–æ–∑–≤—Ä–∞—Ç –ª—É—á—à–∏—Ö –∏–¥–µ–π."""
        ideas = []
        
        for symbol in symbols:
            try:
                idea = asyncio.run(self.analyze_symbol(symbol))
                if idea.signal in [Signal.STRONG_BUY, Signal.STRONG_SELL]:
                    ideas.append(idea)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {symbol}: {e}")
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        ideas.sort(key=lambda x: x.confidence, reverse=True)
        
        return ideas[:10]  # –¢–æ–ø-10 –∏–¥–µ–π

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    assistant = TradingAssistant()
    
    # –ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    idea = asyncio.run(assistant.analyze_symbol("AAPL"))
    print(f"\n{idea.symbol}: {idea.signal.value}")
    print(f"–í—Ö–æ–¥: ${idea.entry_price} | –°—Ç–æ–ø: ${idea.stop_loss}")
    print(f"–¶–µ–ª–∏: {idea.take_profit}")
    print(f"R/R: {idea.risk_reward} | –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {idea.confidence}")
    print(f"–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {idea.rationale}")
    
    # –°–∫—Ä–∏–Ω–∏–Ω–≥ —Ä—ã–Ω–∫–∞
    watchlist = ["AAPL", "MSFT", "GOOGL", "AMZN", "NVDA", "META"]
    top_ideas = assistant.screen_market(watchlist)
    
    print("\n=== –¢–æ–ø —Ç–æ—Ä–≥–æ–≤—ã—Ö –∏–¥–µ–π ===")
    for idea in top_ideas:
        print(f"{idea.symbol}: {idea.signal.value} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {idea.confidence})")
```

---

*–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≤ –ß–∞—Å—Ç–∏ 2...*
