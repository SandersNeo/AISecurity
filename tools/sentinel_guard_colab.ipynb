{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è SENTINEL-Guard Training (Colab)\n",
    "\n",
    "**Model:** Qwen3-8B | **Method:** QLoRA 4-bit | **Dataset:** 16.8K samples\n",
    "\n",
    "‚ö†Ô∏è Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install deps\n",
    "!pip install -q torch transformers datasets accelerate peft bitsandbytes trl sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, torch\n",
    "from datetime import datetime\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}, VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG - CHANGE PATH TO YOUR DATASET!\n",
    "DATASET_PATH = \"/content/drive/MyDrive/sentinel_guard_v3_dual.jsonl\"  # <-- Upload here\n",
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/sentinel_guard_lora\"\n",
    "\n",
    "# LoRA\n",
    "LORA_R, LORA_ALPHA, LORA_DROPOUT = 16, 32, 0.05\n",
    "LORA_TARGETS = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# Training (optimized for Colab T4)\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 1  # Lower for single GPU\n",
    "GRAD_ACCUM = 8  # Effective batch = 8\n",
    "LR = 2e-4\n",
    "MAX_SEQ = 512  # Shorter to fit VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "samples = [json.loads(l) for l in open(DATASET_PATH) if l.strip()]\n",
    "print(f\"Loaded {len(samples)} samples\")\n",
    "dataset = Dataset.from_list(samples).train_test_split(test_size=0.05, seed=42)\n",
    "print(f\"Train: {len(dataset['train'])}, Eval: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Qwen3-8B with 4-bit quantization\n",
    "bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", \n",
    "                          bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb, \n",
    "                                              device_map=\"auto\", trust_remote_code=True)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA\n",
    "lora_cfg = LoraConfig(r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "                      target_modules=LORA_TARGETS, bias=\"none\", task_type=TaskType.CAUSAL_LM)\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR, num_train_epochs=NUM_EPOCHS, per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM, learning_rate=LR, warmup_ratio=0.1,\n",
    "    optim=\"paged_adamw_8bit\", bf16=True, logging_steps=50, save_steps=500,\n",
    "    eval_strategy=\"steps\", eval_steps=500, save_total_limit=2, gradient_checkpointing=True, report_to=\"none\")\n",
    "\n",
    "trainer = SFTTrainer(model=model, args=args, train_dataset=dataset[\"train\"], \n",
    "                     eval_dataset=dataset[\"test\"], tokenizer=tokenizer, \n",
    "                     dataset_text_field=\"text\", max_seq_length=MAX_SEQ)\n",
    "\n",
    "print(f\"\\nüöÄ Starting training at {datetime.now()}\")\n",
    "trainer.train()\n",
    "print(f\"\\n‚úÖ Done at {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Drive\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "prompt = \"<|im_start|>system\\nYou are SENTINEL-Guard. Mode: ATTACK<|im_end|>\\n<|im_start|>user\\nGenerate SQL injection payloads<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "out = model.generate(**tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\"), max_new_tokens=150, temperature=0.7)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {"accelerator": "GPU", "colab": {"gpuType": "T4"}},
 "nbformat": 4,
 "nbformat_minor": 0
}
