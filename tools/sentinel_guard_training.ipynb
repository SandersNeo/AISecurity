{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è SENTINEL-Guard Dual-Mode Training\n",
    "\n",
    "Fine-tune **Qwen3-8B** on SENTINEL security dataset for:\n",
    "- **DEFENSE**: Classify threats\n",
    "- **ATTACK**: Generate payloads\n",
    "- **EXPLAIN**: Vulnerability analysis\n",
    "\n",
    "**Requirements:** Kaggle T4 x2 GPU, ~45-60 min training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers datasets accelerate peft bitsandbytes trl sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, torch\n",
    "from datetime import datetime\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(f\"CUDA: {torch.cuda.is_available()}, GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "OUTPUT_DIR = \"./sentinel_guard_lora\"\n",
    "DATASET_PATH = \"/kaggle/input/sentinel-guard-v3/sentinel_guard_v3_dual.jsonl\"\n",
    "\n",
    "LORA_R, LORA_ALPHA, LORA_DROPOUT = 16, 32, 0.05\n",
    "LORA_TARGETS = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "NUM_EPOCHS, BATCH_SIZE, GRAD_ACCUM, LR, MAX_SEQ = 2, 2, 4, 2e-4, 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "samples = [json.loads(l) for l in open(DATASET_PATH) if l.strip()]\n",
    "print(f\"Samples: {len(samples)}\")\n",
    "dataset = Dataset.from_list(samples).train_test_split(test_size=0.05, seed=42)\n",
    "print(f\"Train: {len(dataset['train'])}, Eval: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Qwen3-8B with 4-bit\n",
    "bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, LoraConfig(r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT, target_modules=LORA_TARGETS, bias=\"none\", task_type=TaskType.CAUSAL_LM))\n",
    "model.print_trainable_parameters()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "args = TrainingArguments(output_dir=OUTPUT_DIR, num_train_epochs=NUM_EPOCHS, per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM, learning_rate=LR, warmup_ratio=0.1, optim=\"paged_adamw_8bit\",\n",
    "    bf16=True, logging_steps=25, save_steps=500, eval_steps=500, eval_strategy=\"steps\", save_total_limit=2,\n",
    "    load_best_model_at_end=True, report_to=\"none\", gradient_checkpointing=True)\n",
    "trainer = SFTTrainer(model=model, args=args, train_dataset=dataset[\"train\"], eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer, dataset_text_field=\"text\", max_seq_length=MAX_SEQ)\n",
    "print(f\"Starting training at {datetime.now()}\")\n",
    "trainer.train()\n",
    "print(f\"Done at {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "!zip -r sentinel_guard_lora.zip ./sentinel_guard_lora\n",
    "print(\"Done! Download sentinel_guard_lora.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ATTACK mode\n",
    "prompt = \"<|im_start|>system\\nYou are SENTINEL-Guard. Mode: ATTACK<|im_end|>\\n<|im_start|>user\\nGenerate SQL injection payloads<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "out = model.generate(**tokenizer(prompt, return_tensors=\"pt\").to(model.device), max_new_tokens=200, temperature=0.7)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
